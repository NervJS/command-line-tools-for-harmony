<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>API Reference: NeuralNeworkRuntime</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">API Reference
   &#160;<span id="projectnumber">5.0.5.165</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('group___neural_nework_runtime.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#files">Files</a> &#124;
<a href="#nested-classes">Data Structures</a> &#124;
<a href="#typedef-members">Typedefs</a> &#124;
<a href="#enum-members">Enumerations</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle">
<div class="title">NeuralNeworkRuntime</div>  </div>
</div><!--header-->
<div class="contents">

<p>Provides APIs of Neural Network Runtime for accelerating the model inference.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="files"></a>
Files</h2></td></tr>
<tr class="memitem:neural__network__core_8h"><td class="memItemLeft" align="right" valign="top">file &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="neural__network__core_8h.html">neural_network_core.h</a></td></tr>
<tr class="memdesc:neural__network__core_8h"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the Neural Network Core APIs. The AI inference framework uses the Native APIs provided by Neural Network Core to compile models and perform inference and computing on acceleration hardware. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:neural__network__runtime_8h"><td class="memItemLeft" align="right" valign="top">file &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="neural__network__runtime_8h.html">neural_network_runtime.h</a></td></tr>
<tr class="memdesc:neural__network__runtime_8h"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the Neural Network Runtime APIs. The AI inference framework uses the Native APIs provided by Neural Network Runtime to construct models. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:neural__network__runtime__type_8h"><td class="memItemLeft" align="right" valign="top">file &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="neural__network__runtime__type_8h.html">neural_network_runtime_type.h</a></td></tr>
<tr class="memdesc:neural__network__runtime__type_8h"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the structure and enumeration. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Data Structures</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This structure is used to store a 32-bit unsigned integer array.  <a href="struct_o_h___n_n___u_int32_array.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="struct_o_h___n_n___quant_param.html">OH_NN_QuantParam</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Quantization information.  <a href="struct_o_h___n_n___quant_param.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the tensor structure.  <a href="struct_o_h___n_n___tensor.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the memory structure.  <a href="struct_o_h___n_n___memory.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="typedef-members"></a>
Typedefs</h2></td></tr>
<tr class="memitem:ga4169d4a60707eae47f924a19a77db0a1"><td class="memItemLeft" align="right" valign="top">typedef struct <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a></td></tr>
<tr class="memdesc:ga4169d4a60707eae47f924a19a77db0a1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the handles of models.  <a href="#ga4169d4a60707eae47f924a19a77db0a1">More...</a><br /></td></tr>
<tr class="separator:ga4169d4a60707eae47f924a19a77db0a1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga717fa252bcbce3942c2ec2b25238492b"><td class="memItemLeft" align="right" valign="top">typedef struct <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a></td></tr>
<tr class="memdesc:ga717fa252bcbce3942c2ec2b25238492b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the compilation handle.  <a href="#ga717fa252bcbce3942c2ec2b25238492b">More...</a><br /></td></tr>
<tr class="separator:ga717fa252bcbce3942c2ec2b25238492b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga93fd7e632b29ec5174a09e138478ce78"><td class="memItemLeft" align="right" valign="top">typedef struct <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a></td></tr>
<tr class="memdesc:ga93fd7e632b29ec5174a09e138478ce78"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the executor handle.  <a href="#ga93fd7e632b29ec5174a09e138478ce78">More...</a><br /></td></tr>
<tr class="separator:ga93fd7e632b29ec5174a09e138478ce78"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga94cf3b68da13f278ad55431da7342619"><td class="memItemLeft" align="right" valign="top">typedef struct <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a></td></tr>
<tr class="memdesc:ga94cf3b68da13f278ad55431da7342619"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the quantization parameter handle.  <a href="#ga94cf3b68da13f278ad55431da7342619">More...</a><br /></td></tr>
<tr class="separator:ga94cf3b68da13f278ad55431da7342619"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga15da798ec7d8a9e924f172a252fcf080"><td class="memItemLeft" align="right" valign="top">typedef struct <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a></td></tr>
<tr class="memdesc:ga15da798ec7d8a9e924f172a252fcf080"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the tensor descriptor handle.  <a href="#ga15da798ec7d8a9e924f172a252fcf080">More...</a><br /></td></tr>
<tr class="separator:ga15da798ec7d8a9e924f172a252fcf080"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga36df49080d9305767e96668e948a8f3b"><td class="memItemLeft" align="right" valign="top">typedef struct <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a></td></tr>
<tr class="memdesc:ga36df49080d9305767e96668e948a8f3b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the tensor handle.  <a href="#ga36df49080d9305767e96668e948a8f3b">More...</a><br /></td></tr>
<tr class="separator:ga36df49080d9305767e96668e948a8f3b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga0686487b4c80b6bd0d686fa1c08b4c8c"><td class="memItemLeft" align="right" valign="top">typedef void(*&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga0686487b4c80b6bd0d686fa1c08b4c8c">NN_OnRunDone</a>) (void *userData, <a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> errCode, void *outputTensor[], int32_t outputCount)</td></tr>
<tr class="memdesc:ga0686487b4c80b6bd0d686fa1c08b4c8c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the callback function handle for the post-process when the asynchronous execution has been done.  <a href="#ga0686487b4c80b6bd0d686fa1c08b4c8c">More...</a><br /></td></tr>
<tr class="separator:ga0686487b4c80b6bd0d686fa1c08b4c8c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga965dabbf39f75570925be38b8d0c8f76"><td class="memItemLeft" align="right" valign="top">typedef void(*&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga965dabbf39f75570925be38b8d0c8f76">NN_OnServiceDied</a>) (void *userData)</td></tr>
<tr class="memdesc:ga965dabbf39f75570925be38b8d0c8f76"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the callback function handle for the post-process when the device driver service is dead during asynchronous execution.  <a href="#ga965dabbf39f75570925be38b8d0c8f76">More...</a><br /></td></tr>
<tr class="separator:ga965dabbf39f75570925be38b8d0c8f76"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gadbfbf3d0f83e86f77d64f368fbbfe583"><td class="memItemLeft" align="right" valign="top">typedef struct <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gadbfbf3d0f83e86f77d64f368fbbfe583">OH_NN_UInt32Array</a></td></tr>
<tr class="memdesc:gadbfbf3d0f83e86f77d64f368fbbfe583"><td class="mdescLeft">&#160;</td><td class="mdescRight">This structure is used to store a 32-bit unsigned integer array.  <a href="#gadbfbf3d0f83e86f77d64f368fbbfe583">More...</a><br /></td></tr>
<tr class="separator:gadbfbf3d0f83e86f77d64f368fbbfe583"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga65d3909e6fb6f4964dda431a41fd48d8"><td class="memItemLeft" align="right" valign="top">typedef struct <a class="el" href="struct_o_h___n_n___quant_param.html">OH_NN_QuantParam</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga65d3909e6fb6f4964dda431a41fd48d8">OH_NN_QuantParam</a></td></tr>
<tr class="memdesc:ga65d3909e6fb6f4964dda431a41fd48d8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Quantization information.  <a href="#ga65d3909e6fb6f4964dda431a41fd48d8">More...</a><br /></td></tr>
<tr class="separator:ga65d3909e6fb6f4964dda431a41fd48d8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gac92614e1d0254fc38a1e9e2534429a18"><td class="memItemLeft" align="right" valign="top">typedef struct <a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gac92614e1d0254fc38a1e9e2534429a18">OH_NN_Tensor</a></td></tr>
<tr class="memdesc:gac92614e1d0254fc38a1e9e2534429a18"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the tensor structure.  <a href="#gac92614e1d0254fc38a1e9e2534429a18">More...</a><br /></td></tr>
<tr class="separator:gac92614e1d0254fc38a1e9e2534429a18"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga3f40d9d094f673cde61073770146f458"><td class="memItemLeft" align="right" valign="top">typedef struct <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga3f40d9d094f673cde61073770146f458">OH_NN_Memory</a></td></tr>
<tr class="memdesc:ga3f40d9d094f673cde61073770146f458"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the memory structure.  <a href="#ga3f40d9d094f673cde61073770146f458">More...</a><br /></td></tr>
<tr class="separator:ga3f40d9d094f673cde61073770146f458"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="enum-members"></a>
Enumerations</h2></td></tr>
<tr class="memitem:ga2ea6daeb95ee5bea81f97a5d9aeef5bf"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga2ea6daeb95ee5bea81f97a5d9aeef5bf">OH_NN_PerformanceMode</a> { <br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga2ea6daeb95ee5bea81f97a5d9aeef5bfa9d728aef494dfd8a589ac3478b5859b0">OH_NN_PERFORMANCE_NONE</a> = 0, 
<a class="el" href="group___neural_nework_runtime.html#gga2ea6daeb95ee5bea81f97a5d9aeef5bfa2858695128b273d26cf032c0c32b8c00">OH_NN_PERFORMANCE_LOW</a> = 1, 
<a class="el" href="group___neural_nework_runtime.html#gga2ea6daeb95ee5bea81f97a5d9aeef5bfafe262b0f5ab7ff4656a540a5a9357336">OH_NN_PERFORMANCE_MEDIUM</a> = 2, 
<a class="el" href="group___neural_nework_runtime.html#gga2ea6daeb95ee5bea81f97a5d9aeef5bfa3bb03471d6f47804d1bd6b935aceca4a">OH_NN_PERFORMANCE_HIGH</a> = 3, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga2ea6daeb95ee5bea81f97a5d9aeef5bfa85000e60fc542508a6292543286ea77f">OH_NN_PERFORMANCE_EXTREME</a> = 4
<br />
 }<tr class="memdesc:ga2ea6daeb95ee5bea81f97a5d9aeef5bf"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the hardware performance mode.  <a href="group___neural_nework_runtime.html#ga2ea6daeb95ee5bea81f97a5d9aeef5bf">More...</a><br /></td></tr>
</td></tr>
<tr class="separator:ga2ea6daeb95ee5bea81f97a5d9aeef5bf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga5ae0ed09b49e98f608f47ac4fe9ecb07"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga5ae0ed09b49e98f608f47ac4fe9ecb07">OH_NN_Priority</a> { <a class="el" href="group___neural_nework_runtime.html#gga5ae0ed09b49e98f608f47ac4fe9ecb07a2078f6a415dd21a0128899cc97a9d010">OH_NN_PRIORITY_NONE</a> = 0, 
<a class="el" href="group___neural_nework_runtime.html#gga5ae0ed09b49e98f608f47ac4fe9ecb07a846f3f7a47504a6069480e4c651060a4">OH_NN_PRIORITY_LOW</a> = 1, 
<a class="el" href="group___neural_nework_runtime.html#gga5ae0ed09b49e98f608f47ac4fe9ecb07a664a335e3b0cb5d3267f90a8a2a001c4">OH_NN_PRIORITY_MEDIUM</a> = 2, 
<a class="el" href="group___neural_nework_runtime.html#gga5ae0ed09b49e98f608f47ac4fe9ecb07a3fcbc2b29deaad3d06dd9601f97555d4">OH_NN_PRIORITY_HIGH</a> = 3
 }<tr class="memdesc:ga5ae0ed09b49e98f608f47ac4fe9ecb07"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the model inference task priority.  <a href="group___neural_nework_runtime.html#ga5ae0ed09b49e98f608f47ac4fe9ecb07">More...</a><br /></td></tr>
</td></tr>
<tr class="separator:ga5ae0ed09b49e98f608f47ac4fe9ecb07"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga28c9d17051cc5833d0b749c0b61e1823"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> { <br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> = 0, 
<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae480781f8f23d040a2c8aad3c170783e">OH_NN_FAILED</a> = 1, 
<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> = 2, 
<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a91a3c5cc6c688cc3feaace95d199e249">OH_NN_MEMORY_ERROR</a> = 3, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> = 4, 
<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ac086f7e028ece986f71cbcd0a8fa91e9">OH_NN_NULL_PTR</a> = 5, 
<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ace52583ce1eccad5a5862f4771818cdf">OH_NN_INVALID_FILE</a> = 6, 
<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a59a096979dfc59e124f7889012bbf233">OH_NN_UNAVALIDABLE_DEVICE</a> = 7, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa7571ab718fe691536f700ce17aa77cf">OH_NN_INVALID_PATH</a> = 8, 
<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a19543b3420787458575cc04c9c582731">OH_NN_TIMEOUT</a> = 9, 
<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae6455b906d5a7aee0597798f7a00c529">OH_NN_UNSUPPORTED</a> = 10, 
<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a6fad4fd55f0d1e9fc1002deb4778769e">OH_NN_CONNECTION_EXCEPTION</a> = 11, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a9d8db5dcdeef28ecdec97aa10aaf6001">OH_NN_SAVE_CACHE_EXCEPTION</a> = 12, 
<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a6d639b026c18ecc858873d1cb45f41eb">OH_NN_DYNAMIC_SHAPE</a> = 13, 
<a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a11ec75439c1cbab1778f0f84d21dc311">OH_NN_UNAVAILABLE_DEVICE</a> = 14
<br />
 }<tr class="memdesc:ga28c9d17051cc5833d0b749c0b61e1823"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines error codes.  <a href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">More...</a><br /></td></tr>
</td></tr>
<tr class="separator:ga28c9d17051cc5833d0b749c0b61e1823"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga22fc23735fd12894d547805d320e01e3"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga22fc23735fd12894d547805d320e01e3">OH_NN_FuseType</a> : int8_t { <a class="el" href="group___neural_nework_runtime.html#gga22fc23735fd12894d547805d320e01e3a58dc2d253afa491754e3e2a13423b3fa">OH_NN_FUSED_NONE</a> = 0, 
<a class="el" href="group___neural_nework_runtime.html#gga22fc23735fd12894d547805d320e01e3a87d4e0c86baf35ff90d4ca2b47e4e3de">OH_NN_FUSED_RELU</a> = 1, 
<a class="el" href="group___neural_nework_runtime.html#gga22fc23735fd12894d547805d320e01e3abac42b4176ce6a8e293e71d34d6cb54a">OH_NN_FUSED_RELU6</a> = 2
 }<tr class="memdesc:ga22fc23735fd12894d547805d320e01e3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines activation function types in the fusion operator.  <a href="group___neural_nework_runtime.html#ga22fc23735fd12894d547805d320e01e3">More...</a><br /></td></tr>
</td></tr>
<tr class="separator:ga22fc23735fd12894d547805d320e01e3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga467f4b3c524b8ec4ebf3c86d680f7f59"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga467f4b3c524b8ec4ebf3c86d680f7f59">OH_NN_Format</a> { <a class="el" href="group___neural_nework_runtime.html#gga467f4b3c524b8ec4ebf3c86d680f7f59ae4c684f3bfbcd02e944326f09ab9c278">OH_NN_FORMAT_NONE</a> = 0, 
<a class="el" href="group___neural_nework_runtime.html#gga467f4b3c524b8ec4ebf3c86d680f7f59a9c530242beecb143c39419ccd6f52287">OH_NN_FORMAT_NCHW</a> = 1, 
<a class="el" href="group___neural_nework_runtime.html#gga467f4b3c524b8ec4ebf3c86d680f7f59a858b34abff99a34d4c1cd38565ab92bd">OH_NN_FORMAT_NHWC</a> = 2, 
<a class="el" href="group___neural_nework_runtime.html#gga467f4b3c524b8ec4ebf3c86d680f7f59ac446cc4ddb5cc377a0fdbeadf68f4bf8">OH_NN_FORMAT_ND</a> = 3
 }<tr class="memdesc:ga467f4b3c524b8ec4ebf3c86d680f7f59"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines the layout type of tensor data.  <a href="group___neural_nework_runtime.html#ga467f4b3c524b8ec4ebf3c86d680f7f59">More...</a><br /></td></tr>
</td></tr>
<tr class="separator:ga467f4b3c524b8ec4ebf3c86d680f7f59"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gae99ff81dfe03a24f714261009b5d2bff"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gae99ff81dfe03a24f714261009b5d2bff">OH_NN_DeviceType</a> { <a class="el" href="group___neural_nework_runtime.html#ggae99ff81dfe03a24f714261009b5d2bffa2b0a4607cc69f1f18fb4279eb8693646">OH_NN_OTHERS</a> = 0, 
<a class="el" href="group___neural_nework_runtime.html#ggae99ff81dfe03a24f714261009b5d2bffac26cc84ea920f877dff72fad57eb5240">OH_NN_CPU</a> = 1, 
<a class="el" href="group___neural_nework_runtime.html#ggae99ff81dfe03a24f714261009b5d2bffa499d4c3f35ea7bed3e45591675697bd7">OH_NN_GPU</a> = 2, 
<a class="el" href="group___neural_nework_runtime.html#ggae99ff81dfe03a24f714261009b5d2bffab2e97cc2878a8915f7e9eef4e29212de">OH_NN_ACCELERATOR</a> = 3
 }<tr class="memdesc:gae99ff81dfe03a24f714261009b5d2bff"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines device types.  <a href="group___neural_nework_runtime.html#gae99ff81dfe03a24f714261009b5d2bff">More...</a><br /></td></tr>
</td></tr>
<tr class="separator:gae99ff81dfe03a24f714261009b5d2bff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga917dcf1cb1e7bb745ccf984e9e67940e"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga917dcf1cb1e7bb745ccf984e9e67940e">OH_NN_DataType</a> { <br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940ea28ff687c508d5623d10fd24fbdbd7cf6">OH_NN_UNKNOWN</a> = 0, 
<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940eabf6e9c671b9d6b8cd276067f4384dca4">OH_NN_BOOL</a> = 1, 
<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940ea3f82397050b6a4a3928798105985a505">OH_NN_INT8</a> = 2, 
<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940ea18a1886aaac5985b294cc58c9d8d14bb">OH_NN_INT16</a> = 3, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940ea750f391a55c5d8d90b98af6f61dd3ee2">OH_NN_INT32</a> = 4, 
<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940eadd0f78fb792e38ca2a2e9eaa8b15ca40">OH_NN_INT64</a> = 5, 
<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940ea2a00cccefe879051f75b781c16b89e4e">OH_NN_UINT8</a> = 6, 
<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940ea66ebbf01c9ac3ca9760ae0824fa87d67">OH_NN_UINT16</a> = 7, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940ea516228d1c155e6c302f067d11416ab14">OH_NN_UINT32</a> = 8, 
<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940ea370a35d2fa1b261871e166e9562c30e8">OH_NN_UINT64</a> = 9, 
<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940ea892d3a48eec5855cfa17fd4f95fe078e">OH_NN_FLOAT16</a> = 10, 
<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940ea33a85906dc39ca7b7987986fea772770">OH_NN_FLOAT32</a> = 11, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga917dcf1cb1e7bb745ccf984e9e67940eac9b9729bd0b4120cb181fa6223c899c6">OH_NN_FLOAT64</a> = 12
<br />
 }<tr class="memdesc:ga917dcf1cb1e7bb745ccf984e9e67940e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines tensor data types.  <a href="group___neural_nework_runtime.html#ga917dcf1cb1e7bb745ccf984e9e67940e">More...</a><br /></td></tr>
</td></tr>
<tr class="separator:ga917dcf1cb1e7bb745ccf984e9e67940e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gab333095762fd39a9f952523d717b95de"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gab333095762fd39a9f952523d717b95de">OH_NN_OperationType</a> { <br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deaf9287433d92cc8e3277f14a775a2a284">OH_NN_OPS_ADD</a> = 1, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea06f5fae28fb297882a43beab9176f1bc">OH_NN_OPS_AVG_POOL</a> = 2, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deaf725e0f19333d7f2979c57a91816f0ce">OH_NN_OPS_BATCH_NORM</a> = 3, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deab7710e95a5742143cea3eaf132e2cb53">OH_NN_OPS_BATCH_TO_SPACE_ND</a> = 4, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deabe2bf16f2542ee10f05e6a4f0f37c324">OH_NN_OPS_BIAS_ADD</a> = 5, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea53beaecf4c2d785ee03b868645d34cc4">OH_NN_OPS_CAST</a> = 6, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deaf037dab1e691c9f59e5745958da982a7">OH_NN_OPS_CONCAT</a> = 7, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea431b2053449cef3c3418869b0f801301">OH_NN_OPS_CONV2D</a> = 8, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea39b5f3514d5064d6f97d0b73d0fa80ce">OH_NN_OPS_CONV2D_TRANSPOSE</a> = 9, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea17d3f84d4fbb36da41c09e00205621b1">OH_NN_OPS_DEPTHWISE_CONV2D_NATIVE</a> = 10, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea422ebc8802510175df7d3afd499c8f2b">OH_NN_OPS_DIV</a> = 11, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea704612af75062b022553433eb1a69cac">OH_NN_OPS_ELTWISE</a> = 12, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea9fa75bc04f31a73255cf907d5f7cbfff">OH_NN_OPS_EXPAND_DIMS</a> = 13, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea1e42979e38d9c8e77302ca9204a1940f">OH_NN_OPS_FILL</a> = 14, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deab4b9fc3159f612337a7e03d671e30e58">OH_NN_OPS_FULL_CONNECTION</a> = 15, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea9c2481b53a707fffbddb81a4925d1d0b">OH_NN_OPS_GATHER</a> = 16, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deafde613b42930bbef3ebb9360feaa8a2b">OH_NN_OPS_HSWISH</a> = 17, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea5e7d9e12b6bb8c24d246d9434dbc065d">OH_NN_OPS_LESS_EQUAL</a> = 18, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea03082b437d4d81c2dac5d6a51e6f7ece">OH_NN_OPS_MATMUL</a> = 19, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea7d882ff508634ee88d8dd53b6fca7209">OH_NN_OPS_MAXIMUM</a> = 20, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deabb2448db4cca23082694cd1ef14a9580">OH_NN_OPS_MAX_POOL</a> = 21, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deac661eb9ec8a6cbf05bf7637040f31213">OH_NN_OPS_MUL</a> = 22, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deab754be46b448538f1ecb54fa1d1ef14f">OH_NN_OPS_ONE_HOT</a> = 23, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deaad110b33f690e01428552db45656516c">OH_NN_OPS_PAD</a> = 24, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea6f4738d88207a358a789312813907dee">OH_NN_OPS_POW</a> = 25, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deab6653766d548fdcd4021362ab0323049">OH_NN_OPS_SCALE</a> = 26, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deacf1aa6c15fbb881835bf5c25123d1bac">OH_NN_OPS_SHAPE</a> = 27, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deaeac8384bd9cd516fab8a6d3fe59ef2ee">OH_NN_OPS_SIGMOID</a> = 28, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea1ff4a90fe0a5a60e586b77142202b537">OH_NN_OPS_SLICE</a> = 29, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea5e423349b5500197c576620264fa510d">OH_NN_OPS_SOFTMAX</a> = 30, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea3c0c046024d21cc6e4f4087fe5a36e05">OH_NN_OPS_SPACE_TO_BATCH_ND</a> = 31, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deaa7996e244b633278573fa9939df5aa2a">OH_NN_OPS_SPLIT</a> = 32, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea554b87de298fcb4f05ebbdc96508d521">OH_NN_OPS_SQRT</a> = 33, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea1d2c4306c8142686b96a20c6b6be2527">OH_NN_OPS_SQUARED_DIFFERENCE</a> = 34, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deab368b8662191cf287bb2f6c17bb0dcdc">OH_NN_OPS_SQUEEZE</a> = 35, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea656d8434c662ae650173a417e90f5d0e">OH_NN_OPS_STACK</a> = 36, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea2f4db1e8c0aa8d852fd82eda12610a04">OH_NN_OPS_STRIDED_SLICE</a> = 37, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea850ad3b59f1240b210417a830156ea41">OH_NN_OPS_SUB</a> = 38, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deab902d02d054a12f1c1ca5a7c0fc20f71">OH_NN_OPS_TANH</a> = 39, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea0a9bb564a7014abe9e14f9da4fc3c54e">OH_NN_OPS_TILE</a> = 40, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea31e8d44adb98d4d7e9b44422ad5bbaf0">OH_NN_OPS_TRANSPOSE</a> = 41, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deab7d7dd01522a3f36aa994a3cd62f38ed">OH_NN_OPS_REDUCE_MEAN</a> = 42, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deac3cd18027ac5576a5e5a0b93f6c2c59b">OH_NN_OPS_RESIZE_BILINEAR</a> = 43, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea4522e96ba3dc57132a9935c1cc2ba8a8">OH_NN_OPS_RSQRT</a> = 44, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deaf6c4b78e0f04152120bc5d679e5c7d75">OH_NN_OPS_RESHAPE</a> = 45, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea2113f865b48d80611fd7b508ff6f3952">OH_NN_OPS_PRELU</a> = 46, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea26ea242c2f0811052b527eb49c379833">OH_NN_OPS_RELU</a> = 47, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea2b8960a62f1473807f1e8916eb9999bb">OH_NN_OPS_RELU6</a> = 48, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea4a58aaac186feded06a7317ad5e347d0">OH_NN_OPS_LAYER_NORM</a> = 49, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea8ca8125647631398fff7da0e458cf38e">OH_NN_OPS_REDUCE_PROD</a> = 50, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea2d08952c0eaedede78cdb71f309f5d6c">OH_NN_OPS_REDUCE_ALL</a> = 51, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea676b1a36bc327d99c8abe59ab03ce466">OH_NN_OPS_QUANT_DTYPE_CAST</a> = 52, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea1f9d7b90110c78bc566c05ff53b26f27">OH_NN_OPS_TOP_K</a> = 53, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deae756ab9ec2d04092118396aaae9b186c">OH_NN_OPS_ARG_MAX</a> = 54, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea3f608d999d582d4edc7162d26a300918">OH_NN_OPS_UNSQUEEZE</a> = 55, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea2adde59b09a21d7337b1e6deec71ade1">OH_NN_OPS_GELU</a> = 56, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea81e40d8318f5101a69dabbb4cb91f444">OH_NN_OPS_UNSTACK</a> = 57, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deab224429da785feb1521bf7483eacbe02">OH_NN_OPS_ABS</a> = 58, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea3dce1426a2e7bc733543ab0e44b1a24d">OH_NN_OPS_ERF</a> = 59, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deaf0ce7a833833f92563b5988e3736afa8">OH_NN_OPS_EXP</a> = 60, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deaf783a922f1163671b5a9a5adc2edb269">OH_NN_OPS_LESS</a> = 61, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea316d7588ad17eb88223e6002e093e855">OH_NN_OPS_SELECT</a> = 62, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea67c0cf3ae57ecb45342fc481e9810dfb">OH_NN_OPS_SQUARE</a> = 63, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deab1d06b4c6161e8559ca2b4ce7937fb2a">OH_NN_OPS_FLATTEN</a> = 64, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea894ed2c4174bfa52275f05c224921b19">OH_NN_OPS_DEPTH_TO_SPACE</a> = 65, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea5ff67209ba763c4e04762efd93b50ffc">OH_NN_OPS_RANGE</a> = 66, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea879b813061be78ee3d5567864896d435">OH_NN_OPS_INSTANCE_NORM</a> = 67, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea91aa949d6e5e302924b8764f72a36395">OH_NN_OPS_CONSTANT_OF_SHAPE</a> = 68, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea5c5b2ae2710dde83371bf1ca5aa3d957">OH_NN_OPS_BROADCAST_TO</a> = 69, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea2ea90a1553227d76a0f52d2a6e979609">OH_NN_OPS_EQUAL</a> = 70, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea2406374d9705ac1425e9edbcc062bf24">OH_NN_OPS_GREATER</a> = 71, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deaece76284d811ca9f303d0b6469af3402">OH_NN_OPS_NOT_EQUAL</a> = 72, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea74135623a09dda73bb256d50eec3e5e7">OH_NN_OPS_GREATER_EQUAL</a> = 73, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea4e8ccb226300ebd2674dfa13beb0f96a">OH_NN_OPS_LEAKY_RELU</a> = 74, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deae486d8a64ea1234c8271040ec2d1c063">OH_NN_OPS_LSTM</a> = 75, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea1e38ae38e1e1149025773ed2366c2844">OH_NN_OPS_CLIP</a> = 76, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deab4392cc281241fc9beb857a32bf0f2f8">OH_NN_OPS_ALL</a> = 77, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea5b861e7c91cd2295284de9aa7226681a">OH_NN_OPS_ASSERT</a> = 78, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deab75d745f70ec4718c2d641d4118b8f31">OH_NN_OPS_COS</a> = 79, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea28df3f7950598076a0003e7bbc6d1c81">OH_NN_OPS_LOG</a> = 80, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea4ad52cba16dd8633620fcf1b5634c284">OH_NN_OPS_LOGICAL_AND</a> = 81, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea3e533201e7633edcdaa84b83193aad8f">OH_NN_OPS_LOGICAL_NOT</a> = 82, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea10a15aaff29f9fffdf59078b6429ae6f">OH_NN_OPS_MOD</a> = 83, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deabc071aba6a212709ffbeae96c317f625">OH_NN_OPS_NEG</a> = 84, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea3053398359438ad92ba31178f4b2d5ea">OH_NN_OPS_RECIPROCAL</a> = 85, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea053f68776536beaff6c54c4bdb286c3b">OH_NN_OPS_SIN</a> = 86, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea53404933d36e88c97bb8e4d7f91ba212">OH_NN_OPS_WHERE</a> = 87, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea75f22874793012fca98a91b4ec61d85c">OH_NN_OPS_SPARSE_TO_DENSE</a> = 88, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deaf48926f0930f019c4c9b671b609e704c">OH_NN_OPS_LOGICAL_OR</a> = 89, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea7bf465e9e10482e7861a1322fa237129">OH_NN_OPS_CEIL</a> = 90, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea4d5dfe0d4c04430e11f4885f5c65b166">OH_NN_OPS_CROP</a> = 91, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea133a9eac85573695c3927c991e67cee0">OH_NN_OPS_DETECTION_POST_PROCESS</a> = 92, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea889379ecfcf070c8817b87f2dd4c4fba">OH_NN_OPS_FLOOR</a> = 93, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deabc765ea026a218e60a400d7431e0e723">OH_NN_OPS_L2_NORMALIZE</a> = 94, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea881e9adeb2585baff3fe58933f252846">OH_NN_OPS_LOG_SOFTMAX</a> = 95, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea27287d3c4fdf83b24cd0486a36c6e274">OH_NN_OPS_LRN</a> = 96, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deab7993f0c9d71a1956ed6ad9c5051d75e">OH_NN_OPS_MINIMUM</a> = 97, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea220384793dd8a373554e36ccc7377169">OH_NN_OPS_RANK</a> = 98, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea90d68f031e655e2c73f1c48bde920ece">OH_NN_OPS_REDUCE_MAX</a> = 99, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea6b0b520f0e6fcd339bc4d7d8569d7d37">OH_NN_OPS_REDUCE_MIN</a> = 100, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deadae5d32a26226b338fabe50e8dda7ee5">OH_NN_OPS_REDUCE_SUM</a> = 101, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deacac1293b1b712b627d2bd26ef433aec3">OH_NN_OPS_ROUND</a> = 102, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dead2095b92aea9eb7b4b0f1a82481ea9cf">OH_NN_OPS_SCATTER_ND</a> = 103, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95deaf3682e2e8e1d4ab665cd45c0e622c4a3">OH_NN_OPS_SPACE_TO_DEPTH</a> = 104, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea3c85d8950be9632309b9807c2cc799f0">OH_NN_OPS_SWISH</a> = 105, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea84aa2a1b11c6018c5f24097ad773f125">OH_NN_OPS_REDUCE_L2</a> = 106, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea9125748be37320c2ac38aabef2160f93">OH_NN_OPS_HARD_SIGMOID</a> = 107, 
<a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea637c1e17ebfec5db5f709b4bb7fac2d6">OH_NN_OPS_GATHER_ND</a> = 108
<br />
 }<tr class="memdesc:gab333095762fd39a9f952523d717b95de"><td class="mdescLeft">&#160;</td><td class="mdescRight">Defines operator types.  <a href="group___neural_nework_runtime.html#gab333095762fd39a9f952523d717b95de">More...</a><br /></td></tr>
</td></tr>
<tr class="separator:gab333095762fd39a9f952523d717b95de"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga09f35be6e0f86f5d747192ce5812552f"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga09f35be6e0f86f5d747192ce5812552f">OH_NN_TensorType</a> { <br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa4e38ff084622d59af25f5848e83f39a0">OH_NN_TENSOR</a> = 0, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa4a42f1f0dbd0ce289619a5490bc7b52a">OH_NN_ADD_ACTIVATIONTYPE</a> = 1, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa6caac3c689ccee4ed4f270e3e7749044">OH_NN_AVG_POOL_KERNEL_SIZE</a> = 2, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa5c4af15b860d9a7bdcaf7dc4106b8775">OH_NN_AVG_POOL_STRIDE</a> = 3, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fae0ad0acadc379b29465efe7b4fe5f459">OH_NN_AVG_POOL_PAD_MODE</a> = 4, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa1539a2b9b69f53f07dce0e7f55ad9e5c">OH_NN_AVG_POOL_PAD</a> = 5, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa1251a336baf8b38f59a2ba99568d110f">OH_NN_AVG_POOL_ACTIVATION_TYPE</a> = 6, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa06c4346b3d7f29384320d049cd17757f">OH_NN_BATCH_NORM_EPSILON</a> = 7, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa76ba858fb9baaa042e94d1df68b63a9f">OH_NN_BATCH_TO_SPACE_ND_BLOCKSIZE</a> = 8, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faca2869d20c6a1a4ba454af6e8c75c5be">OH_NN_BATCH_TO_SPACE_ND_CROPS</a> = 9, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faae4d88b620ec7483949d1a2424b3a2de">OH_NN_CONCAT_AXIS</a> = 10, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fad383864eeeba5a543f7584608ebbfd05">OH_NN_CONV2D_STRIDES</a> = 11, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa961c65f904f390cec7d56950872e1782">OH_NN_CONV2D_PAD</a> = 12, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa4c9e4304c38664c9816c4d24b336bc9a">OH_NN_CONV2D_DILATION</a> = 13, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa7db5636f8757016324dd5c6bf09d5a0d">OH_NN_CONV2D_PAD_MODE</a> = 14, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa077d066af1b8dc2ef4bf8c4565d33617">OH_NN_CONV2D_ACTIVATION_TYPE</a> = 15, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fabc24a47774affcfc2bee3120a9876f53">OH_NN_CONV2D_GROUP</a> = 16, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa9f6c005964e33857fd7620160ed2cd9c">OH_NN_CONV2D_TRANSPOSE_STRIDES</a> = 17, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa814f92059d6378ed98e5fb9e08ec5303">OH_NN_CONV2D_TRANSPOSE_PAD</a> = 18, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fade462cb51f4c2233f2d7ec9c87c91f4c">OH_NN_CONV2D_TRANSPOSE_DILATION</a> = 19, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa325f1958a7297e1898214ed10bb22180">OH_NN_CONV2D_TRANSPOSE_OUTPUT_PADDINGS</a> = 20, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fac50b0792a10063bde3382abc3d8d6900">OH_NN_CONV2D_TRANSPOSE_PAD_MODE</a> = 21, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa8bb021aba1428b6d738883e67ad579c4">OH_NN_CONV2D_TRANSPOSE_ACTIVATION_TYPE</a> = 22, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa77275720bf3bf98d99afecce8d98acc2">OH_NN_CONV2D_TRANSPOSE_GROUP</a> = 23, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faf5453c22b717752b433e95c7057d6b8a">OH_NN_DEPTHWISE_CONV2D_NATIVE_STRIDES</a> = 24, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fac48a51dbbe536f08d7e208cb81823851">OH_NN_DEPTHWISE_CONV2D_NATIVE_PAD</a> = 25, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa6c6858f128705fe13be1d9a28b2c2adb">OH_NN_DEPTHWISE_CONV2D_NATIVE_DILATION</a> = 26, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fad64df7c9202e74fd775ff2ad4cf13037">OH_NN_DEPTHWISE_CONV2D_NATIVE_PAD_MODE</a> = 27, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa6d7a5deab8d2405c06c2e8aa53d4cddb">OH_NN_DEPTHWISE_CONV2D_NATIVE_ACTIVATION_TYPE</a> = 28, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faa64dd2e1b97a9308964d86d87bc88942">OH_NN_DIV_ACTIVATIONTYPE</a> = 29, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa8f38940361c1a0927beb0972680e89f2">OH_NN_ELTWISE_MODE</a> = 30, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa684df8d50dc404c4765a34f260f2913d">OH_NN_FULL_CONNECTION_AXIS</a> = 31, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fab8e993fb6f8b86b50ae645228c78d4f8">OH_NN_FULL_CONNECTION_ACTIVATIONTYPE</a> = 32, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faae2cefdf33074023b89bdbf4d5bac01a">OH_NN_MATMUL_TRANSPOSE_A</a> = 33, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faadb7865daa2c5c26db2ee943c1e32bcb">OH_NN_MATMUL_TRANSPOSE_B</a> = 34, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa786e09ecd4c02fe21f654c2e36252286">OH_NN_MATMUL_ACTIVATION_TYPE</a> = 35, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faf3c395182e18bebd90d3c8dabd0eeffd">OH_NN_MAX_POOL_KERNEL_SIZE</a> = 36, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fab0db1ab02f7b287dac2a01d8ae4eeb08">OH_NN_MAX_POOL_STRIDE</a> = 37, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa8e59f925917331c82394acfd87c70da5">OH_NN_MAX_POOL_PAD_MODE</a> = 38, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faf79aec78b0886bf9cdd7535b47164120">OH_NN_MAX_POOL_PAD</a> = 39, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fafd6132e48a929bf69252ad280e14e6bc">OH_NN_MAX_POOL_ACTIVATION_TYPE</a> = 40, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fae216424221f98aa3552b33dab193e53b">OH_NN_MUL_ACTIVATION_TYPE</a> = 41, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa8544ab5b48a1f56785d04618d385a0c2">OH_NN_ONE_HOT_AXIS</a> = 42, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fad031c13179b85e1b97fefe0ea7ffb0f2">OH_NN_PAD_CONSTANT_VALUE</a> = 43, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fac1f5a9b9277efd997a26b978203c9a47">OH_NN_SCALE_ACTIVATIONTYPE</a> = 44, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa5fe5b3428962e06ef17c1ad788e4b979">OH_NN_SCALE_AXIS</a> = 45, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faa758d60852597b99c5ffb7ed1f62bdd3">OH_NN_SOFTMAX_AXIS</a> = 46, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa6aeffbfb92523d8fcbf7e1bd0237846b">OH_NN_SPACE_TO_BATCH_ND_BLOCK_SHAPE</a> = 47, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faeef9918b19afc9a8622917067837d398">OH_NN_SPACE_TO_BATCH_ND_PADDINGS</a> = 48, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fac53187231243129bd50e4b4abe0ec2bc">OH_NN_SPLIT_AXIS</a> = 49, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa1c5262397b2f6ff7196383a4ffa3d0c3">OH_NN_SPLIT_OUTPUT_NUM</a> = 50, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa083159d4fe01da04c6a5b29e9f2253b1">OH_NN_SPLIT_SIZE_SPLITS</a> = 51, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa84706349aad1ab066c623a26e4ee8bd6">OH_NN_SQUEEZE_AXIS</a> = 52, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa90a700327f96dfa3674a2c38d8fcb1b2">OH_NN_STACK_AXIS</a> = 53, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa6fdd8103ab19f1991f9b95418c16d065">OH_NN_STRIDED_SLICE_BEGIN_MASK</a> = 54, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fab649342189849871c39e83f51332869b">OH_NN_STRIDED_SLICE_END_MASK</a> = 55, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa345d817f8cbc8eb740632eb34c322f6e">OH_NN_STRIDED_SLICE_ELLIPSIS_MASK</a> = 56, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa5a2e97fd31352ec64f456658b87a30b4">OH_NN_STRIDED_SLICE_NEW_AXIS_MASK</a> = 57, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faf10d9e7e0219d380273b70ea27fda2d2">OH_NN_STRIDED_SLICE_SHRINK_AXIS_MASK</a> = 58, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa9c07bc3353943f62c98b2e98040797ad">OH_NN_SUB_ACTIVATIONTYPE</a> = 59, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa01b8d0365b076ef8886b760c04c70389">OH_NN_REDUCE_MEAN_KEEP_DIMS</a> = 60, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa6e3bb04c940b0554f23c5e2645d900b7">OH_NN_RESIZE_BILINEAR_NEW_HEIGHT</a> = 61, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa2160838654c8ca289b3b7c07b51dc386">OH_NN_RESIZE_BILINEAR_NEW_WIDTH</a> = 62, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fad3ed74571e0336b4ced6c858676a5fc7">OH_NN_RESIZE_BILINEAR_PRESERVE_ASPECT_RATIO</a> = 63, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faebe8182e06053a7e7e2ed70c41e34a94">OH_NN_RESIZE_BILINEAR_COORDINATE_TRANSFORM_MODE</a> = 64, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa677799dfd1d5214fc92807c257bf63c9">OH_NN_RESIZE_BILINEAR_EXCLUDE_OUTSIDE</a> = 65, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa54fa58c67a02b6727e7387c1be0c88ac">OH_NN_LAYER_NORM_BEGIN_NORM_AXIS</a> = 66, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa722eee0a2cd4a100b118fac72fb764ed">OH_NN_LAYER_NORM_EPSILON</a> = 67, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa3117fdc4705d25456b28abc38fabf7b6">OH_NN_LAYER_NORM_BEGIN_PARAM_AXIS</a> = 68, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa1460b7d5e051afd08c917c42571a9fe3">OH_NN_LAYER_NORM_ELEMENTWISE_AFFINE</a> = 69, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa5be242c91e91da6dc92e1e6ced0d3a59">OH_NN_REDUCE_PROD_KEEP_DIMS</a> = 70, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fabd9cf8d556af406e8cde57af87c26dfa">OH_NN_REDUCE_ALL_KEEP_DIMS</a> = 71, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa778049e9e5e3186df34d2acb997ca392">OH_NN_QUANT_DTYPE_CAST_SRC_T</a> = 72, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fad8dc93d0feddec113c3d66fe399fbe8a">OH_NN_QUANT_DTYPE_CAST_DST_T</a> = 73, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa0987dfd85d36528ea119885e1cb8c25b">OH_NN_TOP_K_SORTED</a> = 74, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa1522ea5621075974be8a785359fab133">OH_NN_ARG_MAX_AXIS</a> = 75, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faf9baeea3995403a7c0d3a4865a52aae4">OH_NN_ARG_MAX_KEEPDIMS</a> = 76, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fabacb331bf515a3e2d8591a10da1aed3b">OH_NN_UNSQUEEZE_AXIS</a> = 77, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa84dc14c2fc97ad7ff56ac4fd0961df8e">OH_NN_UNSTACK_AXIS</a> = 78, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fafdf7f13517ca31556fc5f2441861de02">OH_NN_FLATTEN_AXIS</a> = 79, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa5516397d37a3ca5d35fa4bac4b1206de">OH_NN_DEPTH_TO_SPACE_BLOCK_SIZE</a> = 80, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faf57e2973efb5fe76af2c9d4b2a6e993d">OH_NN_DEPTH_TO_SPACE_MODE</a> = 81, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fab2dd3c8f2d253ba1dfc4c7a79f6dc057">OH_NN_RANGE_START</a> = 82, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa3f473820d614c41d36f61b24c817b4e1">OH_NN_RANGE_LIMIT</a> = 83, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa7ec77584f32736f27fb792b793e6a623">OH_NN_RANGE_DELTA</a> = 84, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa01640410499014b7b34f45e7b2e6892b">OH_NN_CONSTANT_OF_SHAPE_DATA_TYPE</a> = 85, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa148c32aa1ba2b217d3edfcdecc8a5a1b">OH_NN_CONSTANT_OF_SHAPE_VALUE</a> = 86, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fac40a99706ce1c8445bf30d05255b24f6">OH_NN_BROADCAST_TO_SHAPE</a> = 87, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fab6ef9d3b609ba4ad99179ba41a19a93c">OH_NN_INSTANCE_NORM_EPSILON</a> = 88, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa9e54b75a88e957e050435bb554796437">OH_NN_EXP_BASE</a> = 89, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa778f0d14ad8871aa49165639735aac0a">OH_NN_EXP_SCALE</a> = 90, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fafa656443a1825871dfd44f4cc9e003a8">OH_NN_EXP_SHIFT</a> = 91, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa13c7da6d3eb243a59ab46e03f0e28c6f">OH_NN_LEAKY_RELU_NEGATIVE_SLOPE</a> = 92, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa467fa64e92a51de2aa0480db193e7ee1">OH_NN_LSTM_BIDIRECTIONAL</a> = 93, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552facd799dd4615f12d74d6a68a30cf919e7">OH_NN_LSTM_HAS_BIAS</a> = 94, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fac779214b8b514eef78c03a7bdb7d2850">OH_NN_LSTM_INPUT_SIZE</a> = 95, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faa935d8f7dd6c797f68cf4f61b9a865d4">OH_NN_LSTM_HIDDEN_SIZE</a> = 96, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552facbc3b728de803a2ce1a261bca6f0d812">OH_NN_LSTM_NUM_LAYERS</a> = 97, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552facf791f644ff5050b80e45f5d79081e80">OH_NN_LSTM_NUM_DIRECTIONS</a> = 98, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa0d27b7134c622df9ec79b3677a13b32c">OH_NN_LSTM_DROPOUT</a> = 99, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fadd693416a6698f13e031df58bf1f6d64">OH_NN_LSTM_ZONEOUT_CELL</a> = 100, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faa70af3c9eefa9940f4fe979d3bd8e82f">OH_NN_LSTM_ZONEOUT_HIDDEN</a> = 101, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faec667a632feda15b9aea4d8774945364">OH_NN_LSTM_PROJ_SIZE</a> = 102, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa7af7df1228bcf40872f57bb82396c2af">OH_NN_CLIP_MAX</a> = 103, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faaaa0c94d18bbdfaff68794631b04068c">OH_NN_CLIP_MIN</a> = 104, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa9e4dc6269e795bfda927fe1524ac23d2">OH_NN_ALL_KEEP_DIMS</a> = 105, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa9dbe2cea7a27aa2d093ba5a8a7c938c1">OH_NN_ASSERT_SUMMARIZE</a> = 106, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa1140d3ea38546cedf673357594d7e938">OH_NN_POW_SCALE</a> = 107, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fab2da80754a4f94855240e30943e218c4">OH_NN_POW_SHIFT</a> = 108, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fafcd23485034dc87e7747e78c54616c8d">OH_NN_AVG_POOL_ROUND_MODE</a> = 109, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa9a63eb8abc9b5f89e7577e581080180f">OH_NN_AVG_POOL_GLOBAL</a> = 110, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fac6425cf07bbf0ac36ef186cd5901c94d">OH_NN_FULL_CONNECTION_HAS_BIAS</a> = 111, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa2126c65cb84bd9f8e04c8a961a991109">OH_NN_FULL_CONNECTION_USE_AXIS</a> = 112, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa49059abb10734ad97fd426db8a49bc9c">OH_NN_GELU_APPROXIMATE</a> = 113, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa3a82af47fe67b85238ca0bad1e978ae4">OH_NN_MAX_POOL_ROUND_MODE</a> = 114, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa9193e6136ebc5f9232ec9cf688880392">OH_NN_MAX_POOL_GLOBAL</a> = 115, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fafc24db6b2d11b71f2b7c611f010e2436">OH_NN_PAD_PADDING_MODE</a> = 116, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa8987c80020a86b9863d9b1cd6893cc06">OH_NN_REDUCE_MEAN_REDUCE_TO_END</a> = 117, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa90a1945b7892a3515f5c28256ed606ff">OH_NN_REDUCE_MEAN_COEFF</a> = 118, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa1fb9c6cdcb280e40ddc111bd457a13b3">OH_NN_REDUCE_PROD_REDUCE_TO_END</a> = 119, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa60b0a4b69d881fb1f3084c25c72cf817">OH_NN_REDUCE_PROD_COEFF</a> = 120, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa5ffa7f094b5b2a79caf796b78149a0c9">OH_NN_REDUCE_ALL_REDUCE_TO_END</a> = 121, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faaa5875bed501a2b770909584f2842712">OH_NN_REDUCE_ALL_COEFF</a> = 122, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fad7a15920eeab305ba7516450d7d9adf6">OH_NN_TOP_K_AXIS</a> = 123, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa8fbe61bfd51e637400656286e6be0cf8">OH_NN_ARG_MAX_TOP_K</a> = 124, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fae037118ada28b8cd47a18208a9b7cbe8">OH_NN_ARG_MAX_OUT_MAX_VALUE</a> = 125, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa7c7dcea11e0ade9a5e1ad72e531f5870">OH_NN_QUANT_DTYPE_CAST_AXIS</a> = 126, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fafaf5f665025d330d8245f4a83c7d4526">OH_NN_SLICE_AXES</a> = 127, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faad52d7d2a61b079f4b345a39e999bafa">OH_NN_TILE_DIMS</a> = 128, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faaa379e57adbfcaaf5c51d376e25f2dc4">OH_NN_CROP_AXIS</a> = 129, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faefe2edfe25e4bf5e01cb2d441efea878">OH_NN_CROP_OFFSET</a> = 130, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa65c5145edc01f5ea3d646b11252d1729">OH_NN_DETECTION_POST_PROCESS_INPUT_SIZE</a> = 131, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa92efe047caadfd04b7b8927039408b67">OH_NN_DETECTION_POST_PROCESS_SCALE</a> = 132, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa7e664c3e5f43b11a1e52b2f5d789765d">OH_NN_DETECTION_POST_PROCESS_NMS_IOU_THRESHOLD</a> = 133, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faf22ee8222962503cd6116742bc3c30e1">OH_NN_DETECTION_POST_PROCESS_NMS_SCORE_THRESHOLD</a> = 134, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa2a0730af48d7d87b229a69244d5538e0">OH_NN_DETECTION_POST_PROCESS_MAX_DETECTIONS</a> = 135, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fadf4d0502d20775ea7045c6569ef257da">OH_NN_DETECTION_POST_PROCESS_DETECTIONS_PER_CLASS</a> = 136, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faff291b7805945c64bf459eb066d17ac1">OH_NN_DETECTION_POST_PROCESS_MAX_CLASSES_PER_DETECTION</a> = 137, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fadb9a882c47e6ca54cbacefc5400375d2">OH_NN_DETECTION_POST_PROCESS_NUM_CLASSES</a> = 138, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa251e60db3a813fcc39ee960b9298e684">OH_NN_DETECTION_POST_PROCESS_USE_REGULAR_NMS</a> = 139, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faef60ed0242356dc7ff7a142ac3c3d7ed">OH_NN_DETECTION_POST_PROCESS_OUT_QUANTIZED</a> = 140, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa8ff2c04ba34cf68117ffe4b35b6a9be5">OH_NN_L2_NORMALIZE_AXIS</a> = 141, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fafb145050b161d406c01b927785fc5ffb">OH_NN_L2_NORMALIZE_EPSILON</a> = 142, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa5792b986449b605b2c87904cc5300685">OH_NN_L2_NORMALIZE_ACTIVATION_TYPE</a> = 143, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faeebc9598492b247c655356c164fa7dd8">OH_NN_LOG_SOFTMAX_AXIS</a> = 144, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa54e1cf90454ee1477f367e92507e197e">OH_NN_LRN_DEPTH_RADIUS</a> = 145, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa701cdc0c2be6c1aa7b46b526c8e32a81">OH_NN_LRN_BIAS</a> = 146, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faac470f0863910fdcc8cff393ba3d1b90">OH_NN_LRN_ALPHA</a> = 147, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa62812191a17885810bce750adfc28d73">OH_NN_LRN_BETA</a> = 148, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fad963c246c1ae866c6f870cdf049efa31">OH_NN_LRN_NORM_REGION</a> = 149, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa7dbdece7631136fc9743e5dd1177bbf5">OH_NN_SPACE_TO_DEPTH_BLOCK_SIZE</a> = 150, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa4af0a485a82222cb105d5a615564f59a">OH_NN_REDUCE_MAX_KEEP_DIMS</a> = 151, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa9a1d920075ff790a7727c644eac9da00">OH_NN_REDUCE_MAX_REDUCE_TO_END</a> = 152, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa0a800ba7f1065dc691b69127f9f75319">OH_NN_REDUCE_MAX_COEFF</a> = 153, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fadea8a9a7bb0241d09d5af11e806bfce5">OH_NN_REDUCE_MIN_KEEP_DIMS</a> = 154, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa3d2691b08e0cf2943f8d4297f43f003f">OH_NN_REDUCE_MIN_REDUCE_TO_END</a> = 155, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fad30afd8cd016012120b5a3f85437f32e">OH_NN_REDUCE_MIN_COEFF</a> = 156, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552facfd0be63a9ca09c9023bded7140d4c80">OH_NN_REDUCE_SUM_KEEP_DIMS</a> = 157, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa34b9e023ea90ca66a8173cb41619419b">OH_NN_REDUCE_SUM_REDUCE_TO_END</a> = 158, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa64699309ad1e0eb474cecd6b12a4cb0a">OH_NN_REDUCE_SUM_COEFF</a> = 159, 
<br />
&#160;&#160;<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa1e695825f28458244d3b692166e36426">OH_NN_REDUCE_L2_KEEP_DIMS</a> = 160, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa14b4399bae5ff35b1555af4d7f06150a">OH_NN_REDUCE_L2_REDUCE_TO_END</a> = 161, 
<a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552faae7eafa55ceadd4761ca1d2549673522">OH_NN_REDUCE_L2_COEFF</a> = 162
<br />
 }<tr class="memdesc:ga09f35be6e0f86f5d747192ce5812552f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Enumerates the tensor data types.  <a href="group___neural_nework_runtime.html#ga09f35be6e0f86f5d747192ce5812552f">More...</a><br /></td></tr>
</td></tr>
<tr class="separator:ga09f35be6e0f86f5d747192ce5812552f"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ga642ce605311075cd5d5cc5b527fee3b6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga642ce605311075cd5d5cc5b527fee3b6">OH_NNCompilation_Construct</a> (const <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *model)</td></tr>
<tr class="memdesc:ga642ce605311075cd5d5cc5b527fee3b6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates a compilation instance of the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> type.  <a href="#ga642ce605311075cd5d5cc5b527fee3b6">More...</a><br /></td></tr>
<tr class="separator:ga642ce605311075cd5d5cc5b527fee3b6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gace3c05942fb3649efd3ab712e35a2ef3"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gace3c05942fb3649efd3ab712e35a2ef3">OH_NNCompilation_ConstructWithOfflineModelFile</a> (const char *modelPath)</td></tr>
<tr class="memdesc:gace3c05942fb3649efd3ab712e35a2ef3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates a compilation instance based on an offline model file.  <a href="#gace3c05942fb3649efd3ab712e35a2ef3">More...</a><br /></td></tr>
<tr class="separator:gace3c05942fb3649efd3ab712e35a2ef3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaeb58a0dce316a5b1c5fe281e277e7830"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gaeb58a0dce316a5b1c5fe281e277e7830">OH_NNCompilation_ConstructWithOfflineModelBuffer</a> (const void *modelBuffer, size_t modelSize)</td></tr>
<tr class="memdesc:gaeb58a0dce316a5b1c5fe281e277e7830"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates a compilation instance based on an offline model file buffer.  <a href="#gaeb58a0dce316a5b1c5fe281e277e7830">More...</a><br /></td></tr>
<tr class="separator:gaeb58a0dce316a5b1c5fe281e277e7830"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaad1fae1380a70f07920c2aad41b38f8b"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gaad1fae1380a70f07920c2aad41b38f8b">OH_NNCompilation_ConstructForCache</a> ()</td></tr>
<tr class="memdesc:gaad1fae1380a70f07920c2aad41b38f8b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates a empty compilation instance for restoration from cache later.  <a href="#gaad1fae1380a70f07920c2aad41b38f8b">More...</a><br /></td></tr>
<tr class="separator:gaad1fae1380a70f07920c2aad41b38f8b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga22d3a939459ea07c94ed1b4cc3458314"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga22d3a939459ea07c94ed1b4cc3458314">OH_NNCompilation_ExportCacheToBuffer</a> (<a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *compilation, const void *buffer, size_t length, size_t *modelSize)</td></tr>
<tr class="memdesc:ga22d3a939459ea07c94ed1b4cc3458314"><td class="mdescLeft">&#160;</td><td class="mdescRight">Exports the cache to a given buffer.  <a href="#ga22d3a939459ea07c94ed1b4cc3458314">More...</a><br /></td></tr>
<tr class="separator:ga22d3a939459ea07c94ed1b4cc3458314"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga1c2c203801d794b98769fd655e24c302"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga1c2c203801d794b98769fd655e24c302">OH_NNCompilation_ImportCacheFromBuffer</a> (<a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *compilation, const void *buffer, size_t modelSize)</td></tr>
<tr class="memdesc:ga1c2c203801d794b98769fd655e24c302"><td class="mdescLeft">&#160;</td><td class="mdescRight">Imports the cache from a given buffer.  <a href="#ga1c2c203801d794b98769fd655e24c302">More...</a><br /></td></tr>
<tr class="separator:ga1c2c203801d794b98769fd655e24c302"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gad22c665884bc562e63141bd553d081e8"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gad22c665884bc562e63141bd553d081e8">OH_NNCompilation_AddExtensionConfig</a> (<a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *compilation, const char *configName, const void *configValue, const size_t configValueSize)</td></tr>
<tr class="memdesc:gad22c665884bc562e63141bd553d081e8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds an extension config for a custom hardware attribute.  <a href="#gad22c665884bc562e63141bd553d081e8">More...</a><br /></td></tr>
<tr class="separator:gad22c665884bc562e63141bd553d081e8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga7a49f89704bc0ca23718089ccdb7f4bc"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga7a49f89704bc0ca23718089ccdb7f4bc">OH_NNCompilation_SetDevice</a> (<a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *compilation, size_t deviceID)</td></tr>
<tr class="memdesc:ga7a49f89704bc0ca23718089ccdb7f4bc"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specifies the device for model compilation and computing.  <a href="#ga7a49f89704bc0ca23718089ccdb7f4bc">More...</a><br /></td></tr>
<tr class="separator:ga7a49f89704bc0ca23718089ccdb7f4bc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaf39f535a7f493c13b3da544b11808e64"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gaf39f535a7f493c13b3da544b11808e64">OH_NNCompilation_SetCache</a> (<a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *compilation, const char *cachePath, uint32_t version)</td></tr>
<tr class="memdesc:gaf39f535a7f493c13b3da544b11808e64"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the cache directory and version of the compiled model.  <a href="#gaf39f535a7f493c13b3da544b11808e64">More...</a><br /></td></tr>
<tr class="separator:gaf39f535a7f493c13b3da544b11808e64"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaf7924c5da4c40acd7ffacacec273ab5d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gaf7924c5da4c40acd7ffacacec273ab5d">OH_NNCompilation_SetPerformanceMode</a> (<a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *compilation, <a class="el" href="group___neural_nework_runtime.html#ga2ea6daeb95ee5bea81f97a5d9aeef5bf">OH_NN_PerformanceMode</a> performanceMode)</td></tr>
<tr class="memdesc:gaf7924c5da4c40acd7ffacacec273ab5d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the performance mode for model computing.  <a href="#gaf7924c5da4c40acd7ffacacec273ab5d">More...</a><br /></td></tr>
<tr class="separator:gaf7924c5da4c40acd7ffacacec273ab5d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6299bad43aee5562ab729211b46cb121"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga6299bad43aee5562ab729211b46cb121">OH_NNCompilation_SetPriority</a> (<a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *compilation, <a class="el" href="group___neural_nework_runtime.html#ga5ae0ed09b49e98f608f47ac4fe9ecb07">OH_NN_Priority</a> priority)</td></tr>
<tr class="memdesc:ga6299bad43aee5562ab729211b46cb121"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the model computing priority.  <a href="#ga6299bad43aee5562ab729211b46cb121">More...</a><br /></td></tr>
<tr class="separator:ga6299bad43aee5562ab729211b46cb121"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2ca3b78ee42ff11890d52923238eebb2"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga2ca3b78ee42ff11890d52923238eebb2">OH_NNCompilation_EnableFloat16</a> (<a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *compilation, bool enableFloat16)</td></tr>
<tr class="memdesc:ga2ca3b78ee42ff11890d52923238eebb2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Enables float16 for computing.  <a href="#ga2ca3b78ee42ff11890d52923238eebb2">More...</a><br /></td></tr>
<tr class="separator:ga2ca3b78ee42ff11890d52923238eebb2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga8a65c2ea5b8ce232f5acefbf064c1a9b"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga8a65c2ea5b8ce232f5acefbf064c1a9b">OH_NNCompilation_Build</a> (<a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *compilation)</td></tr>
<tr class="memdesc:ga8a65c2ea5b8ce232f5acefbf064c1a9b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compiles a model.  <a href="#ga8a65c2ea5b8ce232f5acefbf064c1a9b">More...</a><br /></td></tr>
<tr class="separator:ga8a65c2ea5b8ce232f5acefbf064c1a9b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gae3ded5e4036db67e17313cbe7275f906"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gae3ded5e4036db67e17313cbe7275f906">OH_NNCompilation_Destroy</a> (<a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> **compilation)</td></tr>
<tr class="memdesc:gae3ded5e4036db67e17313cbe7275f906"><td class="mdescLeft">&#160;</td><td class="mdescRight">Releases the <b>Compilation</b> object.  <a href="#gae3ded5e4036db67e17313cbe7275f906">More...</a><br /></td></tr>
<tr class="separator:gae3ded5e4036db67e17313cbe7275f906"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga69293f465289045fd6ce120d8f7fb58c"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga69293f465289045fd6ce120d8f7fb58c">OH_NNTensorDesc_Create</a> ()</td></tr>
<tr class="memdesc:ga69293f465289045fd6ce120d8f7fb58c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates an <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance.  <a href="#ga69293f465289045fd6ce120d8f7fb58c">More...</a><br /></td></tr>
<tr class="separator:ga69293f465289045fd6ce120d8f7fb58c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga4e299cd1a5d0f2c1e2507f86b880fb60"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga4e299cd1a5d0f2c1e2507f86b880fb60">OH_NNTensorDesc_Destroy</a> (<a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> **tensorDesc)</td></tr>
<tr class="memdesc:ga4e299cd1a5d0f2c1e2507f86b880fb60"><td class="mdescLeft">&#160;</td><td class="mdescRight">Releases an <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance.  <a href="#ga4e299cd1a5d0f2c1e2507f86b880fb60">More...</a><br /></td></tr>
<tr class="separator:ga4e299cd1a5d0f2c1e2507f86b880fb60"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaca0cfcef499c386c2a49725798a55ed2"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gaca0cfcef499c386c2a49725798a55ed2">OH_NNTensorDesc_SetName</a> (<a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc, const char *name)</td></tr>
<tr class="memdesc:gaca0cfcef499c386c2a49725798a55ed2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the name of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#gaca0cfcef499c386c2a49725798a55ed2">More...</a><br /></td></tr>
<tr class="separator:gaca0cfcef499c386c2a49725798a55ed2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6a4c3c06e3c85c9169da0d2645d7ef9e"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga6a4c3c06e3c85c9169da0d2645d7ef9e">OH_NNTensorDesc_GetName</a> (const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc, const char **name)</td></tr>
<tr class="memdesc:ga6a4c3c06e3c85c9169da0d2645d7ef9e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the name of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#ga6a4c3c06e3c85c9169da0d2645d7ef9e">More...</a><br /></td></tr>
<tr class="separator:ga6a4c3c06e3c85c9169da0d2645d7ef9e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga5da36f7eb8fa101edb9965439f728ac6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga5da36f7eb8fa101edb9965439f728ac6">OH_NNTensorDesc_SetDataType</a> (<a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc, <a class="el" href="group___neural_nework_runtime.html#ga917dcf1cb1e7bb745ccf984e9e67940e">OH_NN_DataType</a> dataType)</td></tr>
<tr class="memdesc:ga5da36f7eb8fa101edb9965439f728ac6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the data type of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#ga5da36f7eb8fa101edb9965439f728ac6">More...</a><br /></td></tr>
<tr class="separator:ga5da36f7eb8fa101edb9965439f728ac6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaea89367832dba35f842c526d0325bc5b"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gaea89367832dba35f842c526d0325bc5b">OH_NNTensorDesc_GetDataType</a> (const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc, <a class="el" href="group___neural_nework_runtime.html#ga917dcf1cb1e7bb745ccf984e9e67940e">OH_NN_DataType</a> *dataType)</td></tr>
<tr class="memdesc:gaea89367832dba35f842c526d0325bc5b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the data type of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#gaea89367832dba35f842c526d0325bc5b">More...</a><br /></td></tr>
<tr class="separator:gaea89367832dba35f842c526d0325bc5b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2050fd8e0fac1a8a234629c43c80fad3"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga2050fd8e0fac1a8a234629c43c80fad3">OH_NNTensorDesc_SetShape</a> (<a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc, const int32_t *shape, size_t shapeLength)</td></tr>
<tr class="memdesc:ga2050fd8e0fac1a8a234629c43c80fad3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the shape of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#ga2050fd8e0fac1a8a234629c43c80fad3">More...</a><br /></td></tr>
<tr class="separator:ga2050fd8e0fac1a8a234629c43c80fad3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gae2c7ad5cf0133ec216d66d168aedc10f"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gae2c7ad5cf0133ec216d66d168aedc10f">OH_NNTensorDesc_GetShape</a> (const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc, int32_t **shape, size_t *shapeLength)</td></tr>
<tr class="memdesc:gae2c7ad5cf0133ec216d66d168aedc10f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the shape of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#gae2c7ad5cf0133ec216d66d168aedc10f">More...</a><br /></td></tr>
<tr class="separator:gae2c7ad5cf0133ec216d66d168aedc10f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga066b3fdcea563ab96e9d8fadf9df3c8d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga066b3fdcea563ab96e9d8fadf9df3c8d">OH_NNTensorDesc_SetFormat</a> (<a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc, <a class="el" href="group___neural_nework_runtime.html#ga467f4b3c524b8ec4ebf3c86d680f7f59">OH_NN_Format</a> format)</td></tr>
<tr class="memdesc:ga066b3fdcea563ab96e9d8fadf9df3c8d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the format of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#ga066b3fdcea563ab96e9d8fadf9df3c8d">More...</a><br /></td></tr>
<tr class="separator:ga066b3fdcea563ab96e9d8fadf9df3c8d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga54953cf8b797779898660fd50e550623"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga54953cf8b797779898660fd50e550623">OH_NNTensorDesc_GetFormat</a> (const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc, <a class="el" href="group___neural_nework_runtime.html#ga467f4b3c524b8ec4ebf3c86d680f7f59">OH_NN_Format</a> *format)</td></tr>
<tr class="memdesc:ga54953cf8b797779898660fd50e550623"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the format of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#ga54953cf8b797779898660fd50e550623">More...</a><br /></td></tr>
<tr class="separator:ga54953cf8b797779898660fd50e550623"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gada602068be210bc6c7f778f0931e5848"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gada602068be210bc6c7f778f0931e5848">OH_NNTensorDesc_GetElementCount</a> (const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc, size_t *elementCount)</td></tr>
<tr class="memdesc:gada602068be210bc6c7f778f0931e5848"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the element count of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#gada602068be210bc6c7f778f0931e5848">More...</a><br /></td></tr>
<tr class="separator:gada602068be210bc6c7f778f0931e5848"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gabeca8077942b547ad6160807f38332c1"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gabeca8077942b547ad6160807f38332c1">OH_NNTensorDesc_GetByteSize</a> (const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc, size_t *byteSize)</td></tr>
<tr class="memdesc:gabeca8077942b547ad6160807f38332c1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the byte size of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#gabeca8077942b547ad6160807f38332c1">More...</a><br /></td></tr>
<tr class="separator:gabeca8077942b547ad6160807f38332c1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga724c81fe4a2e48021fd545b3c072ba50"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga724c81fe4a2e48021fd545b3c072ba50">OH_NNTensor_Create</a> (size_t deviceID, <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc)</td></tr>
<tr class="memdesc:ga724c81fe4a2e48021fd545b3c072ba50"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance from <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#ga724c81fe4a2e48021fd545b3c072ba50">More...</a><br /></td></tr>
<tr class="separator:ga724c81fe4a2e48021fd545b3c072ba50"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gad658b50239b119baece25c7470ccf403"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gad658b50239b119baece25c7470ccf403">OH_NNTensor_CreateWithSize</a> (size_t deviceID, <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc, size_t size)</td></tr>
<tr class="memdesc:gad658b50239b119baece25c7470ccf403"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance with specified size and <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#gad658b50239b119baece25c7470ccf403">More...</a><br /></td></tr>
<tr class="separator:gad658b50239b119baece25c7470ccf403"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gafd7fac565ba766422257e1525dc416ce"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gafd7fac565ba766422257e1525dc416ce">OH_NNTensor_CreateWithFd</a> (size_t deviceID, <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc, int fd, size_t size, size_t offset)</td></tr>
<tr class="memdesc:gafd7fac565ba766422257e1525dc416ce"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance with specified file descriptor and <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>.  <a href="#gafd7fac565ba766422257e1525dc416ce">More...</a><br /></td></tr>
<tr class="separator:gafd7fac565ba766422257e1525dc416ce"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga33d9d392ba17947c578b099961d32789"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga33d9d392ba17947c578b099961d32789">OH_NNTensor_Destroy</a> (<a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> **tensor)</td></tr>
<tr class="memdesc:ga33d9d392ba17947c578b099961d32789"><td class="mdescLeft">&#160;</td><td class="mdescRight">Releases a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance.  <a href="#ga33d9d392ba17947c578b099961d32789">More...</a><br /></td></tr>
<tr class="separator:ga33d9d392ba17947c578b099961d32789"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaaeb03b1a5cd44ba29f71b5814acb73b2"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gaaeb03b1a5cd44ba29f71b5814acb73b2">OH_NNTensor_GetTensorDesc</a> (const <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *tensor)</td></tr>
<tr class="memdesc:gaaeb03b1a5cd44ba29f71b5814acb73b2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance of a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>.  <a href="#gaaeb03b1a5cd44ba29f71b5814acb73b2">More...</a><br /></td></tr>
<tr class="separator:gaaeb03b1a5cd44ba29f71b5814acb73b2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga8ac1491aa7df59382a46ffceb7302638"><td class="memItemLeft" align="right" valign="top">void *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga8ac1491aa7df59382a46ffceb7302638">OH_NNTensor_GetDataBuffer</a> (const <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *tensor)</td></tr>
<tr class="memdesc:ga8ac1491aa7df59382a46ffceb7302638"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the data buffer of a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>.  <a href="#ga8ac1491aa7df59382a46ffceb7302638">More...</a><br /></td></tr>
<tr class="separator:ga8ac1491aa7df59382a46ffceb7302638"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga581e19d403376f61434edd1d86b30c11"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga581e19d403376f61434edd1d86b30c11">OH_NNTensor_GetFd</a> (const <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *tensor, int *fd)</td></tr>
<tr class="memdesc:ga581e19d403376f61434edd1d86b30c11"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the file descriptor of the shared memory of a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>.  <a href="#ga581e19d403376f61434edd1d86b30c11">More...</a><br /></td></tr>
<tr class="separator:ga581e19d403376f61434edd1d86b30c11"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gac5f825923842d3484b66e799b6bda650"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gac5f825923842d3484b66e799b6bda650">OH_NNTensor_GetSize</a> (const <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *tensor, size_t *size)</td></tr>
<tr class="memdesc:gac5f825923842d3484b66e799b6bda650"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the size of the shared memory of a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>.  <a href="#gac5f825923842d3484b66e799b6bda650">More...</a><br /></td></tr>
<tr class="separator:gac5f825923842d3484b66e799b6bda650"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gae4d72419b8e7fc0ac70bf91ab7510ecb"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gae4d72419b8e7fc0ac70bf91ab7510ecb">OH_NNTensor_GetOffset</a> (const <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *tensor, size_t *offset)</td></tr>
<tr class="memdesc:gae4d72419b8e7fc0ac70bf91ab7510ecb"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get the data offset of a tensor.  <a href="#gae4d72419b8e7fc0ac70bf91ab7510ecb">More...</a><br /></td></tr>
<tr class="separator:gae4d72419b8e7fc0ac70bf91ab7510ecb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaca3d4495ef04ebb7e32400bbf9d61c5d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gaca3d4495ef04ebb7e32400bbf9d61c5d">OH_NNExecutor_Construct</a> (<a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *compilation)</td></tr>
<tr class="memdesc:gaca3d4495ef04ebb7e32400bbf9d61c5d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates an executor instance of the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> type.  <a href="#gaca3d4495ef04ebb7e32400bbf9d61c5d">More...</a><br /></td></tr>
<tr class="separator:gaca3d4495ef04ebb7e32400bbf9d61c5d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gadcc8c5d6bec42ffb06d2ce3b6698207d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gadcc8c5d6bec42ffb06d2ce3b6698207d">OH_NNExecutor_GetOutputShape</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, uint32_t outputIndex, int32_t **shape, uint32_t *shapeLength)</td></tr>
<tr class="memdesc:gadcc8c5d6bec42ffb06d2ce3b6698207d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Obtains the dimension information about the output tensor.  <a href="#gadcc8c5d6bec42ffb06d2ce3b6698207d">More...</a><br /></td></tr>
<tr class="separator:gadcc8c5d6bec42ffb06d2ce3b6698207d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga8d7fade61b14d54446f7076e688c30f2"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga8d7fade61b14d54446f7076e688c30f2">OH_NNExecutor_Destroy</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> **executor)</td></tr>
<tr class="memdesc:ga8d7fade61b14d54446f7076e688c30f2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Destroys an executor instance to release the memory occupied by the executor.  <a href="#ga8d7fade61b14d54446f7076e688c30f2">More...</a><br /></td></tr>
<tr class="separator:ga8d7fade61b14d54446f7076e688c30f2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga15cc92e6c5cb825b154c367108bc28cc"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga15cc92e6c5cb825b154c367108bc28cc">OH_NNExecutor_GetInputCount</a> (const <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, size_t *inputCount)</td></tr>
<tr class="memdesc:ga15cc92e6c5cb825b154c367108bc28cc"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the input tensor count.  <a href="#ga15cc92e6c5cb825b154c367108bc28cc">More...</a><br /></td></tr>
<tr class="separator:ga15cc92e6c5cb825b154c367108bc28cc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaf679893bb2413ce72229b310c91e3663"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gaf679893bb2413ce72229b310c91e3663">OH_NNExecutor_GetOutputCount</a> (const <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, size_t *outputCount)</td></tr>
<tr class="memdesc:gaf679893bb2413ce72229b310c91e3663"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the output tensor count.  <a href="#gaf679893bb2413ce72229b310c91e3663">More...</a><br /></td></tr>
<tr class="separator:gaf679893bb2413ce72229b310c91e3663"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gad55eda40f5087c543911469a6e003916"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gad55eda40f5087c543911469a6e003916">OH_NNExecutor_CreateInputTensorDesc</a> (const <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, size_t index)</td></tr>
<tr class="memdesc:gad55eda40f5087c543911469a6e003916"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates an input tensor descriptor with its index.  <a href="#gad55eda40f5087c543911469a6e003916">More...</a><br /></td></tr>
<tr class="separator:gad55eda40f5087c543911469a6e003916"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga9f8d0428fc8f7568e7569e8bf9e3eb4e"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga9f8d0428fc8f7568e7569e8bf9e3eb4e">OH_NNExecutor_CreateOutputTensorDesc</a> (const <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, size_t index)</td></tr>
<tr class="memdesc:ga9f8d0428fc8f7568e7569e8bf9e3eb4e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates an output tensor descriptor with its index.  <a href="#ga9f8d0428fc8f7568e7569e8bf9e3eb4e">More...</a><br /></td></tr>
<tr class="separator:ga9f8d0428fc8f7568e7569e8bf9e3eb4e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga91e0d9066ddd07ed2e12d2bd3971c9fa"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga91e0d9066ddd07ed2e12d2bd3971c9fa">OH_NNExecutor_GetInputDimRange</a> (const <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, size_t index, size_t **minInputDims, size_t **maxInputDims, size_t *shapeLength)</td></tr>
<tr class="memdesc:ga91e0d9066ddd07ed2e12d2bd3971c9fa"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the dimension ranges of an input tensor.  <a href="#ga91e0d9066ddd07ed2e12d2bd3971c9fa">More...</a><br /></td></tr>
<tr class="separator:ga91e0d9066ddd07ed2e12d2bd3971c9fa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2a5f79e766f1c4a7eab4dcbd8d582743"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga2a5f79e766f1c4a7eab4dcbd8d582743">OH_NNExecutor_SetOnRunDone</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, <a class="el" href="group___neural_nework_runtime.html#ga0686487b4c80b6bd0d686fa1c08b4c8c">NN_OnRunDone</a> onRunDone)</td></tr>
<tr class="memdesc:ga2a5f79e766f1c4a7eab4dcbd8d582743"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the callback function handle for the post-process when the asynchronous execution has been done.  <a href="#ga2a5f79e766f1c4a7eab4dcbd8d582743">More...</a><br /></td></tr>
<tr class="separator:ga2a5f79e766f1c4a7eab4dcbd8d582743"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaf2fbdb20804b7f4839abc838413bc241"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gaf2fbdb20804b7f4839abc838413bc241">OH_NNExecutor_SetOnServiceDied</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, <a class="el" href="group___neural_nework_runtime.html#ga965dabbf39f75570925be38b8d0c8f76">NN_OnServiceDied</a> onServiceDied)</td></tr>
<tr class="memdesc:gaf2fbdb20804b7f4839abc838413bc241"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the callback function handle for the post-process when the device driver service is dead during asynchronous execution.  <a href="#gaf2fbdb20804b7f4839abc838413bc241">More...</a><br /></td></tr>
<tr class="separator:gaf2fbdb20804b7f4839abc838413bc241"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga77aca3467cfda5a05042be320d34e90c"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga77aca3467cfda5a05042be320d34e90c">OH_NNExecutor_RunSync</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *inputTensor[], size_t inputCount, <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *outputTensor[], size_t outputCount)</td></tr>
<tr class="memdesc:ga77aca3467cfda5a05042be320d34e90c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Synchronous execution of the model inference.  <a href="#ga77aca3467cfda5a05042be320d34e90c">More...</a><br /></td></tr>
<tr class="separator:ga77aca3467cfda5a05042be320d34e90c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga199e78f7de09248707a4aaa3bf76d07a"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga199e78f7de09248707a4aaa3bf76d07a">OH_NNExecutor_RunAsync</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *inputTensor[], size_t inputCount, <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *outputTensor[], size_t outputCount, int32_t <a class="el" href="group___usb_ddk.html#gab5627d8d8b095c198e2523c44ca380ac">timeout</a>, void *userData)</td></tr>
<tr class="memdesc:ga199e78f7de09248707a4aaa3bf76d07a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Asynchronous execution of the model inference.  <a href="#ga199e78f7de09248707a4aaa3bf76d07a">More...</a><br /></td></tr>
<tr class="separator:ga199e78f7de09248707a4aaa3bf76d07a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga66c11e4935abaa6710a1d8eb4d09395f"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga66c11e4935abaa6710a1d8eb4d09395f">OH_NNDevice_GetAllDevicesID</a> (const size_t **allDevicesID, uint32_t *deviceCount)</td></tr>
<tr class="memdesc:ga66c11e4935abaa6710a1d8eb4d09395f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Obtains the IDs of all devices connected.  <a href="#ga66c11e4935abaa6710a1d8eb4d09395f">More...</a><br /></td></tr>
<tr class="separator:ga66c11e4935abaa6710a1d8eb4d09395f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga27d205f26b9e1dd1a0a5bb2e058341a9"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga27d205f26b9e1dd1a0a5bb2e058341a9">OH_NNDevice_GetName</a> (size_t deviceID, const char **name)</td></tr>
<tr class="memdesc:ga27d205f26b9e1dd1a0a5bb2e058341a9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Obtains the name of the specified device.  <a href="#ga27d205f26b9e1dd1a0a5bb2e058341a9">More...</a><br /></td></tr>
<tr class="separator:ga27d205f26b9e1dd1a0a5bb2e058341a9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga06a27876b3f3bf0170031b26c4d04f62"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga06a27876b3f3bf0170031b26c4d04f62">OH_NNDevice_GetType</a> (size_t deviceID, <a class="el" href="group___neural_nework_runtime.html#gae99ff81dfe03a24f714261009b5d2bff">OH_NN_DeviceType</a> *deviceType)</td></tr>
<tr class="memdesc:ga06a27876b3f3bf0170031b26c4d04f62"><td class="mdescLeft">&#160;</td><td class="mdescRight">Obtains the type information of the specified device.  <a href="#ga06a27876b3f3bf0170031b26c4d04f62">More...</a><br /></td></tr>
<tr class="separator:ga06a27876b3f3bf0170031b26c4d04f62"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga5e4fff1b517ad1451f0106983d04491b"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga5e4fff1b517ad1451f0106983d04491b">OH_NNQuantParam_Create</a> ()</td></tr>
<tr class="memdesc:ga5e4fff1b517ad1451f0106983d04491b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates a <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance.  <a href="#ga5e4fff1b517ad1451f0106983d04491b">More...</a><br /></td></tr>
<tr class="separator:ga5e4fff1b517ad1451f0106983d04491b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaff24853638b0aa423fcd2724e3dfaaf1"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gaff24853638b0aa423fcd2724e3dfaaf1">OH_NNQuantParam_SetScales</a> (<a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> *quantParams, const double *scales, size_t quantCount)</td></tr>
<tr class="memdesc:gaff24853638b0aa423fcd2724e3dfaaf1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the scales of the <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance.  <a href="#gaff24853638b0aa423fcd2724e3dfaaf1">More...</a><br /></td></tr>
<tr class="separator:gaff24853638b0aa423fcd2724e3dfaaf1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gac6c284fb1577981a5e9da90cedc65b86"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gac6c284fb1577981a5e9da90cedc65b86">OH_NNQuantParam_SetZeroPoints</a> (<a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> *quantParams, const int32_t *zeroPoints, size_t quantCount)</td></tr>
<tr class="memdesc:gac6c284fb1577981a5e9da90cedc65b86"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the zero points of the <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance.  <a href="#gac6c284fb1577981a5e9da90cedc65b86">More...</a><br /></td></tr>
<tr class="separator:gac6c284fb1577981a5e9da90cedc65b86"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gad7ddfbe997bb70b6b5fb1d490dbeb8d8"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gad7ddfbe997bb70b6b5fb1d490dbeb8d8">OH_NNQuantParam_SetNumBits</a> (<a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> *quantParams, const uint32_t *numBits, size_t quantCount)</td></tr>
<tr class="memdesc:gad7ddfbe997bb70b6b5fb1d490dbeb8d8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the number bits of the <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance.  <a href="#gad7ddfbe997bb70b6b5fb1d490dbeb8d8">More...</a><br /></td></tr>
<tr class="separator:gad7ddfbe997bb70b6b5fb1d490dbeb8d8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gadb7a5667b419e1d3e1065e542a7f78a9"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gadb7a5667b419e1d3e1065e542a7f78a9">OH_NNQuantParam_Destroy</a> (<a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> **quantParams)</td></tr>
<tr class="memdesc:gadb7a5667b419e1d3e1065e542a7f78a9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Releases a <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance.  <a href="#gadb7a5667b419e1d3e1065e542a7f78a9">More...</a><br /></td></tr>
<tr class="separator:gadb7a5667b419e1d3e1065e542a7f78a9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga65e3b60e38110bd0aa951f3fa6618262"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga65e3b60e38110bd0aa951f3fa6618262">OH_NNModel_Construct</a> (void)</td></tr>
<tr class="memdesc:ga65e3b60e38110bd0aa951f3fa6618262"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates a model instance of the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> type and uses other APIs provided by OH_NNModel to construct the model instance.  <a href="#ga65e3b60e38110bd0aa951f3fa6618262">More...</a><br /></td></tr>
<tr class="separator:ga65e3b60e38110bd0aa951f3fa6618262"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga32d5660e09c42a92636c7c97bfa35c7d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga32d5660e09c42a92636c7c97bfa35c7d">OH_NNModel_AddTensorToModel</a> (<a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *model, const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *tensorDesc)</td></tr>
<tr class="memdesc:ga32d5660e09c42a92636c7c97bfa35c7d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds a tensor to the model instance.  <a href="#ga32d5660e09c42a92636c7c97bfa35c7d">More...</a><br /></td></tr>
<tr class="separator:ga32d5660e09c42a92636c7c97bfa35c7d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga3cb6d4ba6e56798e47794127f77b31a8"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga3cb6d4ba6e56798e47794127f77b31a8">OH_NNModel_SetTensorData</a> (<a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *model, uint32_t index, const void *dataBuffer, size_t length)</td></tr>
<tr class="memdesc:ga3cb6d4ba6e56798e47794127f77b31a8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the tensor value.  <a href="#ga3cb6d4ba6e56798e47794127f77b31a8">More...</a><br /></td></tr>
<tr class="separator:ga3cb6d4ba6e56798e47794127f77b31a8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gab9a0bf7d814e23bb400c8f1ed05bff18"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gab9a0bf7d814e23bb400c8f1ed05bff18">OH_NNModel_SetTensorQuantParams</a> (<a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *model, uint32_t index, <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> *quantParam)</td></tr>
<tr class="memdesc:gab9a0bf7d814e23bb400c8f1ed05bff18"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the quantization parameter of a tensor.  <a href="#gab9a0bf7d814e23bb400c8f1ed05bff18">More...</a><br /></td></tr>
<tr class="separator:gab9a0bf7d814e23bb400c8f1ed05bff18"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2b62ce6e33d810a38aa1137e70c9c274"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga2b62ce6e33d810a38aa1137e70c9c274">OH_NNModel_SetTensorType</a> (<a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *model, uint32_t index, <a class="el" href="group___neural_nework_runtime.html#ga09f35be6e0f86f5d747192ce5812552f">OH_NN_TensorType</a> tensorType)</td></tr>
<tr class="memdesc:ga2b62ce6e33d810a38aa1137e70c9c274"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the tensor type. See <a class="el" href="group___neural_nework_runtime.html#ga09f35be6e0f86f5d747192ce5812552f">OH_NN_TensorType</a> for details.  <a href="#ga2b62ce6e33d810a38aa1137e70c9c274">More...</a><br /></td></tr>
<tr class="separator:ga2b62ce6e33d810a38aa1137e70c9c274"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga665a2d23d91063d7e7de63723db44a20"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga665a2d23d91063d7e7de63723db44a20">OH_NNModel_AddOperation</a> (<a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *model, <a class="el" href="group___neural_nework_runtime.html#gab333095762fd39a9f952523d717b95de">OH_NN_OperationType</a> op, const <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a> *paramIndices, const <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a> *inputIndices, const <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a> *outputIndices)</td></tr>
<tr class="memdesc:ga665a2d23d91063d7e7de63723db44a20"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds an operator to a model instance.  <a href="#ga665a2d23d91063d7e7de63723db44a20">More...</a><br /></td></tr>
<tr class="separator:ga665a2d23d91063d7e7de63723db44a20"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga57188ebd6bbbbfd127bf96af994a4310"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> (<a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *model, const <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a> *inputIndices, const <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a> *outputIndices)</td></tr>
<tr class="memdesc:ga57188ebd6bbbbfd127bf96af994a4310"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specifies the inputs and outputs of a model.  <a href="#ga57188ebd6bbbbfd127bf96af994a4310">More...</a><br /></td></tr>
<tr class="separator:ga57188ebd6bbbbfd127bf96af994a4310"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga39888a40afb57fc807c11d6b880e8442"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga39888a40afb57fc807c11d6b880e8442">OH_NNModel_Finish</a> (<a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *model)</td></tr>
<tr class="memdesc:ga39888a40afb57fc807c11d6b880e8442"><td class="mdescLeft">&#160;</td><td class="mdescRight">Completes model composition.  <a href="#ga39888a40afb57fc807c11d6b880e8442">More...</a><br /></td></tr>
<tr class="separator:ga39888a40afb57fc807c11d6b880e8442"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6a56b11554f4eb977c9d7fd831c89020"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga6a56b11554f4eb977c9d7fd831c89020">OH_NNModel_Destroy</a> (<a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> **model)</td></tr>
<tr class="memdesc:ga6a56b11554f4eb977c9d7fd831c89020"><td class="mdescLeft">&#160;</td><td class="mdescRight">Releases a model instance.  <a href="#ga6a56b11554f4eb977c9d7fd831c89020">More...</a><br /></td></tr>
<tr class="separator:ga6a56b11554f4eb977c9d7fd831c89020"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga9d4bdd9a5e7689b7778a927865389302"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga9d4bdd9a5e7689b7778a927865389302">OH_NNModel_GetAvailableOperations</a> (<a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *model, size_t deviceID, const bool **isSupported, uint32_t *opCount)</td></tr>
<tr class="memdesc:ga9d4bdd9a5e7689b7778a927865389302"><td class="mdescLeft">&#160;</td><td class="mdescRight">Queries whether the device supports operators in the model. The support status is indicated by the Boolean value.  <a href="#ga9d4bdd9a5e7689b7778a927865389302">More...</a><br /></td></tr>
<tr class="separator:ga9d4bdd9a5e7689b7778a927865389302"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaca9b631aeec257142ed7acd9a66afad4"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gaca9b631aeec257142ed7acd9a66afad4">OH_NNModel_AddTensor</a> (<a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *model, const <a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a> *tensor)</td></tr>
<tr class="memdesc:gaca9b631aeec257142ed7acd9a66afad4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds a tensor to a model instance.  <a href="#gaca9b631aeec257142ed7acd9a66afad4">More...</a><br /></td></tr>
<tr class="separator:gaca9b631aeec257142ed7acd9a66afad4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga3e71e749f923dbf8578c6f9bcb2aac86"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga3e71e749f923dbf8578c6f9bcb2aac86">OH_NNExecutor_SetInput</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, uint32_t inputIndex, const <a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a> *tensor, const void *dataBuffer, size_t length)</td></tr>
<tr class="memdesc:ga3e71e749f923dbf8578c6f9bcb2aac86"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the single input data for a model.  <a href="#ga3e71e749f923dbf8578c6f9bcb2aac86">More...</a><br /></td></tr>
<tr class="separator:ga3e71e749f923dbf8578c6f9bcb2aac86"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gac066212acc73c1522032fcb04a4f06df"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gac066212acc73c1522032fcb04a4f06df">OH_NNExecutor_SetOutput</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, uint32_t outputIndex, void *dataBuffer, size_t length)</td></tr>
<tr class="memdesc:gac066212acc73c1522032fcb04a4f06df"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the buffer for a single output of a model.  <a href="#gac066212acc73c1522032fcb04a4f06df">More...</a><br /></td></tr>
<tr class="separator:gac066212acc73c1522032fcb04a4f06df"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga716af07e8996677f1364a88c7ae30908"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga716af07e8996677f1364a88c7ae30908">OH_NNExecutor_Run</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor)</td></tr>
<tr class="memdesc:ga716af07e8996677f1364a88c7ae30908"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs inference.  <a href="#ga716af07e8996677f1364a88c7ae30908">More...</a><br /></td></tr>
<tr class="separator:ga716af07e8996677f1364a88c7ae30908"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga7d32f81c66dc901ab40c36b17995e32d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga7d32f81c66dc901ab40c36b17995e32d">OH_NNExecutor_AllocateInputMemory</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, uint32_t inputIndex, size_t length)</td></tr>
<tr class="memdesc:ga7d32f81c66dc901ab40c36b17995e32d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Allocates shared memory to a single input on a device.  <a href="#ga7d32f81c66dc901ab40c36b17995e32d">More...</a><br /></td></tr>
<tr class="separator:ga7d32f81c66dc901ab40c36b17995e32d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gad0ed524e694defcf0b2d7479a7784967"><td class="memItemLeft" align="right" valign="top"><a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gad0ed524e694defcf0b2d7479a7784967">OH_NNExecutor_AllocateOutputMemory</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, uint32_t outputIndex, size_t length)</td></tr>
<tr class="memdesc:gad0ed524e694defcf0b2d7479a7784967"><td class="mdescLeft">&#160;</td><td class="mdescRight">Allocates shared memory to a single output on a device.  <a href="#gad0ed524e694defcf0b2d7479a7784967">More...</a><br /></td></tr>
<tr class="separator:gad0ed524e694defcf0b2d7479a7784967"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2e52accdd70c1811eb67da79d557749d"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga2e52accdd70c1811eb67da79d557749d">OH_NNExecutor_DestroyInputMemory</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, uint32_t inputIndex, <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> **memory)</td></tr>
<tr class="memdesc:ga2e52accdd70c1811eb67da79d557749d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Releases the input memory to which the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance points.  <a href="#ga2e52accdd70c1811eb67da79d557749d">More...</a><br /></td></tr>
<tr class="separator:ga2e52accdd70c1811eb67da79d557749d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga7702355958f4c9faf48509bdf386695b"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga7702355958f4c9faf48509bdf386695b">OH_NNExecutor_DestroyOutputMemory</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, uint32_t outputIndex, <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> **memory)</td></tr>
<tr class="memdesc:ga7702355958f4c9faf48509bdf386695b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Releases the output memory to which the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance points.  <a href="#ga7702355958f4c9faf48509bdf386695b">More...</a><br /></td></tr>
<tr class="separator:ga7702355958f4c9faf48509bdf386695b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gab98aa06c8215d47be190254cc8682236"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#gab98aa06c8215d47be190254cc8682236">OH_NNExecutor_SetInputWithMemory</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, uint32_t inputIndex, const <a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a> *tensor, const <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> *memory)</td></tr>
<tr class="memdesc:gab98aa06c8215d47be190254cc8682236"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specifies the hardware shared memory pointed to by the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance as the shared memory used by a single input.  <a href="#gab98aa06c8215d47be190254cc8682236">More...</a><br /></td></tr>
<tr class="separator:gab98aa06c8215d47be190254cc8682236"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga98e76f299d78cf688c8fc515dbab1c33"><td class="memItemLeft" align="right" valign="top"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group___neural_nework_runtime.html#ga98e76f299d78cf688c8fc515dbab1c33">OH_NNExecutor_SetOutputWithMemory</a> (<a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *executor, uint32_t outputIndex, const <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> *memory)</td></tr>
<tr class="memdesc:ga98e76f299d78cf688c8fc515dbab1c33"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specifies the hardware shared memory pointed to by the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance as the shared memory used by a single output.  <a href="#ga98e76f299d78cf688c8fc515dbab1c33">More...</a><br /></td></tr>
<tr class="separator:ga98e76f299d78cf688c8fc515dbab1c33"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p>Provides APIs of Neural Network Runtime for accelerating the model inference. </p>
<p>Provides APIs for accelerating the Neural Network Runtime model inference.</p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>2.0 </dd></dl>
<h2 class="groupheader">Typedef Documentation</h2>
<a id="ga0686487b4c80b6bd0d686fa1c08b4c8c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga0686487b4c80b6bd0d686fa1c08b4c8c">&#9670;&nbsp;</a></span>NN_OnRunDone</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef void(* NN_OnRunDone) (void *userData, <a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> errCode, void *outputTensor[], int32_t outputCount)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the callback function handle for the post-process when the asynchronous execution has been done. </p>
<p>Use <b>userData</b> to identify the asynchronous execution you want to get. It is the argument <b>userData</b> passed to <a class="el" href="group___neural_nework_runtime.html#ga199e78f7de09248707a4aaa3bf76d07a">OH_NNExecutor_RunAsync</a>.<br />
 Use <b>errCode</b> of type <a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> to get the error code returned by the asynchronous execution.<br />
 The <b>outputTensor</b> and <b>outputCount</b> are the inference results, which is the same as ones passed to <a class="el" href="group___neural_nework_runtime.html#ga199e78f7de09248707a4aaa3bf76d07a">OH_NNExecutor_RunAsync</a>.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">userData</td><td>Asynchronous execution identifier, which is the argument <b>userData</b> passed to <a class="el" href="group___neural_nework_runtime.html#ga199e78f7de09248707a4aaa3bf76d07a">OH_NNExecutor_RunAsync</a>. </td></tr>
    <tr><td class="paramname">errCode</td><td>Error code <a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> returned by the asynchronous execution. </td></tr>
    <tr><td class="paramname">outputTensor</td><td>An array of output tensors <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> of the model, which is the same as the argument <b>outputTensor</b> passed to <a class="el" href="group___neural_nework_runtime.html#ga199e78f7de09248707a4aaa3bf76d07a">OH_NNExecutor_RunAsync</a>. </td></tr>
    <tr><td class="paramname">outputCount</td><td>Output tensor count, which is the same as the argument <b>outputCount</b> passed to <a class="el" href="group___neural_nework_runtime.html#ga199e78f7de09248707a4aaa3bf76d07a">OH_NNExecutor_RunAsync</a>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00215">215</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga965dabbf39f75570925be38b8d0c8f76"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga965dabbf39f75570925be38b8d0c8f76">&#9670;&nbsp;</a></span>NN_OnServiceDied</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef void(* NN_OnServiceDied) (void *userData)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the callback function handle for the post-process when the device driver service is dead during asynchronous execution. </p>
<p>You should recompile the model if this callback function is called.<br />
 Use <b>userData</b> to identify the asynchronous execution you want to get. It is the argument <b>userData</b> passed to <a class="el" href="group___neural_nework_runtime.html#ga199e78f7de09248707a4aaa3bf76d07a">OH_NNExecutor_RunAsync</a>.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">userData</td><td>Asynchronous execution identifier, which is the argument <b>userData</b> passed to <a class="el" href="group___neural_nework_runtime.html#ga199e78f7de09248707a4aaa3bf76d07a">OH_NNExecutor_RunAsync</a>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00231">231</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga94cf3b68da13f278ad55431da7342619"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga94cf3b68da13f278ad55431da7342619">&#9670;&nbsp;</a></span>NN_QuantParam</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef struct <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the quantization parameter handle. </p>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00084">84</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga36df49080d9305767e96668e948a8f3b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga36df49080d9305767e96668e948a8f3b">&#9670;&nbsp;</a></span>NN_Tensor</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef struct <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the tensor handle. </p>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00100">100</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga15da798ec7d8a9e924f172a252fcf080"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga15da798ec7d8a9e924f172a252fcf080">&#9670;&nbsp;</a></span>NN_TensorDesc</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef struct <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the tensor descriptor handle. </p>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00092">92</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga3f40d9d094f673cde61073770146f458"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga3f40d9d094f673cde61073770146f458">&#9670;&nbsp;</a></span>OH_NN_Memory</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef struct <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a>  <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the memory structure. </p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000106">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga65d3909e6fb6f4964dda431a41fd48d8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga65d3909e6fb6f4964dda431a41fd48d8">&#9670;&nbsp;</a></span>OH_NN_QuantParam</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef struct <a class="el" href="struct_o_h___n_n___quant_param.html">OH_NN_QuantParam</a>  <a class="el" href="struct_o_h___n_n___quant_param.html">OH_NN_QuantParam</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Quantization information. </p>
<p>In quantization scenarios, the 32-bit floating-point data type is quantized into the fixed-point data type according to the following formula: </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ q = clamp(round(\frac{r}{s}+z), q_{min}, q_{max}) \]" src="form_0.png"/>
</p>
<p> s and z are quantization parameters, which are stored by <b>scale</b> and <b>zeroPoint</b> in <a class="el" href="struct_o_h___n_n___quant_param.html">OH_NN_QuantParam</a>. r is a floating point number, q is the quantization result, q_min is the lower bound of the quantization result, and q_max is an upper bound of a quantization result. The calculation method is as follows:</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ \text{clamp}(x,min,max) = \begin{cases} q_{min} = -(1 &lt;&lt; (numBits - 1)) \\ q_{max} = (1 &lt;&lt; (numBits - 1)) \\ \end{cases} \]" src="form_1.png"/>
</p>
<p> The clamp function is defined as follows: </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ \text{clamp}(x,min,max) = \begin{cases} \text{max} &amp; \text{ if } x &gt; \text{ max } \\ \text{min} &amp; \text{ if } x &lt; \text{ min } \\ x &amp; \text{ otherwise } \\ \end{cases} \]" src="form_2.png"/>
</p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000104">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gac92614e1d0254fc38a1e9e2534429a18"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gac92614e1d0254fc38a1e9e2534429a18">&#9670;&nbsp;</a></span>OH_NN_Tensor</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef struct <a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a>  <a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the tensor structure. </p>
<p>It is usually used to construct data nodes and operator parameters in a model graph. When constructing a tensor, you need to specify the data type, number of dimensions, dimension information, and quantization information.</p>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000105">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gadbfbf3d0f83e86f77d64f368fbbfe583"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gadbfbf3d0f83e86f77d64f368fbbfe583">&#9670;&nbsp;</a></span>OH_NN_UInt32Array</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef struct <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a>  <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This structure is used to store a 32-bit unsigned integer array. </p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga717fa252bcbce3942c2ec2b25238492b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga717fa252bcbce3942c2ec2b25238492b">&#9670;&nbsp;</a></span>OH_NNCompilation</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef struct <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the compilation handle. </p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00068">68</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga93fd7e632b29ec5174a09e138478ce78"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga93fd7e632b29ec5174a09e138478ce78">&#9670;&nbsp;</a></span>OH_NNExecutor</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef struct <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the executor handle. </p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00076">76</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga4169d4a60707eae47f924a19a77db0a1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga4169d4a60707eae47f924a19a77db0a1">&#9670;&nbsp;</a></span>OH_NNModel</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef struct <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the handles of models. </p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00060">60</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<h2 class="groupheader">Enumeration Type Documentation</h2>
<a id="ga917dcf1cb1e7bb745ccf984e9e67940e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga917dcf1cb1e7bb745ccf984e9e67940e">&#9670;&nbsp;</a></span>OH_NN_DataType</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="group___neural_nework_runtime.html#ga917dcf1cb1e7bb745ccf984e9e67940e">OH_NN_DataType</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines tensor data types. </p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940ea28ff687c508d5623d10fd24fbdbd7cf6"></a>OH_NN_UNKNOWN&#160;</td><td class="fielddoc"><p>Unknown type </p>
</td></tr>
<tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940eabf6e9c671b9d6b8cd276067f4384dca4"></a>OH_NN_BOOL&#160;</td><td class="fielddoc"><p>bool </p>
</td></tr>
<tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940ea3f82397050b6a4a3928798105985a505"></a>OH_NN_INT8&#160;</td><td class="fielddoc"><p>int8 </p>
</td></tr>
<tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940ea18a1886aaac5985b294cc58c9d8d14bb"></a>OH_NN_INT16&#160;</td><td class="fielddoc"><p>int16 </p>
</td></tr>
<tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940ea750f391a55c5d8d90b98af6f61dd3ee2"></a>OH_NN_INT32&#160;</td><td class="fielddoc"><p>int32 </p>
</td></tr>
<tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940eadd0f78fb792e38ca2a2e9eaa8b15ca40"></a>OH_NN_INT64&#160;</td><td class="fielddoc"><p>int64 </p>
</td></tr>
<tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940ea2a00cccefe879051f75b781c16b89e4e"></a>OH_NN_UINT8&#160;</td><td class="fielddoc"><p>uint8 </p>
</td></tr>
<tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940ea66ebbf01c9ac3ca9760ae0824fa87d67"></a>OH_NN_UINT16&#160;</td><td class="fielddoc"><p>uint16 </p>
</td></tr>
<tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940ea516228d1c155e6c302f067d11416ab14"></a>OH_NN_UINT32&#160;</td><td class="fielddoc"><p>uint32 </p>
</td></tr>
<tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940ea370a35d2fa1b261871e166e9562c30e8"></a>OH_NN_UINT64&#160;</td><td class="fielddoc"><p>uint64 </p>
</td></tr>
<tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940ea892d3a48eec5855cfa17fd4f95fe078e"></a>OH_NN_FLOAT16&#160;</td><td class="fielddoc"><p>float16 </p>
</td></tr>
<tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940ea33a85906dc39ca7b7987986fea772770"></a>OH_NN_FLOAT32&#160;</td><td class="fielddoc"><p>float32 </p>
</td></tr>
<tr><td class="fieldname"><a id="gga917dcf1cb1e7bb745ccf984e9e67940eac9b9729bd0b4120cb181fa6223c899c6"></a>OH_NN_FLOAT64&#160;</td><td class="fielddoc"><p>float64 </p>
</td></tr>
</table>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00290">290</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="gae99ff81dfe03a24f714261009b5d2bff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gae99ff81dfe03a24f714261009b5d2bff">&#9670;&nbsp;</a></span>OH_NN_DeviceType</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="group___neural_nework_runtime.html#gae99ff81dfe03a24f714261009b5d2bff">OH_NN_DeviceType</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines device types. </p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a id="ggae99ff81dfe03a24f714261009b5d2bffa2b0a4607cc69f1f18fb4279eb8693646"></a>OH_NN_OTHERS&#160;</td><td class="fielddoc"><p>Devices that are not CPU, GPU, or dedicated accelerator </p>
</td></tr>
<tr><td class="fieldname"><a id="ggae99ff81dfe03a24f714261009b5d2bffac26cc84ea920f877dff72fad57eb5240"></a>OH_NN_CPU&#160;</td><td class="fielddoc"><p>CPU device </p>
</td></tr>
<tr><td class="fieldname"><a id="ggae99ff81dfe03a24f714261009b5d2bffa499d4c3f35ea7bed3e45591675697bd7"></a>OH_NN_GPU&#160;</td><td class="fielddoc"><p>GPU device </p>
</td></tr>
<tr><td class="fieldname"><a id="ggae99ff81dfe03a24f714261009b5d2bffab2e97cc2878a8915f7e9eef4e29212de"></a>OH_NN_ACCELERATOR&#160;</td><td class="fielddoc"><p>Dedicated hardware accelerator </p>
</td></tr>
</table>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00273">273</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga467f4b3c524b8ec4ebf3c86d680f7f59"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga467f4b3c524b8ec4ebf3c86d680f7f59">&#9670;&nbsp;</a></span>OH_NN_Format</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="group___neural_nework_runtime.html#ga467f4b3c524b8ec4ebf3c86d680f7f59">OH_NN_Format</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the layout type of tensor data. </p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>2.0 </dd></dl>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a id="gga467f4b3c524b8ec4ebf3c86d680f7f59ae4c684f3bfbcd02e944326f09ab9c278"></a>OH_NN_FORMAT_NONE&#160;</td><td class="fielddoc"><p>The tensor does not have a specific layout type (such as scalar or vector). </p>
</td></tr>
<tr><td class="fieldname"><a id="gga467f4b3c524b8ec4ebf3c86d680f7f59a9c530242beecb143c39419ccd6f52287"></a>OH_NN_FORMAT_NCHW&#160;</td><td class="fielddoc"><p>The tensor arranges data in NCHW format. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga467f4b3c524b8ec4ebf3c86d680f7f59a858b34abff99a34d4c1cd38565ab92bd"></a>OH_NN_FORMAT_NHWC&#160;</td><td class="fielddoc"><p>The tensor arranges data in NHWC format. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga467f4b3c524b8ec4ebf3c86d680f7f59ac446cc4ddb5cc377a0fdbeadf68f4bf8"></a>OH_NN_FORMAT_ND&#160;</td><td class="fielddoc"><p>The tensor arranges data in ND format. </p><dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
</td></tr>
</table>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00254">254</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga22fc23735fd12894d547805d320e01e3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga22fc23735fd12894d547805d320e01e3">&#9670;&nbsp;</a></span>OH_NN_FuseType</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="group___neural_nework_runtime.html#ga22fc23735fd12894d547805d320e01e3">OH_NN_FuseType</a> : int8_t</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines activation function types in the fusion operator. </p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a id="gga22fc23735fd12894d547805d320e01e3a58dc2d253afa491754e3e2a13423b3fa"></a>OH_NN_FUSED_NONE&#160;</td><td class="fielddoc"><p>The fusion activation function is not specified. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga22fc23735fd12894d547805d320e01e3a87d4e0c86baf35ff90d4ca2b47e4e3de"></a>OH_NN_FUSED_RELU&#160;</td><td class="fielddoc"><p>Fusion relu activation function </p>
</td></tr>
<tr><td class="fieldname"><a id="gga22fc23735fd12894d547805d320e01e3abac42b4176ce6a8e293e71d34d6cb54a"></a>OH_NN_FUSED_RELU6&#160;</td><td class="fielddoc"><p>Fusion relu6 activation function </p>
</td></tr>
</table>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00239">239</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="gab333095762fd39a9f952523d717b95de"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gab333095762fd39a9f952523d717b95de">&#9670;&nbsp;</a></span>OH_NN_OperationType</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="group___neural_nework_runtime.html#gab333095762fd39a9f952523d717b95de">OH_NN_OperationType</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines operator types. </p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>2.0 </dd></dl>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deaf9287433d92cc8e3277f14a775a2a284"></a>OH_NN_OPS_ADD&#160;</td><td class="fielddoc"><p>Returns the tensor of the sum of the elements corresponding to two input tensors.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: first input tensor, of the Boolean or number type.</li>
<li><b>input2</b>: second input tensor, whose data type must be the same as that of the first tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: sum of <b>input1</b> and <b>input2</b>. The data shape is the same as that of the input after broadcasting, and the data type is the same as that of the input with a higher precision. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea06f5fae28fb297882a43beab9176f1bc"></a>OH_NN_OPS_AVG_POOL&#160;</td><td class="fielddoc"><p>Apply 2D average pooling to the input tensor, which now must be in NHWC format. The int8 quantization input is supported.</p>
<p>If the input contains the <b>padMode</b> parameter:</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>kernelSize</b> indicates the kernel size used to obtain the average value. It is an int array [kernelHeight, kernelWidth]. The first number indicates the kernel height, and the second number indicates the kernel width.</li>
<li><b>strides</b> indicates the distance of kernel moving. The value is an int array [strideHeight, strideWidth]. The first number indicates the moving step in height, and the second number indicates the moving step in width.</li>
<li><b>padMode</b>: padding mode, which is optional. The value is of the int type and can be <b>0</b> (same) or <b>1</b> (valid). The nearest neighbor value is used for padding. <b>0</b> (same): The height and width of the output are the same as those of the input. The total and padding quantity is calculated horizontally and vertically and evenly distributed to the top, bottom, left, right if possible. Otherwise, the last additional padding will be completed from the bottom and right. <b>1</b> (valid): The possible maximum height and width of the output will be returned in case of no padding. Excessive pixels will be discarded.</li>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
<li><b>global</b> Whether to do global pooling.</li>
<li><b>roundMode</b> Boundary handling method. When the pool cannot completely cover the input feature map, the output feature map is rounded up, 0 means round down, 1 means round up.</li>
</ul>
<p>If the input contains the <b>padList</b> parameter:</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>kernelSize</b> indicates the kernel size used to obtain the average value. It is an int array [kernelHeight, kernelWidth]. The first number indicates the kernel height, and the second number indicates the kernel width.</li>
<li><b>strides</b> indicates the distance of kernel moving. The value is an int array [strideHeight, strideWidth]. The first number indicates the moving step in height, and the second number indicates the moving step in width.</li>
<li><b>padList</b>: padding around <b>input</b>. It is an int array [top, bottom, left, right], and the nearest neighbor values are used for padding.</li>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
<li><b>global</b> Whether to do global pooling.</li>
<li><b>roundMode</b> Boundary handling method. When the pool cannot completely cover the input feature map, the output feature map is rounded up, 0 means round down, 1 means round up.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: average pooling result of the input. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deaf725e0f19333d7f2979c57a91816f0ce"></a>OH_NN_OPS_BATCH_NORM&#160;</td><td class="fielddoc"><p>Performs batch normalization on the input tensors. Apply a transformation to keep the average output close to 0 and the output standard deviation close to 1.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor of shape [N, ..., C]. The <em>n</em>th dimension is the number of channels.</li>
<li><b>scale</b>: 1D tensor of the scaling factor used to scale the first normalized tensor.</li>
<li><b>offset</b>: 1D tensor used to move to the first normalized tensor.</li>
<li><b>mean</b>: 1D tensor of the overall mean value. It is used only for inference. In case of training, this parameter must be left empty.</li>
<li><b>variance</b>: 1D tensor used for the overall variance. It is used only for inference. In case of training, this parameter must be left empty.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>epsilon</b>: fixed small additional value.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional output tensor whose shape and data type are the same as those of the input. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deab7710e95a5742143cea3eaf132e2cb53"></a>OH_NN_OPS_BATCH_TO_SPACE_ND&#160;</td><td class="fielddoc"><p>Divides the batch dimension of a 4D tensor into small blocks by <b>blockShape</b>, and interleaves these blocks back into the spatial dimension.</p>
<p>Parameters:</p>
<ul>
<li><b>input</b>: input tensor. The dimension will be divided into small blocks, and these blocks will be interleaved into the spatial dimension.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>blockSize</b>: size of each block to be interleaved into the spatial dimension. The value is an array [heightBlock, widthBlock].</li>
<li><b>crops</b>: elements truncated from the spatial dimension of the output. The value is a 2D array [[crop0Start, crop0End], [crop1Start, crop1End]] with the shape of (2, 2).</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>. Assume that the shape of <b>input</b> is (n,h,w,c) and the shape of <b>output</b> is (n',h',w',c'): n' = n / (blockShape[0] * blockShape[1]) h' = h * blockShape[0] - crops[0][0] - crops[0][1] w' = w * blockShape[1] - crops[1][0] - crops[1][1] c'= c </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deabe2bf16f2542ee10f05e6a4f0f37c324"></a>OH_NN_OPS_BIAS_ADD&#160;</td><td class="fielddoc"><p>Offsets the data in each dimension of the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: input tensor, which can have two to five dimensions.</li>
<li><b>bias</b>: offset of the number of input dimensions.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: sum of the input tensor and the bias in each dimension. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea53beaecf4c2d785ee03b868645d34cc4"></a>OH_NN_OPS_CAST&#160;</td><td class="fielddoc"><p>Converts the data type in the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: input tensor.</li>
<li><b>type</b>: converted data type.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: converted tensor. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deaf037dab1e691c9f59e5745958da982a7"></a>OH_NN_OPS_CONCAT&#160;</td><td class="fielddoc"><p>Connects tensors in a specified dimension.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>N</em> input tensors.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: dimension for connecting tensors.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: result of connecting <em>N</em> tensors along the axis. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea431b2053449cef3c3418869b0f801301"></a>OH_NN_OPS_CONV2D&#160;</td><td class="fielddoc"><p>2D convolutional layer.</p>
<p>If the input contains the <b>padMode</b> parameter:</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: input tensor.</li>
<li><b>weight</b>: convolution weight in [outChannel, kernelHeight, kernelWidth, inChannel/group] format. The value of <b>inChannel</b> must be exactly divided by the value of <b>group</b>.</li>
<li><b>bias</b>: bias of the convolution. It is an array with a length of <b>[outChannel]</b>. In quantization scenarios, the <b>bias</b> parameter does not require quantization parameters. The quantization version requires data input of the <b>OH_NN_INT32</b> type. The actual quantization parameters are determined by <b>input</b> and <b>weight</b>.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>stride</b>: movement stride of the convolution kernel in height and width. It is an int array [strideHeight, strideWidth].</li>
<li><b>dilation</b>: dilation size of the convolution kernel in height and width. It is an int array [dilationHeight, dilationWidth]. The value must be greater than or equal to <b>1</b> and cannot exceed the height and width of <b>input</b>.</li>
<li><p class="startli"><b>padMode</b>: padding mode of <b>input</b>. The value is of the int type and can be <b>0</b> (same) or <b>1</b> (valid). <b>0</b> (same): The height and width of the output are the same as those of the input. The total padding quantity is calculated horizontally and vertically and evenly distributed to the top, bottom, left, and right if possible. Otherwise, the last additional padding will be completed from the bottom and right.</p>
<p class="startli"><b>1</b> (valid): The possible maximum height and width of the output will be returned in case of no padding. The excessive pixels will be discarded.</p>
</li>
<li><b>group</b>: number of groups in which the input is divided by <b>inChannel</b>. The value is of the int type. If <b>group</b> is <b>1</b>, it is a conventional convolution. If <b>group</b> is greater than <b>1</b> and less than or equal to <b>inChannel</b>, it is a group convolution.</li>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
</ul>
<p>If the input contains the <b>padList</b> parameter:</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: input tensor.</li>
<li><b>weight</b>: convolution weight in [outChannel, kernelHeight, kernelWidth, inChannel/group] format. The value of <b>inChannel</b> must be exactly divided by the value of <b>group</b>.</li>
<li><b>bias</b>: bias of the convolution. It is an array with a length of <b>[outChannel]</b>. In quantization scenarios, the <b>bias</b> parameter does not require quantization parameters. The quantization version requires data input of the <b>OH_NN_INT32</b> type. The actual quantization parameters are determined by <b>input</b> and <b>weight</b>.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>stride</b>: movement stride of the convolution kernel in height and width. It is an int array [strideHeight, strideWidth].</li>
<li><b>dilation</b>: dilation size of the convolution kernel in height and width. It is an int array [dilationHeight, dilationWidth]. The value must be greater than or equal to <b>1</b> and cannot exceed the height and width of <b>input</b>.</li>
<li><b>padList</b>: padding around <b>input</b>. It is an int array [top, bottom, left, right].</li>
<li><b>group</b>: number of groups in which the input is divided by <b>inChannel</b>. The value is of the int type. If <b>group</b> is <b>1</b>, it is a conventional convolution. If <b>group</b> is <b>inChannel</b>, it is depthwiseConv2d. In this case, group==inChannel==outChannel. If <b>group</b> is greater than <b>1</b> and less than <b>inChannel</b>, it is a group convolution. In this case, outChannel==group.</li>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: convolution computing result. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea39b5f3514d5064d6f97d0b73d0fa80ce"></a>OH_NN_OPS_CONV2D_TRANSPOSE&#160;</td><td class="fielddoc"><p>2D convolution transposition.</p>
<p>If the input contains the <b>padMode</b> parameter:</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: input tensor.</li>
<li><b>weight</b>: convolution weight in [outChannel, kernelHeight, kernelWidth, inChannel/group] format. The value of <b>inChannel</b> must be exactly divided by the value of <b>group</b>.</li>
<li><b>bias</b>: bias of the convolution. It is an array with a length of <b>[outChannel]</b>. In quantization scenarios, the <b>bias</b> parameter does not require quantization parameters. The quantization version requires data input of the <b>OH_NN_INT32</b> type. The actual quantization parameters are determined by <b>input</b> and <b>weight</b>.</li>
<li><b>stride</b>: movement stride of the convolution kernel in height and width. It is an int array [strideHeight, strideWidth].</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>dilation</b>: dilation size of the convolution kernel in height and width. It is an int array [dilationHeight, dilationWidth]. The value must be greater than or equal to <b>1</b> and cannot exceed the height and width of <b>input</b>.</li>
<li><b>padMode</b>: padding mode of <b>input</b>. The value is of the int type and can be <b>0</b> (same) or <b>1</b> (valid). <b>0</b> (same): The height and width of the output are the same as those of the input. The total padding quantity is calculated horizontally and vertically and evenly distributed to the top, bottom, left, and right if possible. Otherwise, the last additional padding will be completed from the bottom and right. <b>1</b> (valid): The possible maximum height and width of the output will be returned in case of no padding. The excessive pixels will be discarded.</li>
<li><b>group</b>: number of groups in which the input is divided by <b>inChannel</b>. The value is of the int type. If <b>group</b> is <b>1</b>, it is a conventional convolution. If <b>group</b> is greater than <b>1</b> and less than or equal to <b>inChannel</b>, it is a group convolution.</li>
<li><b>outputPads</b>: padding along the height and width of the output tensor. The value is an int or a tuple. It can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a dimension must be less than the stride along this dimension.</li>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
</ul>
<p>If the input contains the <b>padList</b> parameter:</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: input tensor.</li>
<li><b>weight</b>: convolution weight in [outChannel, kernelHeight, kernelWidth, inChannel/group] format. The value of <b>inChannel</b> must be exactly divided by the value of <b>group</b>.</li>
<li><b>bias</b>: bias of the convolution. It is an array with a length of <b>[outChannel]</b>. In quantization scenarios, the <b>bias</b> parameter does not require quantization parameters. The quantization version requires data input of the <b>OH_NN_INT32</b> type. The actual quantization parameters are determined by <b>input</b> and <b>weight</b>.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>stride</b>: movement stride of the convolution kernel in height and width. It is an int array [strideHeight, strideWidth].</li>
<li><b>dilation</b>: dilation size of the convolution kernel in height and width. It is an int array [dilationHeight, dilationWidth]. The value must be greater than or equal to <b>1</b> and cannot exceed the height and width of <b>input</b>.</li>
<li><b>padList</b>: padding around <b>input</b>. It is an int array [top, bottom, left, right].</li>
<li><b>group</b>: number of groups in which the input is divided by <b>inChannel</b>. The value is of the int type. If <b>group</b> is <b>1</b>, it is a conventional convolution. If <b>group</b> is greater than <b>1</b> and less than or equal to <b>inChannel</b>, it is a group convolution.</li>
<li><b>outputPads</b>: padding along the height and width of the output tensor. The value is an int or a tuple. It can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a dimension must be less than the stride along this dimension.</li>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: computing result after convolution and transposition. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea17d3f84d4fbb36da41c09e00205621b1"></a>OH_NN_OPS_DEPTHWISE_CONV2D_NATIVE&#160;</td><td class="fielddoc"><p>2D depthwise separable convolution.</p>
<p>If the input contains the <b>padMode</b> parameter:</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: input tensor.</li>
<li><b>weight</b>: convolution weight in [outChannel, kernelHeight, kernelWidth, 1] format. <b>outChannel</b> is equal to <b>channelMultiplier</b> multiplied by <b>inChannel</b>.</li>
<li><b>bias</b>: bias of the convolution. It is an array with a length of <b>[outChannel]</b>. In quantization scenarios, the <b>bias</b> parameter does not require quantization parameters. The quantization version requires data input of the <b>OH_NN_INT32</b> type. The actual quantization parameters are determined by <b>input</b> and <b>weight</b>.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>stride</b>: movement stride of the convolution kernel in height and width. It is an int array [strideHeight, strideWidth].</li>
<li><b>dilation</b>: dilation size of the convolution kernel in height and width. It is an int array [dilationHeight, dilationWidth]. The value must be greater than or equal to <b>1</b> and cannot exceed the height and width of <b>input</b>.</li>
<li><p class="startli"><b>padMode</b>: padding mode of <b>input</b>. The value is of the int type and can be <b>0</b> (same) or <b>1</b> (valid). <b>0</b> (same): The height and width of the output are the same as those of the input. The total padding quantity is calculated horizontally and vertically and evenly distributed to the top, bottom, left, and right if possible. Otherwise, the last additional padding will be completed from the bottom and right.</p>
<p class="startli"><b>1</b> (valid): The possible maximum height and width of the output will be returned in case of no padding. The excessive pixels will be discarded.</p>
</li>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
</ul>
<p>If the input contains the <b>padList</b> parameter:</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: input tensor.</li>
<li><b>weight</b>: convolution weight in [outChannel, kernelHeight, kernelWidth, 1] format. <b>outChannel</b> is equal to <b>channelMultiplier</b> multiplied by <b>inChannel</b>.</li>
<li><b>bias</b>: bias of the convolution. It is an array with a length of <b>[outChannel]</b>. In quantization scenarios, the <b>bias</b> parameter does not require quantization parameters. The quantization version requires data input of the <b>OH_NN_INT32</b> type. The actual quantization parameters are determined by <b>input</b> and <b>weight</b>.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>stride</b>: movement stride of the convolution kernel in height and width. It is an int array [strideHeight, strideWidth].</li>
<li><b>dilation</b>: dilation size of the convolution kernel in height and width. It is an int array [dilationHeight, dilationWidth]. The value must be greater than or equal to <b>1</b> and cannot exceed the height and width of <b>input</b>.</li>
<li><b>padList</b>: padding around <b>input</b>. It is an int array [top, bottom, left, right].</li>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: convolution computing result. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea422ebc8802510175df7d3afd499c8f2b"></a>OH_NN_OPS_DIV&#160;</td><td class="fielddoc"><p>Divides two input scalars or tensors.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: first input, which is a number, a bool, or a tensor whose data type is number or Boolean.</li>
<li><b>input2</b>: second input, which must meet the following requirements: If the first input is a tensor, the second input can be a real number, a Boolean value, or a tensor whose data type is real number or Boolean value. If the first input is a real number or Boolean value, the second input must be a tensor whose data type is real number or Boolean value.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: result of dividing <b>input1</b> by <b>input2</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea704612af75062b022553433eb1a69cac"></a>OH_NN_OPS_ELTWISE&#160;</td><td class="fielddoc"><p>Sets parameters to perform product (dot product), sum (addition and subtraction), or max (larger value) on the input.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: first input tensor.</li>
<li><b>input2</b>: second input tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>mode</b>: operation mode. The value is an enumerated value.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: computing result, which has the same data type and shape of <b>input1</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea9fa75bc04f31a73255cf907d5f7cbfff"></a>OH_NN_OPS_EXPAND_DIMS&#160;</td><td class="fielddoc"><p>Adds an additional dimension to a tensor in the given dimension.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: input tensor.</li>
<li><b>axis</b>: index of the dimension to be added. The value is of the int32_t type and must be a constant in the range [-dim-1, dim].</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: tensor after dimension expansion. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea1e42979e38d9c8e77302ca9204a1940f"></a>OH_NN_OPS_FILL&#160;</td><td class="fielddoc"><p>Creates a tensor of the specified dimensions and fills it with a scalar.</p>
<p>Inputs:</p>
<ul>
<li><b>value</b>: scalar used to fill the tensor.</li>
<li><b>shape</b>: dimensions of the tensor to be created.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: generated tensor, which has the same data type as <b>value</b>. The tensor shape is specified by the <b>shape</b> parameter. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deab4b9fc3159f612337a7e03d671e30e58"></a>OH_NN_OPS_FULL_CONNECTION&#160;</td><td class="fielddoc"><p>Full connection. The entire input is used as the feature map for feature extraction.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: full-connection input tensor.</li>
<li><b>weight</b>: weight tensor for a full connection.</li>
<li><b>bias</b>: full-connection bias. In quantization scenarios, no quantized parameter is required for this parameter. If quantization is required, the data must be of the OH_NN_INT32 type. The actual quantization parameters are determined by <b>input</b> and <b>weight</b>.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
<li><b>hasBias</b> Whether to use the bias.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: computed tensor.</li>
</ul>
<p>If the input contains the <b>axis</b> parameter or <b>useAxis</b> parameter:</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: full-connection input tensor.</li>
<li><b>weight</b>: weight tensor for a full connection.</li>
<li><b>bias</b>: full-connection bias. In quantization scenarios, no quantized parameter is required for this parameter. If quantization is required, the data must be of the OH_NN_INT32 type. The actual quantization parameters are determined by <b>input</b> and <b>weight</b>.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: axis in which the full connection is applied. The specified axis and its following axes are converted into a 1D tensor for applying the full connection.</li>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
<li><b>useAxis</b> Whether to use the axis.</li>
<li><b>hasBias</b> Whether to use the bias.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: computed tensor. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea9c2481b53a707fffbddb81a4925d1d0b"></a>OH_NN_OPS_GATHER&#160;</td><td class="fielddoc"><p>Returns the slice of the input tensor based on the specified index and axis.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: tensor to be sliced.</li>
<li><b>inputIndices</b>: indices of the specified input on the axis. The value is an array of the int type and must be in the range [0,input.shape[axis]).</li>
<li><b>axis</b>: axis on which <b>input</b> is sliced. The value is an array with one element of the int32_t type.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: sliced tensor. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deafde613b42930bbef3ebb9360feaa8a2b"></a>OH_NN_OPS_HSWISH&#160;</td><td class="fielddoc"><p>Calculate the <b>Hswish</b> activation value of the input.</p>
<p>Inputs:</p>
<ul>
<li>An <em>n</em>-dimensional input tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional <b>Hswish</b> activation value. The data type is the same as that of <b>shape</b> and <b>input</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea5e7d9e12b6bb8c24d246d9434dbc065d"></a>OH_NN_OPS_LESS_EQUAL&#160;</td><td class="fielddoc"><p>For <b>input1</b> and <b>input2</b>, calculate the result of input1[i]&lt;=input2[i] for each pair of elements, where i is the index of each element in the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>, can be a real number, Boolean value, or tensor whose data type is real number or OH_NN_BOOL.</li>
<li><b>input2</b>, can be a real number or a Boolean value if <b>input1</b> is a tensor and must be a tensor with the data type of real number or OH_NN_BOOL if <b>input1</b> is not a tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor of the data type OH_NN_BOOL. When a quantization model is used, the quantization parameters of the output cannot be omitted. However, values of the quantization parameters do not affect the result. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea03082b437d4d81c2dac5d6a51e6f7ece"></a>OH_NN_OPS_MATMUL&#160;</td><td class="fielddoc"><p>Calculate the inner product of <b>input1</b> and <b>input2</b>.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: <em>n</em>-dimensional input tensor.</li>
<li><b>input2</b>: <em>n</em>-dimensional input tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>TransposeX</b>: Boolean value indicating whether to transpose <b>input1</b>.</li>
<li><b>TransposeY</b>: Boolean value indicating whether to transpose <b>input2</b>.</li>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: inner product obtained after calculation. In case of type!=NN_UNKNOWN, the output data type is determined by <b>type</b>. In case of type==NN_UNKNOWN, the output data type depends on the data type converted during computing of <b>inputX</b> and <b>inputY</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea7d882ff508634ee88d8dd53b6fca7209"></a>OH_NN_OPS_MAXIMUM&#160;</td><td class="fielddoc"><p>Calculates the maximum of <b>input1</b> and <b>input2</b> element-wise. The inputs of <b>input1</b><br />
and <b>input2</b> comply with the implicit type conversion rules to make the data types consistent. The inputs must be two tensors or one tensor and one scalar. When the inputs are two tensors, their data types cannot be both OH_NN_BOOL. Their shapes can be broadcast to the same size. When the inputs are one tensor and one scalar, the scalar must be a constant.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: <em>n</em>-dimensional input tensor of the real number or OH_NN_BOOL type.</li>
<li><b>input2</b>: <em>n</em>-dimensional input tensor of the real number or OH_NN_BOOL type.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional output tensor. The <b>shape</b> and data type of <b>output</b> are the same as those of the two inputs with a higher precision. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deabb2448db4cca23082694cd1ef14a9580"></a>OH_NN_OPS_MAX_POOL&#160;</td><td class="fielddoc"><p>Applies 2D maximum pooling to the input tensor.</p>
<p>If the input contains the <b>padMode</b> parameter:</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>kernelSize</b>: kernel size used to obtain the maximum. It is an int array [kernelHeight, kernelWidth]. The first number indicates the kernel height, and the second number indicates the kernel width.</li>
<li><b>strides</b> indicates the distance of kernel moving. The value is an int array [strideHeight, strideWidth]. The first number indicates the moving step in height, and the second number indicates the moving step in width.</li>
<li><b>padMode</b>: padding mode, which is optional. The value is of the int type and can be <b>0</b> (same) or <b>1</b> (valid). The nearest neighbor value is used for padding. <b>0</b> (same): The height and width of the output are the same as those of the input. The total padding quantity is calculated horizontally and vertically and evenly distributed to the top, bottom, left, and right if possible. Otherwise, the last additional padding will be completed from the bottom and right. <b>1</b> (valid): The possible maximum height and width of the output will be returned in case of no padding. The excessive pixels will be discarded.</li>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
<li><b>global</b> Whether to do global pooling.</li>
<li><b>roundMode</b> Boundary handling method. When the pool cannot completely cover the input feature map, the output feature map is rounded up, 0 means round down, 1 means round up.</li>
</ul>
<p>If the input contains the <b>padList</b> parameter:</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>kernelSize</b>: kernel size used to obtain the maximum. It is an int array [kernelHeight, kernelWidth]. The first number indicates the kernel height, and the second number indicates the kernel width.</li>
<li><b>strides</b> indicates the distance of kernel moving. The value is an int array [strideHeight, strideWidth]. The first number indicates the moving step in height, and the second number indicates the moving step in width.</li>
<li><b>padList</b>: padding around <b>input</b>. It is an int array [top, bottom, left, right], and the nearest neighbor values are used for padding.</li>
<li><b>activationType</b> is an integer constant which is contained in <b>FuseType</b>. The specified activation function is called before output.</li>
<li><b>global</b> Whether to do global pooling.</li>
<li><b>roundMode</b> Boundary handling method. When the pool cannot completely cover the input feature map, the output feature map is rounded up, 0 means round down, 1 means round up.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: tensor obtained after maximum pooling is applied to the input. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deac661eb9ec8a6cbf05bf7637040f31213"></a>OH_NN_OPS_MUL&#160;</td><td class="fielddoc"><p>Multiplies elements in the same positions of <b>input1</b> and <b>input2</b> to obtain the output. If <b>input1</b> and <b>input2</b> have different shapes, expand them to the same shape through broadcast and then perform multiplication.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: <em>n</em>-dimensional tensor.</li>
<li><b>input2</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Product of each element of <b>input1</b> and <b>input2</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deab754be46b448538f1ecb54fa1d1ef14f"></a>OH_NN_OPS_ONE_HOT&#160;</td><td class="fielddoc"><p>Generates a one-hot tensor based on the positions specified by <b>indices</b>. The positions specified by <b>indices</b> are determined by <b>onValue</b>, and other positions are determined by <b>offValue</b>.</p>
<p>Inputs:</p>
<ul>
<li><b>indices</b>: <em>n</em>-dimensional tensor. Each element in <b>indices</b> determines the position of <b>onValue</b> in each one-hot vector.</li>
<li><b>depth</b>: integer scalar that determines the depth of the one-hot vector. The value of <b>depth</b> must be greater than <b>0</b>.</li>
<li><b>onValue</b>: scalar that specifies a valid value in the one-hot vector.</li>
<li><b>offValue</b>: scalar that specifies the values of other posistions in the one-hot vector except the valid value.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: integer scalar that specifies the dimension for inserting the one-hot. Assume that the shape of <b>indices</b> is [N, C], and the value of <b>depth</b> is D. When <b>axis</b> is <b>0</b>, the shape of the output is [D, N, C]. When <b>axis</b> is <b>-1</b>, the shape of the output is [N, C, D]. When <b>axis</b> is <b>1</b>, the shape of the output is [N, D, C].</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: (<em>n</em>+1)-dimensional tensor if <b>indices</b> is an <em>n</em>-dimensional tensor. The output shape is determined by <b>indices</b> and <b>axis</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deaad110b33f690e01428552db45656516c"></a>OH_NN_OPS_PAD&#160;</td><td class="fielddoc"><p>Pads <b>inputX</b> in the specified dimensions.</p>
<p>Inputs:</p>
<ul>
<li><b>inputX</b>: <em>n</em>-dimensional tensor in [BatchSize, ...] format.</li>
<li><b>paddings</b>: 2D tensor that specifies the length to pad in each dimension. The shape is [n, 2]. For example, <b>paddings[i][0]</b> indicates the number of paddings to be added preceding <b>inputX</b> in the <em>i</em>th dimension. <b>paddings[i][1]</b> indicates the number of paddings to be added following <b>inputX</b> in the <em>i</em>th dimension.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>constantValue</b>: value to be added to the pad operation. The value is a constant with the same data type as <b>inputX</b>.</li>
<li><b>paddingMode</b>: Padding mode.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor after padding, with the same dimensions and data type as <b>inputX</b>. The shape is determined by <b>inputX</b> and <b>paddings</b>. output.shape[i] = input.shape[i] + paddings[i][0]+paddings[i][1] </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea6f4738d88207a358a789312813907dee"></a>OH_NN_OPS_POW&#160;</td><td class="fielddoc"><p>Calculates the <b>y</b> power of each element in <b>input</b>. The inputs must be two tensors or one tensor and one scalar. When the inputs are two tensors, their data types cannot be both OH_NN_BOOL, and their shapes must be the same. When the inputs are one tensor and one scalar, the scalar must be a constant.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: real number, Boolean value, or tensor whose data type is real number or OH_NN_BOOL.</li>
<li><b>y</b>: real number, Boolean value, or tensor whose data type is real number or OH_NN_BOOL.</li>
</ul>
<p>Parameters:</p><ul>
<li><b>scale</b>: A OH_NN_FLOAT32 scalar that represents the factor of the scale blend.</li>
<li><b>shift</b>: A OH_NN_FLOAT32 scalar that represents the bias of the scale blend.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: tensor, whose shape is determined by the shape of <b>input</b> and <b>y</b> after broadcasting. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deab6653766d548fdcd4021362ab0323049"></a>OH_NN_OPS_SCALE&#160;</td><td class="fielddoc"><p>Scales a tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
<li><b>scale</b>: scaling tensor.</li>
<li><b>bias</b>: bias tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: dimensions to be scaled.</li>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: scaled <em>n</em>-dimensional tensor, whose data type is the same as that of <b>input</b> and shape is determined by <b>axis</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deacf1aa6c15fbb881835bf5c25123d1bac"></a>OH_NN_OPS_SHAPE&#160;</td><td class="fielddoc"><p>Calculates the shape of the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: integer array representing the dimensions of the input tensor. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deaeac8384bd9cd516fab8a6d3fe59ef2ee"></a>OH_NN_OPS_SIGMOID&#160;</td><td class="fielddoc"><p>Applies the <b>sigmoid</b> operation to the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: result of the <b>sigmoid</b> operation. It is an <em>n</em>-dimensional tensor with the same data type and shape as <b>input</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea1ff4a90fe0a5a60e586b77142202b537"></a>OH_NN_OPS_SLICE&#160;</td><td class="fielddoc"><p>Slices a tensor of the specified size from the input in each dimension.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor.</li>
<li><b>begin</b>: start of the slice, which is an array of integers greater than or equal to 0.</li>
<li><b>size</b>: slice length, which is an array of integers greater than or equal to 0. Assume that a dimension is <b>i</b> and 1&lt;=size[i]&lt;=input.shape[i]-begin[i].</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axes</b>: Dimensions on which the tensor is sliced.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor obtained by slicing. The <b>TensorType</b>, shape, and size of the output are the same as those of the input. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea5e423349b5500197c576620264fa510d"></a>OH_NN_OPS_SOFTMAX&#160;</td><td class="fielddoc"><p>Applies the <b>softmax</b> operation to the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: dimension in which the <b>softmax</b> operation is performed. The value is of the int64 type. It is an integer in the range [-n, n).</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: result of the <b>softmax</b> operation. It is an <em>n</em>-dimensional tensor with the same data type and shape as <b>input</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea3c0c046024d21cc6e4f4087fe5a36e05"></a>OH_NN_OPS_SPACE_TO_BATCH_ND&#160;</td><td class="fielddoc"><p>Divides a 4D tensor into small blocks and combines these blocks in the original batch. The number of blocks is <b>blockShape[0]</b> multiplied by <b>blockShape[1]</b>.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: 4D tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>blockShape</b>: a pair of integers. Each of them is greater than or equal to <b>1</b>.</li>
<li><b>paddings</b>: a pair of arrays. Each of them consists of two integers. The four integers that from <b>paddings</b> must be greater than or equal to <b>0</b>. <b>paddings[0][0]</b> and <b>paddings[0][1]</b> specify the number of paddings in the third dimension, and <b>paddings[1][0]</b> and <b>paddings[1][1]</b> specify the number of paddings in the fourth dimension.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: 4D tensor with the same data type as <b>input</b>. The shape is determined by <b>input</b>, <b>blockShape</b>, and <b>paddings</b>. Assume that the input shape is [n,c,h,w], then: output.shape[0] = n * blockShape[0] * blockShape[1] output.shape[1] = c output.shape[2] = (h + paddings[0][0] + paddings[0][1]) / blockShape[0] output.shape[3] = (w + paddings[1][0] + paddings[1][1]) / blockShape[1] (h + paddings[0][0] + paddings[0][1]) and (w + paddings[1][0] + paddings[1][1]) is exactly divisible by (h + paddings[0][0] + paddings[0][1]) and (w + paddings[1][0] + paddings[1][1]). </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deaa7996e244b633278573fa9939df5aa2a"></a>OH_NN_OPS_SPLIT&#160;</td><td class="fielddoc"><p>Splits the input into multiple tensors along the axis dimension. The number of tensors is specified by <b>outputNum</b>.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>outputNum</b>: number of output tensors. The data type is long.</li>
<li><b>sizeSplits</b>: size of each tensor split from the input. The value is a 1D tensor of the int type. If <b>sizeSplits</b> is empty, the input will be evenly split into tensors of the same size. In this case, <b>input.shape[axis]</b> can be exactly divisible by <b>outputNum</b>. If <b>sizeSplits</b> is not empty, the sum of all its elements must be equal to <b>input.shape[axis]</b>.</li>
<li><b>axis</b>: splitting dimension of the int type.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>outputs</b>: array of <em>n</em>-dimensional tensors, with the same data type and dimensions. The data type of each tensor is the same as that of <b>input</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea554b87de298fcb4f05ebbdc96508d521"></a>OH_NN_OPS_SQRT&#160;</td><td class="fielddoc"><p>Calculates the square root of a tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: square root of the input. It is an <em>n</em>-dimensional tensor with the same data type and shape as <b>input</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea1d2c4306c8142686b96a20c6b6be2527"></a>OH_NN_OPS_SQUARED_DIFFERENCE&#160;</td><td class="fielddoc"><p>Calculates the square of the difference between two tensors. The <b>SquaredDifference</b> operator supports tensor and tensor subtraction. If two tensors have different <b>TensorTypes</b>, the Sub operator converts the low-precision tensor to a high-precision one. If two tensors have different shapes, the two tensors can be extended to tensors with the same shape through broadcast.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: minuend, which is a tensor of the OH_NN_FLOAT16, OH_NN_FLOAT32, OH_NN_INT32, or OH_NN_BOOL type.</li>
<li><b>input2</b>: subtrahend, which is a tensor of the OH_NN_FLOAT16, OH_NN_FLOAT32, OH_NN_INT32, or OH_NN_BOOL type.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: square of the difference between two inputs. The output shape is determined by<b>input1</b> and <b>input2</b>. If they have the same shape, the output tensor has the same shape as them. If they have different shapes, perform the broadcast operation on <b>input1</b> and <b>input2</b> and perform subtraction. <b>TensorType</b> of the output is the same as that of the input tensor with higher precision. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deab368b8662191cf287bb2f6c17bb0dcdc"></a>OH_NN_OPS_SQUEEZE&#160;</td><td class="fielddoc"><p>Removes the dimension with a length of 1 from the specified axis. The int8 quantization input is supported. Assume that the input shape is [2, 1, 1, 2, 2] and axis is [0,1], the output shape is [2, 1, 2, 2], which means the dimension whose length is 0 between dimensions 0 and dimension 1 is removed.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: dimension to be removed. The value is of int64_t type and can be an integer in the range [-n, n) or an array.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: output tensor. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea656d8434c662ae650173a417e90f5d0e"></a>OH_NN_OPS_STACK&#160;</td><td class="fielddoc"><p>Stacks multiple tensors along the specified axis. If each tensor has <em>n</em> dimensions before stacking, the output tensor will have <em>n</em>+1 dimensions.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: input for stacking, which can contain multiple <em>n</em>-dimensional tensors. Each of them must have the same shape and type.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: dimension for tensor stacking, which is an integer. The value range is [-(n+1),(n+1)), which means a negative number is allowed.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: stacking result of the input along the axis dimension. The value is an <em>n</em>+1-dimensional tensor and has the same <b>TensorType</b> as the input. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea2f4db1e8c0aa8d852fd82eda12610a04"></a>OH_NN_OPS_STRIDED_SLICE&#160;</td><td class="fielddoc"><p>Slices a tensor with the specified stride.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor.</li>
<li><b>begin</b>: start of slicing, which is a 1D tensor. The length of <b>begin</b> is <em>n</em>. <b>begin[i]</b> specifies the start of slicing in the <em>i</em>th dimension.</li>
<li><b>end</b>: end of slicing, which is a 1D tensor. The length of <b>end</b> is <em>n</em>. <b>end[i]</b> specifies the end of slicing in the <em>i</em>th dimension.</li>
<li><b>strides</b>: slicing stride, which is a 1D tensor. The length of <b>strides</b> is <em>n</em>. strides[i] specifies the stride at which the tensor is sliced in the <em>i</em>th dimension.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>beginMask</b>: an integer used to mask <b>begin</b>. <b>beginMask</b> is represented in binary code. In case of binary(beginMask)[i]==1, for the <em>i</em>th dimension, elements are sliced from the first element at <b>strides[i]</b> until the end[i]-1 element.</li>
<li><b>endMask</b>: an integer used to mask <b>end</b>. <b>endMask</b> is represented in binary code. In case of binary(endMask)[i]==1, elements are sliced from the element at the <b>begin[i]</b> position in the <em>i</em>th dimension until the tensor boundary at <b>strides[i]</b>.</li>
<li><b>ellipsisMask</b>: integer used to mask <b>begin</b> and <b>end</b>. <b>ellipsisMask</b> is represented in binary code. In case of binary(ellipsisMask)[i]==1, elements are sliced from the first element at <b>strides[i]</b> in the <em>i</em>th dimension until the tensor boundary. Only one bit of <b>binary(ellipsisMask)</b> can be a non-zero value.</li>
<li><b>newAxisMask</b>: new dimension, which is an integer. <b>newAxisMask</b> is represented in binary code. In case of binary(newAxisMask)[i]==1, a new dimension whose length is 1 is inserted into the <em>i</em>th dimension.</li>
<li><b>shrinkAxisMask</b>: shrinking dimension, which is an integer. * <b>shrinkAxisMask</b> is represented in binary code. In the case of binary(shrinkAxisMask)[i]==1, all elements in the <em>i</em>th dimension will be discarded, and the length of the <em>i</em>th dimension is shrunk to <b>1</b>.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor, with the same data type as <b>input</b>. The number of dimensions of the output tensor is rank(input[0])+1. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea850ad3b59f1240b210417a830156ea41"></a>OH_NN_OPS_SUB&#160;</td><td class="fielddoc"><p>Calculates the difference between two tensors.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: minuend, which is a tensor.</li>
<li><b>input2</b>: subtrahend, which is a tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>activationType</b> is an integer constant which is contained in <b>OH_NN_FuseType</b>. The specified activation function is called before output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: difference between the two tensors. The output shape is determined by<b>input1</b> and <b>input2</b>. If they have the same shape, the output tensor has the same shape as them. If they have different shapes, perform the broadcast operation on <b>input1</b> and <b>input2</b> and perform subtraction. <b>TensorType</b> of the output is the same as that of the input tensor with higher precision. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deab902d02d054a12f1c1ca5a7c0fc20f71"></a>OH_NN_OPS_TANH&#160;</td><td class="fielddoc"><p>Computes hyperbolic tangent of the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: hyperbolic tangent of the input. The <b>TensorType</b> and tensor shape are the same as those of the input. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea0a9bb564a7014abe9e14f9da4fc3c54e"></a>OH_NN_OPS_TILE&#160;</td><td class="fielddoc"><p>Copies a tensor the specified times.</p>
<p>Inputs:</p><ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
<li><b>multiples</b>: number of times that the input tensor is copied in each dimension. The value is a 1D tensor. The length <em>m</em> is not less than the number of dimensions, that is, <em>n</em>.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>dims</b> A 1D tensor that specifies the number of times that data is copied in each dimension. The length <b>m</b> is not less than the number of dimensions of <b>x</b>.</li>
</ul>
<p>Outputs:</p><ul>
<li>An <em>m</em>-dimensional tensor whose <b>TensorType</b> is the same as that of the input. If <b>input</b> and <b>multiples</b> have the same length, <b>input</b> and <b>output</b> have the same number of dimensions. If the length of <b>multiples</b> is greater than <em>n</em>, 1 is used to fill the input dimension, and then the input is copied in each dimension the specified times to obtain the <em>m</em>-dimensional tensor. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea31e8d44adb98d4d7e9b44422ad5bbaf0"></a>OH_NN_OPS_TRANSPOSE&#160;</td><td class="fielddoc"><p>Transposes data of <b>input</b> based on <b>permutation</b>.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor to be transposed.</li>
<li><b>permutation</b>: The value is a 1D tensor whose length is the same as the number of dimensions of <b>input</b>.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor. <b>TensorType</b> of <b>output</b> is the same as that of <b>input</b>, and the output shape is determined by the shape and <b>permutation</b> of <b>input</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deab7d7dd01522a3f36aa994a3cd62f38ed"></a>OH_NN_OPS_REDUCE_MEAN&#160;</td><td class="fielddoc"><p>Calculates the average value in the specified dimension. If <b>keepDims</b> is set to <b>false</b>, the number of dimensions is reduced for the input; if <b>keepDims</b> is set to <b>true</b>, the number of dimensions is retained.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor, where <em>n</em> is less than 8.</li>
<li><b>axis</b>: dimension used to calculate the average value. The value is a 1D tensor. The value range of each element in <b>axis</b> is [–n, n).</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>keepDims</b>: indicates whether to retain the dimension. The value is a Boolean value.</li>
<li><b>reduceToEnd</b>: boolean value, indicates whether the reduce operation needs to be performed until the last axis.</li>
<li><b>coeff</b>: A OH_NN_FLOAT32 scalar that represents the scale factor of the output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>m</em>-dimensional output tensor whose data type is the same as that of the input. If <b>keepDims</b> is <b>false</b>, m&lt;n. If <b>keepDims</b> is <b>true</b>, m==n. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deac3cd18027ac5576a5e5a0b93f6c2c59b"></a>OH_NN_OPS_RESIZE_BILINEAR&#160;</td><td class="fielddoc"><p>The Bilinear method is used to deform the input based on the given parameters.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: 4D input tensor. Each element in the input cannot be less than 0. The input layout must be [batchSize, height, width, channels].</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>newHeight</b>: resized height of the 4D tensor.</li>
<li><b>newWidth</b>: resized width of the 4D tensor.</li>
<li><b>preserveAspectRatio</b>: indicates whether to maintain the height/width ratio of <b>input</b> after resizing.</li>
<li><b>coordinateTransformMode</b>: coordinate transformation method used by the resize operation. The value is an int32 integer. Currently, the following methods are supported: 0 means ASYMMETRIC, 1 means ALIGN_CORNERS, 2 means HALF_PIXEL.</li>
<li><b>excludeOutside</b>: an int64 floating point number. When its value is <b>1</b>, the sampling weight of the part that exceeds the boundary of <b>input</b> is set to <b>0</b>, and other weights are normalized.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor, with the same shape and data type as <b>input</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea4522e96ba3dc57132a9935c1cc2ba8a8"></a>OH_NN_OPS_RSQRT&#160;</td><td class="fielddoc"><p>Calculates the reciprocal of the square root of a tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor, where <em>n</em> is less than 8. Each element of the tensor cannot be less than 0.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor, with the same shape and data type as <b>input</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deaf6c4b78e0f04152120bc5d679e5c7d75"></a>OH_NN_OPS_RESHAPE&#160;</td><td class="fielddoc"><p>Reshapes a tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor.</li>
<li><b>InputShape</b>: shape of the output tensor. The value is a 1D constant tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: tensor whose data type is the same as that of <b>input</b> and shape is determined by <b>InputShape</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea2113f865b48d80611fd7b508ff6f3952"></a>OH_NN_OPS_PRELU&#160;</td><td class="fielddoc"><p>Calculates the PReLU activation value of <b>input</b> and <b>weight</b>.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor. If <em>n</em> is greater than or equal to 2, <b>input</b> must be [BatchSize, ..., Channels]. The second dimension is the number of channels.</li>
<li><b>weight</b>: 1D tensor. The length of <b>weight</b> must be 1 or equal to the number of channels. If the length of <b>weight</b> is 1, all channels share the same weight. If the length of <b>weight</b> is equal to the number of channels, each channel exclusively has a weight. If <em>n</em> is less than 2 for <b>input</b>, the <b>weight</b> length must be 1.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: PReLU activation value of <b>input</b>, with the same shape and data type as <b>input</b>. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea26ea242c2f0811052b527eb49c379833"></a>OH_NN_OPS_RELU&#160;</td><td class="fielddoc"><p>Calculates the Relu activation value of <b>input</b>.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor, with the same data type and shape as the input tensor. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea2b8960a62f1473807f1e8916eb9999bb"></a>OH_NN_OPS_RELU6&#160;</td><td class="fielddoc"><p>Calculates the Relu6 activation value of the input, that is, calculate min(max(x, 0), 6) for each element x in the input.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional Relu6 tensor, with the same data type and shape as the input tensor. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea4a58aaac186feded06a7317ad5e347d0"></a>OH_NN_OPS_LAYER_NORM&#160;</td><td class="fielddoc"><p>Applies layer normalization for a tensor from the specified axis.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor.</li>
<li><b>gamma</b>: <em>m</em>-dimensional tensor. The dimensions of <b>gamma</b> must be the same as the shape of the part of the input tensor to normalize.</li>
<li><b>beta</b>: <em>m</em>-dimensional tensor with the same shape as <b>gamma</b>.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>beginAxis</b>: an OH_NN_INT32 scalar that specifies the axis from which normalization starts. The value range is [1, rank(input)).</li>
<li><b>epsilon</b>: a scalar of OH_NN_FLOAT32. It is a tiny amount in the normalization formula. The common value is 0.00001f.</li>
<li><b>beginParamsAxis</b>: an OH_NN_INT32 scalar that specifies the start axis of layer normalization of input parameter (gamma, beta).</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor, with the same data type and shape as the input tensor. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea8ca8125647631398fff7da0e458cf38e"></a>OH_NN_OPS_REDUCE_PROD&#160;</td><td class="fielddoc"><p>Calculates the accumulated value for a tensor along the specified dimension. If <b>keepDims</b> is set to <b>false</b>, the number of dimensions is reduced for the input; if <b>keepDims</b> is set to <b>true</b>, the number of dimensions is retained.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor, where <em>n</em> is less than 8.</li>
<li><b>axis</b>: dimension used to calculate the product. The value is a 1D tensor. The value range of each element in <b>axis</b> is [–n, n).</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>keepDims</b>: indicates whether to retain the dimension. The value is a Boolean value.</li>
<li><b>reduceToEnd</b>: boolean value, indicates whether the reduce operation needs to be performed until the last axis.</li>
<li><b>coeff</b>: A OH_NN_FLOAT32 scalar that represents the scale factor of the output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>m</em>-dimensional output tensor whose data type is the same as that of the input. If <b>keepDims</b> is <b>false</b>, m&lt;n. If <b>keepDims</b> is <b>true</b>, m==n. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea2d08952c0eaedede78cdb71f309f5d6c"></a>OH_NN_OPS_REDUCE_ALL&#160;</td><td class="fielddoc"><p>Calculates the logical and value for input tensor along the specified dimension. If <b>keepDims</b> is set to <b>false</b>, the number of dimensions is reduced for the input; if <b>keepDims</b> is set to <b>true</b>, the number of dimensions is retained.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor, where <em>n</em> is less than 8.</li>
<li><b>axis</b>: dimension used to calculate the logical and value. The value is a 1D tensor. The value range of each element in <b>axis</b> is [–n, n).</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>keepDims</b>: indicates whether to retain the dimension. The value is a Boolean value.</li>
<li><b>reduceToEnd</b>: boolean value, indicates whether the reduce operation needs to be performed until the last axis.</li>
<li><b>coeff</b>: A OH_NN_FLOAT32 scalar that represents the scale factor of the output.</li>
</ul>
<p>Outputs:</p><ul>
<li><b>output</b>: <em>m</em>-dimensional output tensor whose data type is the same as that of the input. If <b>keepDims</b> is <b>false</b>, m&lt;n. If <b>keepDims</b> is <b>true</b>, m==n. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea676b1a36bc327d99c8abe59ab03ce466"></a>OH_NN_OPS_QUANT_DTYPE_CAST&#160;</td><td class="fielddoc"><p>Converts the data type.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor. If it is a conversion between a quantized type and a floating-point type, the input tensor should contain quantized parameters.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>srcT</b>: data type of the input.</li>
<li><b>dstT</b>: data type of the output.</li>
<li><b>axis</b>: appoint the dimensions from which the quantization parameters are extracted. If the size of the input tensor quantization parameter is 1, the operator function is layer quantization conversion, and this parameter does not take effect. If the size of the input tensor quantization parameter is greater than 1, the operator function is the quantization conversion along the specific channels, and this parameter takes effect.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor. The data type is determined by <b>dstT</b>. The output shape is the same as the input shape. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea1f9d7b90110c78bc566c05ff53b26f27"></a>OH_NN_OPS_TOP_K&#160;</td><td class="fielddoc"><p>Obtains the values and indices of the largest <em>k</em> entries in the last dimension.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
<li><b>k</b>: first <em>k</em> records of data and their indices.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>sorted</b>: order of sorting. The value <b>true</b> means descending and <b>false</b> means ascending.</li>
<li><b>axis</b>: A OH_NN_INT32 scalar that specifies the dimension that needs to be sorted, default -1, pointing to the last dimension.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output0</b>: largest <em>k</em> elements in each slice of the last dimension.</li>
<li><b>output1</b>: index of the value in the last dimension of the input. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deae756ab9ec2d04092118396aaae9b186c"></a>OH_NN_OPS_ARG_MAX&#160;</td><td class="fielddoc"><p>Returns the index of the maximum tensor value across axes.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor (N, ∗), where ∗ means any number of additional dimensions.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: dimension for calculating the index of the maximum.</li>
<li><b>keepDims</b>: indicates whether to maintain the input tensor dimension. The value is a Boolean value.</li>
<li><b>topK</b>: Whether to keep the output dimensions the same as the input dimensions.</li>
<li><b>outMaxValue</b>: Return the index if the value is <b>false</b>. Return the value if the value is <b>true</b>. The default value is <b>false</b>.</li>
</ul>
<p>Outputs:</p><ul>
<li><b>output</b>: index of the maximum input tensor on the axis. The value is a tensor. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea3f608d999d582d4edc7162d26a300918"></a>OH_NN_OPS_UNSQUEEZE&#160;</td><td class="fielddoc"><p>Adds a dimension based on the value of <b>axis</b>.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: dimension to be added. The value of <b>axis</b> can be an integer or an array of integers. The value range of the integer is [-n, n).</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: output tensor. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea2adde59b09a21d7337b1e6deec71ade1"></a>OH_NN_OPS_GELU&#160;</td><td class="fielddoc"><p>Gaussian error linear unit activation function. The int quantization input is not supported. output=0.5∗input∗(1+tanh(input/2))</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: An <em>n</em>-dimensional input tensor.</li>
</ul>
<p>Parameters:</p><ul>
<li><b>approximate</b>: Whether to use the approximation algorithm.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor, with the same data type and shape as the input tensor. </li>
</ul>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea81e40d8318f5101a69dabbb4cb91f444"></a>OH_NN_OPS_UNSTACK&#160;</td><td class="fielddoc"><p>Unpacks the input tensors base on the given dimension of axis. Unpacks tensors from <b>input</b> by chipping it along the <b>axis</b> dimension. For example, given a tensor of shape (A, B, C, D); If axis == 0, then the i'th tensor in output is the slice value[i, :, :, :],<br />
and each tensor in output will have shape (B, C, D). If axis == 1, then the i'th tensor in output is the slice value[:, i, :, :],<br />
and each tensor in output will have shape (A, C, D). Etc. This is the opposite of <b>OH_NN_OPS_STACK</b>.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: dimension along witch to unpack. Default 0. The range is [-n, n).</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tuple of tensors, the shape of each objects is same.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deab224429da785feb1521bf7483eacbe02"></a>OH_NN_OPS_ABS&#160;</td><td class="fielddoc"><p>Obtains the absolute value of the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor. The absolute value of the input tensor. The shape and data type is the same as inputs'.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea3dce1426a2e7bc733543ab0e44b1a24d"></a>OH_NN_OPS_ERF&#160;</td><td class="fielddoc"><p>Computes the Gauss error function of input element-wise.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor. The dimension should be less than 8, and the data type only support OH_NN_FLOAT32 and OH_NN_FLOAT16.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor. The shape and data type is the same as inputs'.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deaf0ce7a833833f92563b5988e3736afa8"></a>OH_NN_OPS_EXP&#160;</td><td class="fielddoc"><p>Calculates the exponential of the given input tensor element-wise. ExpFusion computes outputs by formula <b>output = base ^ (shift + scale * input)</b>, for base &gt; 0. And the base is default set to -1, which means nature logarithm 'e', and the calculate formula changes to <b>output = exp(shift + scale * input)</b>.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>base</b>: The base of exponential function. Default set to -1 representing nature logarithm 'e'. Input value must be &gt; 0.</li>
<li><b>scale</b>: The amplifcation factor of exponential value, default 1.</li>
<li><b>shift</b>: The offset of exponential value, default 0.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor. The element-wise exponential result of the input tensor.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deaf783a922f1163671b5a9a5adc2edb269"></a>OH_NN_OPS_LESS&#160;</td><td class="fielddoc"><p>For <b>input1</b> and <b>input2</b>, calculate the result of input1[i] &lt; input2[i] for each pair of elements, where i is the index of each element in the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: can be a real number, Boolean value, or tensor whose data type is real number or OH_NN_BOOL.</li>
<li><b>input2</b>: can be a real number or a Boolean value if <b>input1</b> is a tensor and must be a tensor with the data type of real number or OH_NN_BOOL if <b>input1</b> is not a tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor of the data type OH_NN_BOOL. When a quantization model is used, the quantization parameters of the output cannot be omitted. However, values of the quantization parameters do not affect the result.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea316d7588ad17eb88223e6002e093e855"></a>OH_NN_OPS_SELECT&#160;</td><td class="fielddoc"><p>Selects output elements from input1 or input2, depending on condition. If condition is true, choose elements from input1. Otherwise, choose elements from input2 if condition is false. The three inputs, <b>condition</b> , <b>input1</b> and <b>input2</b> must share the same shape.</p>
<p>Inputs:</p>
<ul>
<li><b>condition</b>: <em>n</em>-dimensional tensor or scalar. The condition tensor, decides which element is chosen.</li>
<li><b>input1</b>: <em>n</em>-dimensional tensor. First input tensor to be chosen. If condition is rank 1, input1 may have higher rank, but its first dimension must match the size of condition.</li>
<li><b>input2</b>: <em>n</em>-dimensional tensor. Second input tensor to be chosen.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor, has the same shape and data type as the input.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea67c0cf3ae57ecb45342fc481e9810dfb"></a>OH_NN_OPS_SQUARE&#160;</td><td class="fielddoc"><p>Calculates the square of input tensor element-wise.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor, has the same shape and dtype as the input.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deab1d06b4c6161e8559ca2b4ce7937fb2a"></a>OH_NN_OPS_FLATTEN&#160;</td><td class="fielddoc"><p>Flattens the input tensor into a 2D matrix. If input tensor has shape (d_0, d_1, … d_n), then the output will have shape (d_0 X d_1 … d_(axis-1), d_axis X d_(axis+1) … X dn).</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor. The rank of input should be greater or equal to axis.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output. The value for axis must be in the range [-r, r], where r is the rank of the input tensor. Negative value means counting dimensions from the back. When axis = 0, the shape of the output tensor is (1, (d_0 X d_1 … d_n)), where the shape of the input tensor is (d_0, d_1, … d_n).</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: 2-dimensional tensor after flattened.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea894ed2c4174bfa52275f05c224921b19"></a>OH_NN_OPS_DEPTH_TO_SPACE&#160;</td><td class="fielddoc"><p>DepthToSpace rearranges (permutes) data from depth into blocks of spatial data. This is the reverse transformation of SpaceToDepth. More specifically, this op outputsa copy of the input tensor where values from the depth dimension are moved in spatial blocks to the height and width dimensions. By default, mode = DCR. In the DCR mode, elements along the depth dimension from the input tensor are rearranged in the following order: depth, column, and then row.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: 4-dimensional tensor with specific format of NHWC or NCHW. where N is the batch axis, H is the height, W is the width and C is the channel or depth.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>blockSize</b>: Blocks of [blocksize, blocksize] are moved.</li>
<li><b>mode</b>: DCR (default) for depth-column-row order re-arrangement. Use CRD for column-row-depth order.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Output tensor of [N, H * blocksize, W * blocksize, C/(blocksize * blocksize)] for NHWC format or [N, C/(blocksize * blocksize), H * blocksize, W * blocksize] for NCHW format.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea5ff67209ba763c4e04762efd93b50ffc"></a>OH_NN_OPS_RANGE&#160;</td><td class="fielddoc"><p>Generate a tensor containing a sequence of numbers that begin at <b>start</b><br />
and extends by increments of <b>delta</b> up to <b>limit<b>.</b></b></p>
<p><b><b>Inputs:</b></b></p>
<p><b><b></p><ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p></b></b></p>
<p><b><b>Parameters:</b></b></p>
<p><b><b></p><ul>
<li><b>start</b>: Scalar. First entry for the range of output values.</li>
<li><b>limit</b>: Scalar. Exclusive upper limit for the range of output values.</li>
<li><b>delta</b>: Scalar. Value to step by.</li>
</ul>
<p></b></b></p>
<p><b><b>Outputs:</b></b></p>
<p><b><b></p><ul>
<li><b>output</b>: <em>1</em>-dimensional tensor with specific data type containing generated range of values.</li>
</ul>
<p></b></b></p>
<p><b><b></p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
<p></b></b></p>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea879b813061be78ee3d5567864896d435"></a>OH_NN_OPS_INSTANCE_NORM&#160;</td><td class="fielddoc"><p>Normalize each channel of the input. Make the mean of each channel of the input is 0 and the variance is 1.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: Input data tensor from the previous operator; dimensions for image case are (N x C x H x W), where N is the batch size, C is the number of channels, and H and W are the height and the width of the data. For non image case, the dimensions are in the form of (N x C x D1 x D2 … Dn), where N is the batch size.</li>
<li><b>scale</b>: The input 1-dimensional scale tensor of channel size.</li>
<li><b>bias</b>: The input 1-dimensional bias tensor of channel size.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>epsilon</b>: The epsilon value to use to avoid division by zero.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: The output tensor of the same shape as input.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea91aa949d6e5e302924b8764f72a36395"></a>OH_NN_OPS_CONSTANT_OF_SHAPE&#160;</td><td class="fielddoc"><p>Generate a tensor with given value and shape.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>1</em>-dimensional tensor. Indicates the shape of the expected output tensor. All values must be &gt;= 0.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>dataType</b>: The data type of the output tensor.</li>
<li><b>value</b>: The value of the output elements.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor, has the same shape as the input.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea5c5b2ae2710dde83371bf1ca5aa3d957"></a>OH_NN_OPS_BROADCAST_TO&#160;</td><td class="fielddoc"><p>Broadcast a tensor for a compatiable shape.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>shape</b>: A 1-dimensional Tensor, the shape of the desired output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor after broadcasted.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea2ea90a1553227d76a0f52d2a6e979609"></a>OH_NN_OPS_EQUAL&#160;</td><td class="fielddoc"><p>For <b>input1</b> and <b>input2</b>, calculate the result of input1[i] = input2[i] for each pair of elements, where i is the index of each element in the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>, can be a real number, Boolean value, or tensor whose data type is real number or OH_NN_BOOL.</li>
<li><b>input2</b>, can be a real number or a Boolean value if <b>input1</b> is a tensor and must be a tensor with the data type of real number or OH_NN_BOOL if <b>input1</b> is not a tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor of the data type OH_NN_BOOL. When a quantization model is used, the quantization output cannot be omitted. However, values of the quantization parameters do not affect the result.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea2406374d9705ac1425e9edbcc062bf24"></a>OH_NN_OPS_GREATER&#160;</td><td class="fielddoc"><p>For <b>input1</b> and <b>input2</b>, calculate the result of input1[i] &gt; input2[i] for each pair of elements, where i is the index of each element in the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>, can be a real number, Boolean value, or tensor whose data type is real number or OH_NN_BOOL.</li>
<li><b>input2</b>, can be a real number or a Boolean value if <b>input1</b> is a tensor and must be a tensor with the data type of real number or OH_NN_BOOL if <b>input1</b> is not a tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor of the data type OH_NN_BOOL. When a quantization model is used, the quantization parameters of the output cannot be omitted. However, values of the quantization parameters do not affect the result.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deaece76284d811ca9f303d0b6469af3402"></a>OH_NN_OPS_NOT_EQUAL&#160;</td><td class="fielddoc"><p>For <b>input1</b> and <b>input2</b>, calculate the result of input1[i] != input2[i] for each pair of elements, where i is the index of each element in the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>, can be a real number, Boolean value, or tensor whose data type is real number or OH_NN_BOOL.</li>
<li><b>input2</b>, can be a real number or a Boolean value if <b>input1</b> is a tensor and must be a tensor with the data type of real number or OH_NN_BOOL if <b>input1</b> is not a tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor of the data type OH_NN_BOOL. When a quantization model is used, the quantization parameters of the output cannot be omitted. However, values of the quantization parameters do not affect the result.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea74135623a09dda73bb256d50eec3e5e7"></a>OH_NN_OPS_GREATER_EQUAL&#160;</td><td class="fielddoc"><p>For <b>input1</b> and <b>input2</b>, calculate the result of input1[i] &gt;= input2[i] for each pair of elements, where i is the index of each element in the input tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>, can be a real number, Boolean value, or tensor whose data type is real number or OH_NN_BOOL.</li>
<li><b>input2</b>, can be a real number or a Boolean value if <b>input1</b> is a tensor and must be a tensor with the data type of real number or OH_NN_BOOL if <b>input1</b> is not a tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor of the data type OH_NN_BOOL. When a quantization model is used, the quantization parameters of the output cannot be omitted. However, values of the quantization parameters do not affect the result.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea4e8ccb226300ebd2674dfa13beb0f96a"></a>OH_NN_OPS_LEAKY_RELU&#160;</td><td class="fielddoc"><p>LeakyRelu takes input data (Tensor) and an argument alpha, and produces one output data (Tensor) where the function f(x) = alpha * x for x &lt; 0, f(x) = x for x &gt;= 0, is applied to the data tensor elementwise.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>negativeSlope</b>: Coefficient of leakage.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor, with the same data type and shape as the input tensor.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deae486d8a64ea1234c8271040ec2d1c063"></a>OH_NN_OPS_LSTM&#160;</td><td class="fielddoc"><p>Computes an one-layer LSTM. This operator is usually supported via some custom implementation.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor, shape is [seqLen, batchSize, inputSize].</li>
<li><b>wIh</b>: Weight tensor of input-layer to hidden-layer, shape is [numDirections* numLayers, 4 * hiddenSize, inputSize].</li>
<li><b>wHh</b>: Weight tensor of hidden-layer to hidden-layer, shape is [numDirections* numLayers, 4 * hiddenSize, hiddenSize].</li>
<li><b>bias</b>: Bias tensor of input-layer and hidden-layer to hidden-layer, shape is [numDirections* numLayers, 8 * hiddenSize].</li>
<li><b>hx</b>: Init state of hidden-layer, shape is [numDirections * numLayers, batchSize, hiddenSize].</li>
<li><b>cx</b>: Init state of cell, shape is [numDirections * numLayers, batchSize, hiddenSize].</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>bidirectional</b>: Whether the LSTM operation is bidirectional.</li>
<li><b>hasBias</b>: Whether the operation contains bias.</li>
<li><b>inputSize</b>: Size of input tensor.</li>
<li><b>hiddenSize</b>: Size of hidden state tensor.</li>
<li><b>numLayers</b>: Layers of LSTM network.</li>
<li><b>numDirections</b>: Number of directions, value is 2 if direction == bidirectional else 1.</li>
<li><b>dropout</b>: Dropout probalility of each layer except first-layer.</li>
<li><b>zoneoutCell</b>: Probalility that the cell state retains the previous state. Default: 0.</li>
<li><b>zoneoutHidden</b>: Probalility that the hidden state retains the previous state. Default: 0.</li>
<li><b>projSize</b>: If projSize &gt; 0, will use LSTM with projections of corresponding size. Default: 0.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor that concats all the intermediate output tensor of the hidden, shape is [seqLen, batchSize, numDirections * realHiddenSize].</li>
<li><b>hy</b>: The last output tensor of the hidden-layer, shape is [numDirections * numLayers, batchSize, realHiddenSize].</li>
<li><b>cy</b>: The last output tensor of the cell, shape is [numDirections * numLayers, batchSize, hiddenSize].</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea1e38ae38e1e1149025773ed2366c2844"></a>OH_NN_OPS_CLIP&#160;</td><td class="fielddoc"><p>Returns a tensor of the same type and shape as input tensor with its value clipped to min and max. Any values less than <b>min</b> are set to <b>min</b>. Any values greater than <b>max</b> are set to <b>max</b>.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>max</b>: Maximum value, above which element is replaced by max. It must be a scalar(tensor of empty shape).</li>
<li><b>min</b>: Minimum value, under which element is replaced by min. It must be a scalar(tensor of empty shape).</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor., with the same data type and shape as the input tensor.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deab4392cc281241fc9beb857a32bf0f2f8"></a>OH_NN_OPS_ALL&#160;</td><td class="fielddoc"><p>Determine whether all emements in a given tensor are non-zero. It returns a boolean tensor where each element is 'True' if corresponding element in the input tensor is non-zero, and 'False' otherwise.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor of shape <b>(N,*)</b>, where * indicates any number of additional dimensions.</li>
<li><b>aixs</b>: scalar or tensor, indices the dimension to be computed.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>keepDims</b>: Whether to keep dimension info.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: 1-dimension or n-dimension tensor with boolean data type.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea5b861e7c91cd2295284de9aa7226681a"></a>OH_NN_OPS_ASSERT&#160;</td><td class="fielddoc"><p>Asserts that the given condition si true. If <b>condition</b> evalutes to false, print the list of tensors in data. Summerize determines how many entries of the tensors to print.</p>
<p>Inputs:</p>
<ul>
<li><b>condition</b>: The condition to evalute.</li>
<li><b>data</b>: The tensors to print out when condition is false.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>summarize</b>: The number of entries for each tensor is printed.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Result value judged by condition. If the condition is not true, an Error is returned.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deab75d745f70ec4718c2d641d4118b8f31"></a>OH_NN_OPS_COS&#160;</td><td class="fielddoc"><p>Calculates the cosine of the given input tensor, element-wise.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor. The cosine of the input tensor computed element-wise.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea28df3f7950598076a0003e7bbc6d1c81"></a>OH_NN_OPS_LOG&#160;</td><td class="fielddoc"><p>Calculates the result of nature logarithm of the input.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor. The value must be greater than 0.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor with the same shape as the input tensor.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea4ad52cba16dd8633620fcf1b5634c284"></a>OH_NN_OPS_LOGICAL_AND&#160;</td><td class="fielddoc"><p>Calculates the logical value of <b>input1</b> and <b>input2</b> element-wise.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: Tensor of type boolean or convert to boolean implicitly.</li>
<li><b>input2</b>: Tensor of type boolean or convert to boolean implicitly.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor. The calculation result of logical-and and the numeric type is OH_NN_BOOL.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea3e533201e7633edcdaa84b83193aad8f"></a>OH_NN_OPS_LOGICAL_NOT&#160;</td><td class="fielddoc"><p>Calculates the logical value of NOT <b>input</b> element-wise.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: Tensor of type boolean or convert to boolean implicitly.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor. The calculation result of logical-not and the numeric type is OH_NN_BOOL.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea10a15aaff29f9fffdf59078b6429ae6f"></a>OH_NN_OPS_MOD&#160;</td><td class="fielddoc"><p>Computes the remainder of dividing the first input tensor by the second input tensor element-wise. Inputs of input1 and input2 comply with the implicit type conversion rules to make the data types consistent. The inputs must be two tensors or one tensor and one scalar. When the inputs are two tensors, both dtypes cannot be bool, and the shapes of them could be broadcast. When the inputs are one tensor and one scalar, the scalar could only be a constant.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: The remainder of the scalar or tensor, numeric or OH_NN_BOOL type, or the <em>n</em>-dimensional tensor of the numeric dimension numeric type.</li>
<li><b>input2</b>: Remainder factor. When the first input is an n-dimensional tensor, the second input can be a numeric tensor, a OH_NN_BOOL type, or an n-dimensional tensor of a numeric type dimension, and when the first input is a numeric or OH_NN_BOOL tensor, the second input must be a tensor of the numeric dimension of the data type.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor. The shape is the same as the input after broadcasting, and the data type is the data type with the highest accuracy of the two inputs.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deabc071aba6a212709ffbeae96c317f625"></a>OH_NN_OPS_NEG&#160;</td><td class="fielddoc"><p>Calculate the opposite value of the input tensor element-wise.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor with numeric data type。</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor with the same shape and data type as the input tensor.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea3053398359438ad92ba31178f4b2d5ea"></a>OH_NN_OPS_RECIPROCAL&#160;</td><td class="fielddoc"><p>Calculate reciprocal of a tensor element-wise.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor with the same shape and data type as the input tensor.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea053f68776536beaff6c54c4bdb286c3b"></a>OH_NN_OPS_SIN&#160;</td><td class="fielddoc"><p>Calculate sine of the input element-wise.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor the same data type and shape as the input tensor.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea53404933d36e88c97bb8e4d7f91ba212"></a>OH_NN_OPS_WHERE&#160;</td><td class="fielddoc"><p>Selects elements from input1 or input2 based on condition and returns a tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>condition</b>: <em>n</em>-dimensional tensor or scalar. Judging conditions. If the OH_NN_BOOL element is True, then the element corresponding to the position of input1 is selected, and if the OH_NN_BOOL element is False, the element corresponding to the position of input2 is selected.</li>
<li><b>input1</b>: <em>n</em>-dimensional tensor. First tensor to be chosen.</li>
<li><b>input2</b>: <em>n</em>-dimensional tensor. Second tensor to be chosen.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>-dimensional tensor with the same shape and data type as the input1 and input2.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea75f22874793012fca98a91b4ec61d85c"></a>OH_NN_OPS_SPARSE_TO_DENSE&#160;</td><td class="fielddoc"><p>Converts a sparse tensor into a dense tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>indices</b>: 2-dimensional tensor. Position of an ellement in a sparse tensor. Each element value must be non-negative. The shape is (N, 2).</li>
<li><b>values</b>: 1-dimensional tensor. The value corresponding to the location of indices. The shape is (N).</li>
<li><b>sparseShape</b>: 2-dimensional tensor. The shape of a sparse tensor. The value consists of two positive integers, indicating that the shape of the sparse tensor is (N, C).</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor. The data type is the same as values, and the shape is specified by sparseShape.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deaf48926f0930f019c4c9b671b609e704c"></a>OH_NN_OPS_LOGICAL_OR&#160;</td><td class="fielddoc"><p>Calculates the logical value of <b>input1</b> or <b>input2</b> element-wise.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: Tensor of type boolean or convert to boolean implicitly.</li>
<li><b>input2</b>: Tensor of type boolean or convert to boolean implicitly.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>n</em>&ndash;dimensional tensor. The calculation result of logical-or and the numeric type is OH_NN_BOOL.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea7bf465e9e10482e7861a1322fa237129"></a>OH_NN_OPS_CEIL&#160;</td><td class="fielddoc"><p>Returns element-wise smallest integer in not less than input.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor after ceiled.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea4d5dfe0d4c04430e11f4885f5c65b166"></a>OH_NN_OPS_CROP&#160;</td><td class="fielddoc"><p>Crop given tensor acrodding to axis and offset.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
<li><b>shape</b>: <em>1</em>-dimensional tensor, indices cropped windows dimension.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: Cropped dimension.</li>
<li><b>offset</b>: Cropped offset per dimension.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Cropped output tensor.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea133a9eac85573695c3927c991e67cee0"></a>OH_NN_OPS_DETECTION_POST_PROCESS&#160;</td><td class="fielddoc"><p>The output of the object detection model is post-processed, including decoding the bounding box, class probability and score of the model output, and then performing non-maximum suppression (NMS) to remove the overlapping bounding box, and finally outputting the detection result.</p>
<p>Inputs:</p>
<ul>
<li><b>bbox</b>: Boxes to be predicted.</li>
<li><b>scores</b>: Socres of all boxes.</li>
<li><b>anchors</b>: Information of boxes, includes box, variance and coordinates.</li>
</ul>
<p>Parameters:</p><ul>
<li><b>inputSize</b>: The size of the input tensor.</li>
<li><b>scale</b>: The scaling factor used to convert the output from the normalized form to the original image coordinates.</li>
<li><b>nmsIoUThreshold</b>: The threshold of overlapping region during NMS.</li>
<li><b>nmsScoreThreshold</b>: The socre threshold used to select target bbox duing NMS.</li>
<li><b>maxDetections</b>: Maximum of bboxes per image.</li>
<li><b>detectionsPerClass</b>: Maximum of bboxes per class.</li>
<li><b>maxClassesPerDetection</b>: Maximum of reserved classes per bboxes.</li>
<li><b>numClasses</b>: Number of target classes to be detected.</li>
<li><b>useRegularNms</b>: Whether use NMS based on IoU threshold.</li>
<li><b>outQuantized</b>: Whether need to quantize.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>bboxes</b>: The corrdinates of target detected bboxes.</li>
<li><b>classes</b>: The target class index of target detected bboxes.</li>
<li><b>confidences</b>: The score of target detected bboxes.</li>
<li><b>numDetections</b>: The number of target detected bboxes.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea889379ecfcf070c8817b87f2dd4c4fba"></a>OH_NN_OPS_FLOOR&#160;</td><td class="fielddoc"><p>Returns element-wise largest integer not greater than x.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: A tensor after floored.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deabc765ea026a218e60a400d7431e0e723"></a>OH_NN_OPS_L2_NORMALIZE&#160;</td><td class="fielddoc"><p>Calculate the L2-normalize of the input using the given axis.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: Input to compute the L2-normalization.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: The axis on which to apply normalization, -1 means last axis, default: 0.</li>
<li><b>epsilon</b>: Value added for numerical stability. default: 1e-6;</li>
<li><b>activationType</b>: Activation function type.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Result tensor with the same type and shape as input <b>input</b>.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea881e9adeb2585baff3fe58933f252846"></a>OH_NN_OPS_LOG_SOFTMAX&#160;</td><td class="fielddoc"><p>Computes the log-softmax function to n-dimensional input tensor. The input is transformed by the Softmax function and then by the log function to lie in range[-inf,0).</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>axis</b>: The axis to apply LogSoftmax operation, -1 means the last dimension.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Tensor output. Has the same data type and shape as input.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea27287d3c4fdf83b24cd0486a36c6e274"></a>OH_NN_OPS_LRN&#160;</td><td class="fielddoc"><p>Normalize over local input regions.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>depthRadius</b>: Half-width of the 1-dimension normalization window.</li>
<li><b>bias</b>: Offset.</li>
<li><b>alpha</b>: Scale factor.</li>
<li><b>beta</b>: Exponent.</li>
<li><b>normRegion</b>: Specifies normalization region. Options: "ACROSS_CHNNEL".</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Result output tensor.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deab7993f0c9d71a1956ed6ad9c5051d75e"></a>OH_NN_OPS_MINIMUM&#160;</td><td class="fielddoc"><p>Calculates the minimum of <b>input1</b> and <b>input2</b> element-wise. The inputs of <b>input1</b> and <b>input2</b> comply with the implicit type conversion rules to make the data types are consistent.</p>
<p>The input must be two tensors or one tensor and one scalar. When the input is two tensors, the data types cannot be Boolean at the same time, and their shapes can be broadcast to the same size. When the inputs are one tensor and one scalar, the scalar must be a constant.</p>
<p>Inputs:</p>
<ul>
<li><b>input1</b>: <em>n</em>-dimensional tensor, whose data type can be number or Boolean.</li>
<li><b>input2</b>: <em>n</em>-dimensional tensor, whose data type can be number or Boolean.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Minimum value of the elements of the two tensors.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea220384793dd8a373554e36ccc7377169"></a>OH_NN_OPS_RANK&#160;</td><td class="fielddoc"><p>Calculate the rank of a tensor. The rank of a tensor is the number of indices required to uniquely select each element of the tensor.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Result tensor. 0-D int32 Tensor representing the rank of input.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea90d68f031e655e2c73f1c48bde920ece"></a>OH_NN_OPS_REDUCE_MAX&#160;</td><td class="fielddoc"><p>Calculates the maximum value for input tensor along the specified dimension. If <b>keepDims</b> is set to <b>false</b>, the number of dimensions is reduced for the input; if <b>keepDims</b> is set to <b>true</b>, the number of dimensions is retained.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor, where <em>n</em> is less than 8.</li>
<li><b>axis</b>: dimension used to calculate the maximum value. The value is a 1D tensor. The value range of each element in <b>axis</b> is [–n, n).</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>keepDims</b>: indicates whether to retain the dimension. The value is a Boolean value.</li>
<li><b>reduceToEnd</b>: boolean value, indicates whether the reduce operation needs to be performed until the last axis.</li>
<li><b>coeff</b>: A OH_NN_FLOAT32 scalar that represents the scale factor of the output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>m</em>-dimensional output tensor whose data type is the same as that of the input. If <b>keepDims</b> is <b>false</b>, m&lt;n. If <b>keepDims</b> is <b>true</b>, m==n.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea6b0b520f0e6fcd339bc4d7d8569d7d37"></a>OH_NN_OPS_REDUCE_MIN&#160;</td><td class="fielddoc"><p>Calculates the minimum value for input tensor along the specified dimension. If <b>keepDims</b> is set to <b>false</b>, the number of dimensions is reduced for the input; if <b>keepDims</b> is set to <b>true</b>, the number of dimensions is retained.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor, where <em>n</em> is less than 8.</li>
<li><b>axis</b>: dimension used to calculate the minimum value. The value is a 1D tensor. The value range of each element in <b>axis</b> is [–n, n).</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>keepDims</b>: indicates whether to retain the dimension. The value is a Boolean value.</li>
<li><b>reduceToEnd</b>: boolean value, indicates whether the reduce operation needs to be performed until the last axis.</li>
<li><b>coeff</b>: A OH_NN_FLOAT32 scalar that represents the scale factor of the output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>m</em>-dimensional output tensor whose data type is the same as that of the input. If <b>keepDims</b> is <b>false</b>, m&lt;n. If <b>keepDims</b> is <b>true</b>, m==n.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deadae5d32a26226b338fabe50e8dda7ee5"></a>OH_NN_OPS_REDUCE_SUM&#160;</td><td class="fielddoc"><p>Calculates the numerical sum value for input tensor along the specified dimension. If <b>keepDims</b> is set to <b>false</b>, the number of dimensions is reduced for the input; if <b>keepDims</b> is set to <b>true</b>, the number of dimensions is retained.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional input tensor, where <em>n</em> is less than 8.</li>
<li><b>axis</b>: dimension used to calculate the sum value. The value is a 1D tensor. The value range of each element in <b>axis</b> is [–n, n).</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>keepDims</b>: indicates whether to retain the dimension. The value is a Boolean value.</li>
<li><b>reduceToEnd</b>: boolean value, indicates whether the reduce operation needs to be performed until the last axis.</li>
<li><b>coeff</b>: A OH_NN_FLOAT32 scalar that represents the scale factor of the output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: <em>m</em>-dimensional output tensor whose data type is the same as that of the input. If <b>keepDims</b> is <b>false</b>, m&lt;n. If <b>keepDims</b> is <b>true</b>, m==n.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deacac1293b1b712b627d2bd26ef433aec3"></a>OH_NN_OPS_ROUND&#160;</td><td class="fielddoc"><p>Calculate half to even of a tensor element-wise.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Result tensor with the same shape as the input.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dead2095b92aea9eb7b4b0f1a82481ea9cf"></a>OH_NN_OPS_SCATTER_ND&#160;</td><td class="fielddoc"><p>Scatters a tensor into a new tensor depending on the specified indices.</p>
<p>Inputs:</p>
<ul>
<li><b>indices</b>: The index of scattering in the new tensor with int32 or int64 data type. The rank of indices must be at least 2 and indicesShape[-1] &lt;= len(shape).</li>
<li><b>updates</b>: The source tensor to be scattered. It has shape indicesShape[:-1]+shape[indicesShape[-1]:].</li>
<li><b>shape</b>: The shape of the output tensor, has the same data type as <b>indices</b>.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Result tensor with the same type as <b>update</b> and the same shape as <b>shape</b>.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95deaf3682e2e8e1d4ab665cd45c0e622c4a3"></a>OH_NN_OPS_SPACE_TO_DEPTH&#160;</td><td class="fielddoc"><p>Rearrange blocks of spatial data into depth. The output tensor’s height dimension is height / blocksize; The output tensor’s weight dimension is weight / blocksize; The depth of output tensor is blocksize * blocksize * inputDepth; The input tensor’s height and width must be divisible by blocksize.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>4</em>-dimensional tensor.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>blocksize</b>: The block size used to divide spatial data. It must be &gt;= 2.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Result tensor with the same dataType as the input.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea3c85d8950be9632309b9807c2cc799f0"></a>OH_NN_OPS_SWISH&#160;</td><td class="fielddoc"><p>Swish activation function</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Output tensor.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea84aa2a1b11c6018c5f24097ad773f125"></a>OH_NN_OPS_REDUCE_L2&#160;</td><td class="fielddoc"><p>Calculates the L2 norm of the input tensor along the specified axis, replacing other elements of the dimension with the L2 norm value of the specified dimension to remove the dimension, or to reduce the dimension size to 1. Control whether the dimensions of the output and input are the same by specifying the keepDims parameter.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: input tensor.</li>
<li><b>axis</b>: Dimensions to perform L2-Norm calculations.</li>
</ul>
<p>Parameters:</p>
<ul>
<li><b>keepDims</b>: indicates whether to retain the dimension. The value is a Boolean value.</li>
<li><b>reduceToEnd</b>: boolean value, indicates whether the reduce operation needs to be performed until the last axis.</li>
<li><b>coeff</b>: A OH_NN_FLOAT32 scalar that represents the scale factor of the output.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Result tensor with the same dataType as the input.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea9125748be37320c2ac38aabef2160f93"></a>OH_NN_OPS_HARD_SIGMOID&#160;</td><td class="fielddoc"><p>HardSigmoid activation function. Calculate the output by element.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Result tensor with the same shape and dataType as the input.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="ggab333095762fd39a9f952523d717b95dea637c1e17ebfec5db5f709b4bb7fac2d6"></a>OH_NN_OPS_GATHER_ND&#160;</td><td class="fielddoc"><p>Gets the element at the location specified by the input tensor according to the index.</p>
<p>Inputs:</p>
<ul>
<li><b>input</b>: <em>n</em>-dimensional tensor.</li>
<li><b>indices</b>: index tensor.</li>
</ul>
<p>Outputs:</p>
<ul>
<li><b>output</b>: Result tensor with the same shape as the input.</li>
</ul>
<dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
</table>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00326">326</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga2ea6daeb95ee5bea81f97a5d9aeef5bf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2ea6daeb95ee5bea81f97a5d9aeef5bf">&#9670;&nbsp;</a></span>OH_NN_PerformanceMode</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="group___neural_nework_runtime.html#ga2ea6daeb95ee5bea81f97a5d9aeef5bf">OH_NN_PerformanceMode</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the hardware performance mode. </p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a id="gga2ea6daeb95ee5bea81f97a5d9aeef5bfa9d728aef494dfd8a589ac3478b5859b0"></a>OH_NN_PERFORMANCE_NONE&#160;</td><td class="fielddoc"><p>No performance mode preference </p>
</td></tr>
<tr><td class="fieldname"><a id="gga2ea6daeb95ee5bea81f97a5d9aeef5bfa2858695128b273d26cf032c0c32b8c00"></a>OH_NN_PERFORMANCE_LOW&#160;</td><td class="fielddoc"><p>Low power consumption mode </p>
</td></tr>
<tr><td class="fieldname"><a id="gga2ea6daeb95ee5bea81f97a5d9aeef5bfafe262b0f5ab7ff4656a540a5a9357336"></a>OH_NN_PERFORMANCE_MEDIUM&#160;</td><td class="fielddoc"><p>Medium performance mode </p>
</td></tr>
<tr><td class="fieldname"><a id="gga2ea6daeb95ee5bea81f97a5d9aeef5bfa3bb03471d6f47804d1bd6b935aceca4a"></a>OH_NN_PERFORMANCE_HIGH&#160;</td><td class="fielddoc"><p>High performance mode </p>
</td></tr>
<tr><td class="fieldname"><a id="gga2ea6daeb95ee5bea81f97a5d9aeef5bfa85000e60fc542508a6292543286ea77f"></a>OH_NN_PERFORMANCE_EXTREME&#160;</td><td class="fielddoc"><p>Ultimate performance mode </p>
</td></tr>
</table>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00108">108</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga5ae0ed09b49e98f608f47ac4fe9ecb07"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga5ae0ed09b49e98f608f47ac4fe9ecb07">&#9670;&nbsp;</a></span>OH_NN_Priority</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="group___neural_nework_runtime.html#ga5ae0ed09b49e98f608f47ac4fe9ecb07">OH_NN_Priority</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines the model inference task priority. </p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a id="gga5ae0ed09b49e98f608f47ac4fe9ecb07a2078f6a415dd21a0128899cc97a9d010"></a>OH_NN_PRIORITY_NONE&#160;</td><td class="fielddoc"><p>No priority preference </p>
</td></tr>
<tr><td class="fieldname"><a id="gga5ae0ed09b49e98f608f47ac4fe9ecb07a846f3f7a47504a6069480e4c651060a4"></a>OH_NN_PRIORITY_LOW&#160;</td><td class="fielddoc"><p>Low priority </p>
</td></tr>
<tr><td class="fieldname"><a id="gga5ae0ed09b49e98f608f47ac4fe9ecb07a664a335e3b0cb5d3267f90a8a2a001c4"></a>OH_NN_PRIORITY_MEDIUM&#160;</td><td class="fielddoc"><p>Medium priority </p>
</td></tr>
<tr><td class="fieldname"><a id="gga5ae0ed09b49e98f608f47ac4fe9ecb07a3fcbc2b29deaad3d06dd9601f97555d4"></a>OH_NN_PRIORITY_HIGH&#160;</td><td class="fielddoc"><p>High priority </p>
</td></tr>
</table>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00127">127</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga28c9d17051cc5833d0b749c0b61e1823"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga28c9d17051cc5833d0b749c0b61e1823">&#9670;&nbsp;</a></span>OH_NN_ReturnCode</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Defines error codes. </p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>2.0 </dd></dl>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61"></a>OH_NN_SUCCESS&#160;</td><td class="fielddoc"><p>The operation is successful. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823ae480781f8f23d040a2c8aad3c170783e"></a>OH_NN_FAILED&#160;</td><td class="fielddoc"><p>The operation failed. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289"></a>OH_NN_INVALID_PARAMETER&#160;</td><td class="fielddoc"><p>Invalid parameter. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823a91a3c5cc6c688cc3feaace95d199e249"></a>OH_NN_MEMORY_ERROR&#160;</td><td class="fielddoc"><p>Memory-related error, for example, insufficient memory, memory data copy failure, or memory application failure. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4"></a>OH_NN_OPERATION_FORBIDDEN&#160;</td><td class="fielddoc"><p>Invalid operation. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823ac086f7e028ece986f71cbcd0a8fa91e9"></a>OH_NN_NULL_PTR&#160;</td><td class="fielddoc"><p>Null pointer exception </p>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823ace52583ce1eccad5a5862f4771818cdf"></a>OH_NN_INVALID_FILE&#160;</td><td class="fielddoc"><p>Invalid file. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823a59a096979dfc59e124f7889012bbf233"></a>OH_NN_UNAVALIDABLE_DEVICE&#160;</td><td class="fielddoc"><p>A hardware error occurs, for example, HDL service crash. </p><dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000107">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a11ec75439c1cbab1778f0f84d21dc311">OH_NN_UNAVAILABLE_DEVICE</a> </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823aa7571ab718fe691536f700ce17aa77cf"></a>OH_NN_INVALID_PATH&#160;</td><td class="fielddoc"><p>Invalid path. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823a19543b3420787458575cc04c9c582731"></a>OH_NN_TIMEOUT&#160;</td><td class="fielddoc"><p>Timeout. </p><dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823ae6455b906d5a7aee0597798f7a00c529"></a>OH_NN_UNSUPPORTED&#160;</td><td class="fielddoc"><p>Unsupported. </p><dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823a6fad4fd55f0d1e9fc1002deb4778769e"></a>OH_NN_CONNECTION_EXCEPTION&#160;</td><td class="fielddoc"><p>Connection Exception. </p><dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823a9d8db5dcdeef28ecdec97aa10aaf6001"></a>OH_NN_SAVE_CACHE_EXCEPTION&#160;</td><td class="fielddoc"><p>Save cache exception. </p><dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823a6d639b026c18ecc858873d1cb45f41eb"></a>OH_NN_DYNAMIC_SHAPE&#160;</td><td class="fielddoc"><p>Dynamic shape. </p><dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga28c9d17051cc5833d0b749c0b61e1823a11ec75439c1cbab1778f0f84d21dc311"></a>OH_NN_UNAVAILABLE_DEVICE&#160;</td><td class="fielddoc"><p>A hardware error occurs, for example, HDL service crash. </p><dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
</td></tr>
</table>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l00144">144</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<a id="ga09f35be6e0f86f5d747192ce5812552f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga09f35be6e0f86f5d747192ce5812552f">&#9670;&nbsp;</a></span>OH_NN_TensorType</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="group___neural_nework_runtime.html#ga09f35be6e0f86f5d747192ce5812552f">OH_NN_TensorType</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Enumerates the tensor data types. </p>
<p>Tensors are usually used to set the input, output, and operator parameters of a model. When a tensor is used as the input or output of a model (or operator), set the tensor type to <a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa4e38ff084622d59af25f5848e83f39a0">OH_NN_TENSOR</a>. When the tensor is used as an operator parameter, select an enumerated value other than <a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa4e38ff084622d59af25f5848e83f39a0">OH_NN_TENSOR</a> as the tensor type. Assume that the <b>pad</b> parameter of the <a class="el" href="group___neural_nework_runtime.html#ggab333095762fd39a9f952523d717b95dea431b2053449cef3c3418869b0f801301">OH_NN_OPS_CONV2D</a> operator is being set. You need to set the <b>type</b> attribute of the <a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a> instance to <a class="el" href="group___neural_nework_runtime.html#gga09f35be6e0f86f5d747192ce5812552fa961c65f904f390cec7d56950872e1782">OH_NN_CONV2D_PAD</a>. The settings of other operator parameters are similar. The enumerated values are named in the format OH_NN_{<em>Operator name</em>}_{<em>Attribute name</em>}.</p>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>2.0 </dd></dl>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa4e38ff084622d59af25f5848e83f39a0"></a>OH_NN_TENSOR&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the input or output of a model (or operator). </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa4a42f1f0dbd0ce289619a5490bc7b52a"></a>OH_NN_ADD_ACTIVATIONTYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>activationType</b> parameter of the Add operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa6caac3c689ccee4ed4f270e3e7749044"></a>OH_NN_AVG_POOL_KERNEL_SIZE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>kernelSize</b> parameter of the AvgPool operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa5c4af15b860d9a7bdcaf7dc4106b8775"></a>OH_NN_AVG_POOL_STRIDE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>stride</b> parameter of the AvgPool operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fae0ad0acadc379b29465efe7b4fe5f459"></a>OH_NN_AVG_POOL_PAD_MODE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>padMode</b> parameter of the AvgPool operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa1539a2b9b69f53f07dce0e7f55ad9e5c"></a>OH_NN_AVG_POOL_PAD&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>pad</b> parameter of the AvgPool operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa1251a336baf8b38f59a2ba99568d110f"></a>OH_NN_AVG_POOL_ACTIVATION_TYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>activationType</b> parameter of the AvgPool operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa06c4346b3d7f29384320d049cd17757f"></a>OH_NN_BATCH_NORM_EPSILON&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>eosilon</b> parameter of the BatchNorm operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa76ba858fb9baaa042e94d1df68b63a9f"></a>OH_NN_BATCH_TO_SPACE_ND_BLOCKSIZE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>blockSize</b> parameter of the BatchToSpaceND operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faca2869d20c6a1a4ba454af6e8c75c5be"></a>OH_NN_BATCH_TO_SPACE_ND_CROPS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>crops</b> parameter of the BatchToSpaceND operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faae4d88b620ec7483949d1a2424b3a2de"></a>OH_NN_CONCAT_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the Concat operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fad383864eeeba5a543f7584608ebbfd05"></a>OH_NN_CONV2D_STRIDES&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>strides</b> parameter of the Conv2D operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa961c65f904f390cec7d56950872e1782"></a>OH_NN_CONV2D_PAD&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>pad</b> parameter of the Conv2D operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa4c9e4304c38664c9816c4d24b336bc9a"></a>OH_NN_CONV2D_DILATION&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>dilation</b> parameter of the Conv2D operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa7db5636f8757016324dd5c6bf09d5a0d"></a>OH_NN_CONV2D_PAD_MODE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>padMode</b> parameter of the Conv2D operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa077d066af1b8dc2ef4bf8c4565d33617"></a>OH_NN_CONV2D_ACTIVATION_TYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>activationType</b> parameter of the Conv2D operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fabc24a47774affcfc2bee3120a9876f53"></a>OH_NN_CONV2D_GROUP&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>group</b> parameter of the Conv2D operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa9f6c005964e33857fd7620160ed2cd9c"></a>OH_NN_CONV2D_TRANSPOSE_STRIDES&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>strides</b> parameter of the Conv2DTranspose operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa814f92059d6378ed98e5fb9e08ec5303"></a>OH_NN_CONV2D_TRANSPOSE_PAD&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>pad</b> parameter of the Conv2DTranspose operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fade462cb51f4c2233f2d7ec9c87c91f4c"></a>OH_NN_CONV2D_TRANSPOSE_DILATION&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>dilation</b> parameter of the Conv2DTranspose operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa325f1958a7297e1898214ed10bb22180"></a>OH_NN_CONV2D_TRANSPOSE_OUTPUT_PADDINGS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>outputPaddings</b> parameter of the Conv2DTranspose operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fac50b0792a10063bde3382abc3d8d6900"></a>OH_NN_CONV2D_TRANSPOSE_PAD_MODE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>padMode</b> parameter of the Conv2DTranspose operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa8bb021aba1428b6d738883e67ad579c4"></a>OH_NN_CONV2D_TRANSPOSE_ACTIVATION_TYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>activationType</b> parameter of the Conv2DTranspose operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa77275720bf3bf98d99afecce8d98acc2"></a>OH_NN_CONV2D_TRANSPOSE_GROUP&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>group</b> parameter of the Conv2DTranspose operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faf5453c22b717752b433e95c7057d6b8a"></a>OH_NN_DEPTHWISE_CONV2D_NATIVE_STRIDES&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>strides</b> parameter of the DepthwiseConv2dNative operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fac48a51dbbe536f08d7e208cb81823851"></a>OH_NN_DEPTHWISE_CONV2D_NATIVE_PAD&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>pad</b> parameter of the DepthwiseConv2dNative operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa6c6858f128705fe13be1d9a28b2c2adb"></a>OH_NN_DEPTHWISE_CONV2D_NATIVE_DILATION&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>dilation</b> parameter of the DepthwiseConv2dNative operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fad64df7c9202e74fd775ff2ad4cf13037"></a>OH_NN_DEPTHWISE_CONV2D_NATIVE_PAD_MODE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>padMode</b> parameter of the DepthwiseConv2dNative operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa6d7a5deab8d2405c06c2e8aa53d4cddb"></a>OH_NN_DEPTHWISE_CONV2D_NATIVE_ACTIVATION_TYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>activationType</b> parameter of the DepthwiseConv2dNative operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faa64dd2e1b97a9308964d86d87bc88942"></a>OH_NN_DIV_ACTIVATIONTYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>activationType</b> parameter of the Div operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa8f38940361c1a0927beb0972680e89f2"></a>OH_NN_ELTWISE_MODE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>mode</b> parameter of the Eltwise operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa684df8d50dc404c4765a34f260f2913d"></a>OH_NN_FULL_CONNECTION_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the FullConnection operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fab8e993fb6f8b86b50ae645228c78d4f8"></a>OH_NN_FULL_CONNECTION_ACTIVATIONTYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>activationType</b> parameter of the FullConnection operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faae2cefdf33074023b89bdbf4d5bac01a"></a>OH_NN_MATMUL_TRANSPOSE_A&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>transposeA</b> parameter of the Matmul operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faadb7865daa2c5c26db2ee943c1e32bcb"></a>OH_NN_MATMUL_TRANSPOSE_B&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>transposeB</b> parameter of the Matmul operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa786e09ecd4c02fe21f654c2e36252286"></a>OH_NN_MATMUL_ACTIVATION_TYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>activationType</b> parameter of the Matmul operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faf3c395182e18bebd90d3c8dabd0eeffd"></a>OH_NN_MAX_POOL_KERNEL_SIZE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>kernelSize</b> parameter of the MaxPool operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fab0db1ab02f7b287dac2a01d8ae4eeb08"></a>OH_NN_MAX_POOL_STRIDE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>stride</b> parameter of the MaxPool operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa8e59f925917331c82394acfd87c70da5"></a>OH_NN_MAX_POOL_PAD_MODE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>padMode</b> parameter of the MaxPool operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faf79aec78b0886bf9cdd7535b47164120"></a>OH_NN_MAX_POOL_PAD&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>pad</b> parameter of the MaxPool operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fafd6132e48a929bf69252ad280e14e6bc"></a>OH_NN_MAX_POOL_ACTIVATION_TYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>activationType</b> parameter of the MaxPool operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fae216424221f98aa3552b33dab193e53b"></a>OH_NN_MUL_ACTIVATION_TYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>activationType</b> parameter of the Mul operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa8544ab5b48a1f56785d04618d385a0c2"></a>OH_NN_ONE_HOT_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the OneHot operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fad031c13179b85e1b97fefe0ea7ffb0f2"></a>OH_NN_PAD_CONSTANT_VALUE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>constantValue</b> parameter of the Pad operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fac1f5a9b9277efd997a26b978203c9a47"></a>OH_NN_SCALE_ACTIVATIONTYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>activationType</b> parameter of the Scale operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa5fe5b3428962e06ef17c1ad788e4b979"></a>OH_NN_SCALE_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the Scale operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faa758d60852597b99c5ffb7ed1f62bdd3"></a>OH_NN_SOFTMAX_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the Softmax operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa6aeffbfb92523d8fcbf7e1bd0237846b"></a>OH_NN_SPACE_TO_BATCH_ND_BLOCK_SHAPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>BlockShape</b> parameter of the SpaceToBatchND operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faeef9918b19afc9a8622917067837d398"></a>OH_NN_SPACE_TO_BATCH_ND_PADDINGS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>Paddings</b> parameter of the SpaceToBatchND operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fac53187231243129bd50e4b4abe0ec2bc"></a>OH_NN_SPLIT_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>Axis</b> parameter of the Split operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa1c5262397b2f6ff7196383a4ffa3d0c3"></a>OH_NN_SPLIT_OUTPUT_NUM&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>OutputNum</b> parameter of the Split operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa083159d4fe01da04c6a5b29e9f2253b1"></a>OH_NN_SPLIT_SIZE_SPLITS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>SizeSplits</b> parameter of the Split operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa84706349aad1ab066c623a26e4ee8bd6"></a>OH_NN_SQUEEZE_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>Axis</b> parameter of the Squeeze operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa90a700327f96dfa3674a2c38d8fcb1b2"></a>OH_NN_STACK_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>Axis</b> parameter of the Stack operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa6fdd8103ab19f1991f9b95418c16d065"></a>OH_NN_STRIDED_SLICE_BEGIN_MASK&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>BeginMask</b> parameter of the StridedSlice operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fab649342189849871c39e83f51332869b"></a>OH_NN_STRIDED_SLICE_END_MASK&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>EndMask</b> parameter of the StridedSlice operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa345d817f8cbc8eb740632eb34c322f6e"></a>OH_NN_STRIDED_SLICE_ELLIPSIS_MASK&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>EllipsisMask</b> parameter of the StridedSlice operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa5a2e97fd31352ec64f456658b87a30b4"></a>OH_NN_STRIDED_SLICE_NEW_AXIS_MASK&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>NewAxisMask</b> parameter of the StridedSlice operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faf10d9e7e0219d380273b70ea27fda2d2"></a>OH_NN_STRIDED_SLICE_SHRINK_AXIS_MASK&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>ShrinkAxisMask</b> parameter of the StridedSlice operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa9c07bc3353943f62c98b2e98040797ad"></a>OH_NN_SUB_ACTIVATIONTYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>ActivationType</b> parameter of the Sub operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa01b8d0365b076ef8886b760c04c70389"></a>OH_NN_REDUCE_MEAN_KEEP_DIMS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>keepDims</b> parameter of the ReduceMean operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa6e3bb04c940b0554f23c5e2645d900b7"></a>OH_NN_RESIZE_BILINEAR_NEW_HEIGHT&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>newHeight</b> parameter of the ResizeBilinear operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa2160838654c8ca289b3b7c07b51dc386"></a>OH_NN_RESIZE_BILINEAR_NEW_WIDTH&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>newWidth</b> parameter of the ResizeBilinear operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fad3ed74571e0336b4ced6c858676a5fc7"></a>OH_NN_RESIZE_BILINEAR_PRESERVE_ASPECT_RATIO&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>preserveAspectRatio</b> parameter of the ResizeBilinear operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faebe8182e06053a7e7e2ed70c41e34a94"></a>OH_NN_RESIZE_BILINEAR_COORDINATE_TRANSFORM_MODE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>coordinateTransformMode</b> parameter of the ResizeBilinear operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa677799dfd1d5214fc92807c257bf63c9"></a>OH_NN_RESIZE_BILINEAR_EXCLUDE_OUTSIDE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>excludeOutside</b> parameter of the ResizeBilinear operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa54fa58c67a02b6727e7387c1be0c88ac"></a>OH_NN_LAYER_NORM_BEGIN_NORM_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>beginNormAxis</b> parameter of the LayerNorm operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa722eee0a2cd4a100b118fac72fb764ed"></a>OH_NN_LAYER_NORM_EPSILON&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>epsilon</b> parameter of the LayerNorm operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa3117fdc4705d25456b28abc38fabf7b6"></a>OH_NN_LAYER_NORM_BEGIN_PARAM_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>beginParamsAxis</b> parameter of the LayerNorm operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa1460b7d5e051afd08c917c42571a9fe3"></a>OH_NN_LAYER_NORM_ELEMENTWISE_AFFINE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>elementwiseAffine</b> parameter of the LayerNorm operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa5be242c91e91da6dc92e1e6ced0d3a59"></a>OH_NN_REDUCE_PROD_KEEP_DIMS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>keepDims</b> parameter of the ReduceProd operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fabd9cf8d556af406e8cde57af87c26dfa"></a>OH_NN_REDUCE_ALL_KEEP_DIMS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>keepDims</b> parameter of the ReduceAll operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa778049e9e5e3186df34d2acb997ca392"></a>OH_NN_QUANT_DTYPE_CAST_SRC_T&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>src_t</b> parameter of the QuantDTypeCast operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fad8dc93d0feddec113c3d66fe399fbe8a"></a>OH_NN_QUANT_DTYPE_CAST_DST_T&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>dst_t</b> parameter of the QuantDTypeCast operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa0987dfd85d36528ea119885e1cb8c25b"></a>OH_NN_TOP_K_SORTED&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>Sorted</b> parameter of the Topk operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa1522ea5621075974be8a785359fab133"></a>OH_NN_ARG_MAX_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the ArgMax operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faf9baeea3995403a7c0d3a4865a52aae4"></a>OH_NN_ARG_MAX_KEEPDIMS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>keepDims</b> parameter of the ArgMax operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fabacb331bf515a3e2d8591a10da1aed3b"></a>OH_NN_UNSQUEEZE_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the Unsqueeze operator. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa84dc14c2fc97ad7ff56ac4fd0961df8e"></a>OH_NN_UNSTACK_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the Unstack operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fafdf7f13517ca31556fc5f2441861de02"></a>OH_NN_FLATTEN_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the Flatten operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa5516397d37a3ca5d35fa4bac4b1206de"></a>OH_NN_DEPTH_TO_SPACE_BLOCK_SIZE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>blockSize</b> parameter of the DepthToSpace operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faf57e2973efb5fe76af2c9d4b2a6e993d"></a>OH_NN_DEPTH_TO_SPACE_MODE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>mode</b> parameter of the DepthToSpace operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fab2dd3c8f2d253ba1dfc4c7a79f6dc057"></a>OH_NN_RANGE_START&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>start</b> parameter of the Range operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa3f473820d614c41d36f61b24c817b4e1"></a>OH_NN_RANGE_LIMIT&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>limit</b> parameter of the Range operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa7ec77584f32736f27fb792b793e6a623"></a>OH_NN_RANGE_DELTA&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>delta</b> parameter of the Range operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa01640410499014b7b34f45e7b2e6892b"></a>OH_NN_CONSTANT_OF_SHAPE_DATA_TYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>dataType</b> parameter of the ConstantOfShape operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa148c32aa1ba2b217d3edfcdecc8a5a1b"></a>OH_NN_CONSTANT_OF_SHAPE_VALUE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>value</b> parameter of the ConstantOfShape operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fac40a99706ce1c8445bf30d05255b24f6"></a>OH_NN_BROADCAST_TO_SHAPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>shape</b> parameter of the BroadcastTo operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fab6ef9d3b609ba4ad99179ba41a19a93c"></a>OH_NN_INSTANCE_NORM_EPSILON&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>epsilon</b> parameter of the InstanceNorm operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa9e54b75a88e957e050435bb554796437"></a>OH_NN_EXP_BASE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>base</b> parameter of the Exp operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa778f0d14ad8871aa49165639735aac0a"></a>OH_NN_EXP_SCALE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>scale</b> parameter of the Exp operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fafa656443a1825871dfd44f4cc9e003a8"></a>OH_NN_EXP_SHIFT&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>shift</b> parameter of the Exp operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa13c7da6d3eb243a59ab46e03f0e28c6f"></a>OH_NN_LEAKY_RELU_NEGATIVE_SLOPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>negativeSlope</b> parameter of the LeakyRelu operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa467fa64e92a51de2aa0480db193e7ee1"></a>OH_NN_LSTM_BIDIRECTIONAL&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>bidirectional</b> parameter of the LSTM operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552facd799dd4615f12d74d6a68a30cf919e7"></a>OH_NN_LSTM_HAS_BIAS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>hasBias</b> parameter of the LSTM operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fac779214b8b514eef78c03a7bdb7d2850"></a>OH_NN_LSTM_INPUT_SIZE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>inputSize</b> parameter of the LSTM operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faa935d8f7dd6c797f68cf4f61b9a865d4"></a>OH_NN_LSTM_HIDDEN_SIZE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>hiddenSize</b> parameter of the LSTM operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552facbc3b728de803a2ce1a261bca6f0d812"></a>OH_NN_LSTM_NUM_LAYERS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>numLayers</b> parameter of the LSTM operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552facf791f644ff5050b80e45f5d79081e80"></a>OH_NN_LSTM_NUM_DIRECTIONS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>numDirections</b> parameter of the LSTM operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa0d27b7134c622df9ec79b3677a13b32c"></a>OH_NN_LSTM_DROPOUT&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>dropout</b> parameter of the LSTM operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fadd693416a6698f13e031df58bf1f6d64"></a>OH_NN_LSTM_ZONEOUT_CELL&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>zoneoutCell</b> parameter of the LSTM operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faa70af3c9eefa9940f4fe979d3bd8e82f"></a>OH_NN_LSTM_ZONEOUT_HIDDEN&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>zoneoutHidden</b> parameter of the LSTM operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faec667a632feda15b9aea4d8774945364"></a>OH_NN_LSTM_PROJ_SIZE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>projSize</b> parameter of the LSTM operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa7af7df1228bcf40872f57bb82396c2af"></a>OH_NN_CLIP_MAX&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>max</b> parameter of the Clip operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faaaa0c94d18bbdfaff68794631b04068c"></a>OH_NN_CLIP_MIN&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>min</b> parameter of the Clip operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa9e4dc6269e795bfda927fe1524ac23d2"></a>OH_NN_ALL_KEEP_DIMS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>keepDims</b> parameter of the All operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa9dbe2cea7a27aa2d093ba5a8a7c938c1"></a>OH_NN_ASSERT_SUMMARIZE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>summarize</b> parameter of the Assert operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa1140d3ea38546cedf673357594d7e938"></a>OH_NN_POW_SCALE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>scale</b> parameter of the pow operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fab2da80754a4f94855240e30943e218c4"></a>OH_NN_POW_SHIFT&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>shift</b> parameter of the pow operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fafcd23485034dc87e7747e78c54616c8d"></a>OH_NN_AVG_POOL_ROUND_MODE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>roundMode</b> parameter of the AvgPool operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa9a63eb8abc9b5f89e7577e581080180f"></a>OH_NN_AVG_POOL_GLOBAL&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>global</b> parameter of the AvgPool operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fac6425cf07bbf0ac36ef186cd5901c94d"></a>OH_NN_FULL_CONNECTION_HAS_BIAS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>hasBias</b> parameter of the FullConnection operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa2126c65cb84bd9f8e04c8a961a991109"></a>OH_NN_FULL_CONNECTION_USE_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>useAxis</b> parameter of the FullConnection operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa49059abb10734ad97fd426db8a49bc9c"></a>OH_NN_GELU_APPROXIMATE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>approximate</b> parameter of the GeLU operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa3a82af47fe67b85238ca0bad1e978ae4"></a>OH_NN_MAX_POOL_ROUND_MODE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>roundMode</b> parameter of the MaxPool operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa9193e6136ebc5f9232ec9cf688880392"></a>OH_NN_MAX_POOL_GLOBAL&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>global</b> parameter of the MaxPool operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fafc24db6b2d11b71f2b7c611f010e2436"></a>OH_NN_PAD_PADDING_MODE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>paddingMode</b> parameter of the Pad operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa8987c80020a86b9863d9b1cd6893cc06"></a>OH_NN_REDUCE_MEAN_REDUCE_TO_END&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>reduceToEnd</b> parameter of the ReduceMean operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa90a1945b7892a3515f5c28256ed606ff"></a>OH_NN_REDUCE_MEAN_COEFF&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>coeff</b> parameter of the ReduceMean operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa1fb9c6cdcb280e40ddc111bd457a13b3"></a>OH_NN_REDUCE_PROD_REDUCE_TO_END&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>reduceToEnd</b> parameter of the ReduceProd operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa60b0a4b69d881fb1f3084c25c72cf817"></a>OH_NN_REDUCE_PROD_COEFF&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>coeff</b> parameter of the ReduceProd operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa5ffa7f094b5b2a79caf796b78149a0c9"></a>OH_NN_REDUCE_ALL_REDUCE_TO_END&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>reduceToEnd</b> parameter of the ReduceAll operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faaa5875bed501a2b770909584f2842712"></a>OH_NN_REDUCE_ALL_COEFF&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>coeff</b> parameter of the ReduceAll operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fad7a15920eeab305ba7516450d7d9adf6"></a>OH_NN_TOP_K_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the Topk operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa8fbe61bfd51e637400656286e6be0cf8"></a>OH_NN_ARG_MAX_TOP_K&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>topK</b> parameter of the ArgMax operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fae037118ada28b8cd47a18208a9b7cbe8"></a>OH_NN_ARG_MAX_OUT_MAX_VALUE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>outMaxValue</b> parameter of the ArgMax operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa7c7dcea11e0ade9a5e1ad72e531f5870"></a>OH_NN_QUANT_DTYPE_CAST_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the QuantDTypeCast operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fafaf5f665025d330d8245f4a83c7d4526"></a>OH_NN_SLICE_AXES&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axes</b> parameter of the Slice operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faad52d7d2a61b079f4b345a39e999bafa"></a>OH_NN_TILE_DIMS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>dims</b> parameter of the Tile operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faaa379e57adbfcaaf5c51d376e25f2dc4"></a>OH_NN_CROP_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the crop operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faefe2edfe25e4bf5e01cb2d441efea878"></a>OH_NN_CROP_OFFSET&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>offset</b> parameter of the crop operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa65c5145edc01f5ea3d646b11252d1729"></a>OH_NN_DETECTION_POST_PROCESS_INPUT_SIZE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>inputSize</b> parameter of the detectionPostProcess operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa92efe047caadfd04b7b8927039408b67"></a>OH_NN_DETECTION_POST_PROCESS_SCALE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>scale</b> parameter of the detectionPostProcess operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa7e664c3e5f43b11a1e52b2f5d789765d"></a>OH_NN_DETECTION_POST_PROCESS_NMS_IOU_THRESHOLD&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>nmsIoUThreshold</b> parameter of the detectionPostProcess operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faf22ee8222962503cd6116742bc3c30e1"></a>OH_NN_DETECTION_POST_PROCESS_NMS_SCORE_THRESHOLD&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>nmsScoreThreshold</b> parameter of the detectionPostProcess operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa2a0730af48d7d87b229a69244d5538e0"></a>OH_NN_DETECTION_POST_PROCESS_MAX_DETECTIONS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>maxDetections</b> parameter of the detectionPostProcess operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fadf4d0502d20775ea7045c6569ef257da"></a>OH_NN_DETECTION_POST_PROCESS_DETECTIONS_PER_CLASS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>detectionsPerClass</b> parameter of the detectionPostProcess operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faff291b7805945c64bf459eb066d17ac1"></a>OH_NN_DETECTION_POST_PROCESS_MAX_CLASSES_PER_DETECTION&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>maxClassesPerDetection</b> parameter of the detectionPostProcess operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fadb9a882c47e6ca54cbacefc5400375d2"></a>OH_NN_DETECTION_POST_PROCESS_NUM_CLASSES&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>numClasses</b> parameter of the detectionPostProcess operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa251e60db3a813fcc39ee960b9298e684"></a>OH_NN_DETECTION_POST_PROCESS_USE_REGULAR_NMS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>useRegularNms</b> parameter of the detectionPostProcess operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faef60ed0242356dc7ff7a142ac3c3d7ed"></a>OH_NN_DETECTION_POST_PROCESS_OUT_QUANTIZED&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>outQuantized</b> parameter of the detectionPostProcess operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa8ff2c04ba34cf68117ffe4b35b6a9be5"></a>OH_NN_L2_NORMALIZE_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the L2Normalize operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fafb145050b161d406c01b927785fc5ffb"></a>OH_NN_L2_NORMALIZE_EPSILON&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>epsilon</b> parameter of the L2Normalize operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa5792b986449b605b2c87904cc5300685"></a>OH_NN_L2_NORMALIZE_ACTIVATION_TYPE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>activationType</b> parameter of the L2Normalize operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faeebc9598492b247c655356c164fa7dd8"></a>OH_NN_LOG_SOFTMAX_AXIS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>axis</b> parameter of the softmax operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa54e1cf90454ee1477f367e92507e197e"></a>OH_NN_LRN_DEPTH_RADIUS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>depthRedius</b> parameter of the LRN operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa701cdc0c2be6c1aa7b46b526c8e32a81"></a>OH_NN_LRN_BIAS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>bias</b> parameter of the LRN operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faac470f0863910fdcc8cff393ba3d1b90"></a>OH_NN_LRN_ALPHA&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>alpha</b> parameter of the LRN operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa62812191a17885810bce750adfc28d73"></a>OH_NN_LRN_BETA&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>beta</b> parameter of the LRN operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fad963c246c1ae866c6f870cdf049efa31"></a>OH_NN_LRN_NORM_REGION&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>normRegion</b> parameter of the LRN operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa7dbdece7631136fc9743e5dd1177bbf5"></a>OH_NN_SPACE_TO_DEPTH_BLOCK_SIZE&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>blockSize</b> parameter of the spaceToDepth operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa4af0a485a82222cb105d5a615564f59a"></a>OH_NN_REDUCE_MAX_KEEP_DIMS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>keepDims</b> parameter of the ReduceMax operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa9a1d920075ff790a7727c644eac9da00"></a>OH_NN_REDUCE_MAX_REDUCE_TO_END&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>reduceToEnd</b> parameter of the ReduceMax operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa0a800ba7f1065dc691b69127f9f75319"></a>OH_NN_REDUCE_MAX_COEFF&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>coeff</b> parameter of the ReduceMax operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fadea8a9a7bb0241d09d5af11e806bfce5"></a>OH_NN_REDUCE_MIN_KEEP_DIMS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>keepDims</b> parameter of the ReduceMin operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa3d2691b08e0cf2943f8d4297f43f003f"></a>OH_NN_REDUCE_MIN_REDUCE_TO_END&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>reduceToEnd</b> parameter of the ReduceMin operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fad30afd8cd016012120b5a3f85437f32e"></a>OH_NN_REDUCE_MIN_COEFF&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>coeff</b> parameter of the ReduceMin operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552facfd0be63a9ca09c9023bded7140d4c80"></a>OH_NN_REDUCE_SUM_KEEP_DIMS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>keepDims</b> parameter of the ReduceSum operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa34b9e023ea90ca66a8173cb41619419b"></a>OH_NN_REDUCE_SUM_REDUCE_TO_END&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>reduceToEnd</b> parameter of the ReduceSum operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa64699309ad1e0eb474cecd6b12a4cb0a"></a>OH_NN_REDUCE_SUM_COEFF&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>coeff</b> parameter of the ReduceSum operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa1e695825f28458244d3b692166e36426"></a>OH_NN_REDUCE_L2_KEEP_DIMS&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>keepDims</b> parameter of the ReduceL2 operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552fa14b4399bae5ff35b1555af4d7f06150a"></a>OH_NN_REDUCE_L2_REDUCE_TO_END&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>reduceToEnd</b> parameter of the ReduceL2 operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
<tr><td class="fieldname"><a id="gga09f35be6e0f86f5d747192ce5812552faae7eafa55ceadd4761ca1d2549673522"></a>OH_NN_REDUCE_L2_COEFF&#160;</td><td class="fielddoc"><p>This enumerated value is used when the tensor is used as the <b>coeff</b> parameter of the ReduceL2 operator. </p><dl class="section since"><dt>Since</dt><dd>12 </dd></dl>
</td></tr>
</table>

<p class="definition">Definition at line <a class="el" href="neural__network__runtime__type_8h_source.html#l02801">2801</a> of file <a class="el" href="neural__network__runtime__type_8h_source.html">neural_network_runtime_type.h</a>.</p>

</div>
</div>
<h2 class="groupheader">Function Documentation</h2>
<a id="gad22c665884bc562e63141bd553d081e8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gad22c665884bc562e63141bd553d081e8">&#9670;&nbsp;</a></span>OH_NNCompilation_AddExtensionConfig()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNCompilation_AddExtensionConfig </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td>
          <td class="paramname"><em>compilation</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const char *&#160;</td>
          <td class="paramname"><em>configName</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const void *&#160;</td>
          <td class="paramname"><em>configValue</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const size_t&#160;</td>
          <td class="paramname"><em>configValueSize</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Adds an extension config for a custom hardware attribute. </p>
<p>Some devices have their own specific attributes which have not been opened in NNRt. This method provides an additional way for you to set these custom hardware attributes of the device. You should query their names and values from the device vendor's documents, and add them into compilation instance one by one. These attributes will be passed directly to device driver, and this method will return error code if the driver cannot parse them. <br />
 After <a class="el" href="group___neural_nework_runtime.html#ga8a65c2ea5b8ce232f5acefbf064c1a9b">OH_NNCompilation_Build</a> is called, the <b>configName</b> and <b>configValue</b> can be released. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">compilation</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance. </td></tr>
    <tr><td class="paramname">configName</td><td>Config name. </td></tr>
    <tr><td class="paramname">configValue</td><td>A byte buffer saving the config value. </td></tr>
    <tr><td class="paramname">configValueSize</td><td>Byte size of the config value. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> add extension config successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to add extension config. The possible reason for failure is that the <b>compilation</b>, <b>configName</b> or <b>configValue</b> is nullptr, or <b>configValueSize</b> is 0.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae480781f8f23d040a2c8aad3c170783e">OH_NN_FAILED</a> other failures, such as memory error during object creation.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga8a65c2ea5b8ce232f5acefbf064c1a9b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga8a65c2ea5b8ce232f5acefbf064c1a9b">&#9670;&nbsp;</a></span>OH_NNCompilation_Build()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNCompilation_Build </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td>
          <td class="paramname"><em>compilation</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compiles a model. </p>
<p>After the compilation configuration is complete, call this method to return the compilation result. The compilation instance pushes the model and compilation options to the device for compilation. After this method is called, additional compilation operations cannot be performed. <br />
 If the <a class="el" href="group___neural_nework_runtime.html#ga7a49f89704bc0ca23718089ccdb7f4bc">OH_NNCompilation_SetDevice</a>, <a class="el" href="group___neural_nework_runtime.html#gaf39f535a7f493c13b3da544b11808e64">OH_NNCompilation_SetCache</a>, <a class="el" href="group___neural_nework_runtime.html#gaf7924c5da4c40acd7ffacacec273ab5d">OH_NNCompilation_SetPerformanceMode</a>, <a class="el" href="group___neural_nework_runtime.html#ga6299bad43aee5562ab729211b46cb121">OH_NNCompilation_SetPriority</a>, and <a class="el" href="group___neural_nework_runtime.html#ga2ca3b78ee42ff11890d52923238eebb2">OH_NNCompilation_EnableFloat16</a> methods are called, <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> is returned. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">compilation</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> build model successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to build model. The possible reason for failure is that the <b>compilation</b> is nullptr, or the parameters set before is invalid.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae480781f8f23d040a2c8aad3c170783e">OH_NN_FAILED</a> fail to build model.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> the backend device is not supported the model.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga642ce605311075cd5d5cc5b527fee3b6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga642ce605311075cd5d5cc5b527fee3b6">&#9670;&nbsp;</a></span>OH_NNCompilation_Construct()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a>* OH_NNCompilation_Construct </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *&#160;</td>
          <td class="paramname"><em>model</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates a compilation instance of the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> type. </p>
<p>After the OH_NNModel module completes model construction, APIs provided by the OH_NNCompilation module pass the model to underlying device for compilation. This method creates a <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance based on the passed <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance. The <a class="el" href="group___neural_nework_runtime.html#ga7a49f89704bc0ca23718089ccdb7f4bc">OH_NNCompilation_SetDevice</a> method is called to set the device to compile on, and <a class="el" href="group___neural_nework_runtime.html#ga8a65c2ea5b8ce232f5acefbf064c1a9b">OH_NNCompilation_Build</a> is then called to complete compilation.<br />
 In addition to computing device selection, the OH_NNCompilation module supports features such as model caching, performance preference, priority setting, and float16 computing, which can be implemented by the following methods:<br />
<a class="el" href="group___neural_nework_runtime.html#gaf39f535a7f493c13b3da544b11808e64">OH_NNCompilation_SetCache</a><br />
<a class="el" href="group___neural_nework_runtime.html#gaf7924c5da4c40acd7ffacacec273ab5d">OH_NNCompilation_SetPerformanceMode</a><br />
<a class="el" href="group___neural_nework_runtime.html#ga6299bad43aee5562ab729211b46cb121">OH_NNCompilation_SetPriority</a><br />
<a class="el" href="group___neural_nework_runtime.html#ga2ca3b78ee42ff11890d52923238eebb2">OH_NNCompilation_EnableFloat16</a><br />
 After <a class="el" href="group___neural_nework_runtime.html#ga8a65c2ea5b8ce232f5acefbf064c1a9b">OH_NNCompilation_Build</a> is called, the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance can be released.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">model</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pointer to a <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance, or NULL if it fails to create. The possible reason for failure is that the parameters of model are invalid or there is a problem with the model format. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gaad1fae1380a70f07920c2aad41b38f8b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaad1fae1380a70f07920c2aad41b38f8b">&#9670;&nbsp;</a></span>OH_NNCompilation_ConstructForCache()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a>* OH_NNCompilation_ConstructForCache </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates a empty compilation instance for restoration from cache later. </p>
<p>See <a class="el" href="group___neural_nework_runtime.html#gaf39f535a7f493c13b3da544b11808e64">OH_NNCompilation_SetCache</a> for the description of cache.<br />
 The restoration time from the cache is less than compilation with <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a>.<br />
 You should call <a class="el" href="group___neural_nework_runtime.html#gaf39f535a7f493c13b3da544b11808e64">OH_NNCompilation_SetCache</a> or <a class="el" href="group___neural_nework_runtime.html#ga1c2c203801d794b98769fd655e24c302">OH_NNCompilation_ImportCacheFromBuffer</a> first, and then call <a class="el" href="group___neural_nework_runtime.html#ga8a65c2ea5b8ce232f5acefbf064c1a9b">OH_NNCompilation_Build</a> to complete the restoration.<br />
 </p><dl class="section return"><dt>Returns</dt><dd>Pointer to an <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance, or NULL if it fails to create. The possible reason for failure is that the cache file saved before is invalid. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gaeb58a0dce316a5b1c5fe281e277e7830"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaeb58a0dce316a5b1c5fe281e277e7830">&#9670;&nbsp;</a></span>OH_NNCompilation_ConstructWithOfflineModelBuffer()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a>* OH_NNCompilation_ConstructWithOfflineModelBuffer </td>
          <td>(</td>
          <td class="paramtype">const void *&#160;</td>
          <td class="paramname"><em>modelBuffer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>modelSize</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates a compilation instance based on an offline model file buffer. </p>
<p>This method conflicts with the way of passing an online built model or an offline model file path, and you have to choose only one of the three construction methods. <br />
 Note that the returned <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance only saves the <b>modelBuffer</b> pointer inside, instead of copying its data. You should not release <b>modelBuffer</b> before the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance is destroied. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">modelBuffer</td><td>Offline model file buffer. </td></tr>
    <tr><td class="paramname">modelSize</td><td>Offfline model buffer size. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pointer to an <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance, or NULL if it fails to create. The possible reason for failure is that the modelBuffer or modelSize is invalid. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gace3c05942fb3649efd3ab712e35a2ef3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gace3c05942fb3649efd3ab712e35a2ef3">&#9670;&nbsp;</a></span>OH_NNCompilation_ConstructWithOfflineModelFile()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a>* OH_NNCompilation_ConstructWithOfflineModelFile </td>
          <td>(</td>
          <td class="paramtype">const char *&#160;</td>
          <td class="paramname"><em>modelPath</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates a compilation instance based on an offline model file. </p>
<p>This method conflicts with the way of passing an online built model or an offline model file buffer, and you have to choose only one of the three construction methods. <br />
 Offline model is a type of model that is offline compiled by the model converter provided by a device vendor. So that the offline model can only be used on the specified device, but the compilation time of offline model is usually much less than <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a>. <br />
</p>
<p>You should perform the offline compilation during your development and deploy the offline model in your app package. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">modelPath</td><td>Offline model file path. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pointer to an <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance, or NULL if it fails to create. The possible reason for failure is that the modelPath is invalid. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gae3ded5e4036db67e17313cbe7275f906"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gae3ded5e4036db67e17313cbe7275f906">&#9670;&nbsp;</a></span>OH_NNCompilation_Destroy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void OH_NNCompilation_Destroy </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> **&#160;</td>
          <td class="paramname"><em>compilation</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Releases the <b>Compilation</b> object. </p>
<p>This method needs to be called to release the compilation instance created by <a class="el" href="group___neural_nework_runtime.html#ga642ce605311075cd5d5cc5b527fee3b6">OH_NNCompilation_Construct</a>, <a class="el" href="group___neural_nework_runtime.html#gace3c05942fb3649efd3ab712e35a2ef3">OH_NNCompilation_ConstructWithOfflineModelFile</a>, <a class="el" href="group___neural_nework_runtime.html#gaeb58a0dce316a5b1c5fe281e277e7830">OH_NNCompilation_ConstructWithOfflineModelBuffer</a> and <a class="el" href="group___neural_nework_runtime.html#gaad1fae1380a70f07920c2aad41b38f8b">OH_NNCompilation_ConstructForCache</a>. Otherwise, the memory leak will occur. <br />
 If <b>compilation</b> or <b>*compilation</b> is a null pointer, this method only prints warning logs and does not execute the release. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">compilation</td><td>Double pointer to the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance. After a compilation instance is destroyed, this method sets <b>*compilation</b> to a null pointer. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga2ca3b78ee42ff11890d52923238eebb2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2ca3b78ee42ff11890d52923238eebb2">&#9670;&nbsp;</a></span>OH_NNCompilation_EnableFloat16()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNCompilation_EnableFloat16 </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td>
          <td class="paramname"><em>compilation</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>enableFloat16</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Enables float16 for computing. </p>
<p>Float32 is used by default for the model of float type. If this method is called on a device that supports float16, float16 will be used for computing the float32 model to reduce memory usage and execution time. <br />
 This option is useless for the model of int type, e.g. int8 type. <br />
 If this method is called on the device that does not support float16, the <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a59a096979dfc59e124f7889012bbf233">OH_NN_UNAVALIDABLE_DEVICE</a> error code is returned. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">compilation</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance. </td></tr>
    <tr><td class="paramname">enableFloat16</td><td>Indicates whether to enable float16. If this parameter is set to <b>true</b>, float16 inference is performed. If this parameter is set to <b>false</b>, float32 inference is performed. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> enable fp16 successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to enable fp16. The possible reason for failure is that the <b>compilation</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga22d3a939459ea07c94ed1b4cc3458314"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga22d3a939459ea07c94ed1b4cc3458314">&#9670;&nbsp;</a></span>OH_NNCompilation_ExportCacheToBuffer()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNCompilation_ExportCacheToBuffer </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td>
          <td class="paramname"><em>compilation</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const void *&#160;</td>
          <td class="paramname"><em>buffer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>length</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>modelSize</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Exports the cache to a given buffer. </p>
<p>See <a class="el" href="group___neural_nework_runtime.html#gaf39f535a7f493c13b3da544b11808e64">OH_NNCompilation_SetCache</a> for the description of cache.<br />
 Note that the cache is the result of compilation building <a class="el" href="group___neural_nework_runtime.html#ga8a65c2ea5b8ce232f5acefbf064c1a9b">OH_NNCompilation_Build</a>, so that this method must be called after <a class="el" href="group___neural_nework_runtime.html#ga8a65c2ea5b8ce232f5acefbf064c1a9b">OH_NNCompilation_Build</a>.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">compilation</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance. </td></tr>
    <tr><td class="paramname">buffer</td><td>Pointer to the given buffer. </td></tr>
    <tr><td class="paramname">length</td><td>Buffer length. </td></tr>
    <tr><td class="paramname">modelSize</td><td>Byte size of the model cache. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> export cache to buffer successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to export cache to buffer. The possible reason for failure is that the <b>compilation</b>, <b>buffer</b> or <b>modelSize</b> is nullptr, or <b>length</b> is 0, or <b>compilation</b> is invalid.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae6455b906d5a7aee0597798f7a00c529">OH_NN_UNSUPPORTED</a> exporting cache to buffer is unsupported.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga1c2c203801d794b98769fd655e24c302"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga1c2c203801d794b98769fd655e24c302">&#9670;&nbsp;</a></span>OH_NNCompilation_ImportCacheFromBuffer()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNCompilation_ImportCacheFromBuffer </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td>
          <td class="paramname"><em>compilation</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const void *&#160;</td>
          <td class="paramname"><em>buffer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>modelSize</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Imports the cache from a given buffer. </p>
<p>See <a class="el" href="group___neural_nework_runtime.html#gaf39f535a7f493c13b3da544b11808e64">OH_NNCompilation_SetCache</a> for the description of cache.<br />
 <a class="el" href="group___neural_nework_runtime.html#ga8a65c2ea5b8ce232f5acefbf064c1a9b">OH_NNCompilation_Build</a> should be called to complete the restoration after <a class="el" href="group___neural_nework_runtime.html#ga1c2c203801d794b98769fd655e24c302">OH_NNCompilation_ImportCacheFromBuffer</a> is called.<br />
 Note that <b>compilation</b> only saves the <b>buffer</b> pointer inside, instead of copying its data. You should not release <b>buffer</b> before <b>compilation</b> is destroied.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">compilation</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance. </td></tr>
    <tr><td class="paramname">buffer</td><td>Pointer to the given buffer. </td></tr>
    <tr><td class="paramname">modelSize</td><td>Byte size of the model cache. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> import cache from buffer successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to import cache from buffer. The possible reason for failure is that the <b>compilation</b> or <b>buffer</b> is nullptr, or <b>modelSize</b> is 0, or content of <b>buffer</b> is invalid.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gaf39f535a7f493c13b3da544b11808e64"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaf39f535a7f493c13b3da544b11808e64">&#9670;&nbsp;</a></span>OH_NNCompilation_SetCache()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNCompilation_SetCache </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td>
          <td class="paramname"><em>compilation</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const char *&#160;</td>
          <td class="paramname"><em>cachePath</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>version</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Set the cache directory and version of the compiled model. </p>
<p>On the device that supports caching, a model can be saved as a cache file after being compiled on the device driver. The model can be directly read from the cache file in the next compilation, saving recompilation time. This method performs different operations based on the passed cache directory and version: <br />
</p><ul>
<li>No file exists in the cache directory: Caches the compiled model to the directory and sets the cache version to <b>version</b>. <br />
</li>
<li>A complete cache file exists in the cache directory, and its version is <b>version</b>: Reads the cache file in the path and passes the data to the underlying device for conversion into executable model instances. <br />
</li>
<li>A complete cache file exists in the cache directory, and its version is earlier than <b>version</b>: When model compilation is complete on the underlying device, overwrites the cache file and changes the version number to <b>version</b>. <br />
</li>
<li>A complete cache file exists in the cache directory, and its version is later than <b>version</b>: Returns the <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> error code without reading the cache file. <br />
</li>
<li>The cache file in the cache directory is incomplete or you do not have the permission to access the cache file. Returns the <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ace52583ce1eccad5a5862f4771818cdf">OH_NN_INVALID_FILE</a> error code. <br />
</li>
<li>The cache directory does not exist or you do not have the access permission. Returns the <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa7571ab718fe691536f700ce17aa77cf">OH_NN_INVALID_PATH</a> error code. <br />
 <dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">compilation</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance. </td></tr>
    <tr><td class="paramname">cachePath</td><td>Directory for storing model cache files. This method creates directories for different devices in the <b>cachePath</b> directory. You are advised to use a separate cache directory for each model. </td></tr>
    <tr><td class="paramname">version</td><td>Cache version. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set cache path and version successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set cache path and version. The possible reason for failure is that the <b>compilation</b> or <b>cachePath</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>
</li>
</ul>

</div>
</div>
<a id="ga7a49f89704bc0ca23718089ccdb7f4bc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga7a49f89704bc0ca23718089ccdb7f4bc">&#9670;&nbsp;</a></span>OH_NNCompilation_SetDevice()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNCompilation_SetDevice </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td>
          <td class="paramname"><em>compilation</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>deviceID</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Specifies the device for model compilation and computing. </p>
<p>In the compilation phase, you need to specify the device for model compilation and computing. Call <a class="el" href="group___neural_nework_runtime.html#ga66c11e4935abaa6710a1d8eb4d09395f">OH_NNDevice_GetAllDevicesID</a> to obtain available device IDs. Call <a class="el" href="group___neural_nework_runtime.html#ga06a27876b3f3bf0170031b26c4d04f62">OH_NNDevice_GetType</a> and <a class="el" href="group___neural_nework_runtime.html#ga27d205f26b9e1dd1a0a5bb2e058341a9">OH_NNDevice_GetName</a> to obtain device information and pass target device ID to this method for setting. <br />
</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">compilation</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance. </td></tr>
    <tr><td class="paramname">deviceID</td><td>Device id. If it is 0, the first device in the current device list will be used by default. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set device successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set device. The possible reason for failure is that the <b>compilation</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gaf7924c5da4c40acd7ffacacec273ab5d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaf7924c5da4c40acd7ffacacec273ab5d">&#9670;&nbsp;</a></span>OH_NNCompilation_SetPerformanceMode()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNCompilation_SetPerformanceMode </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td>
          <td class="paramname"><em>compilation</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga2ea6daeb95ee5bea81f97a5d9aeef5bf">OH_NN_PerformanceMode</a>&#160;</td>
          <td class="paramname"><em>performanceMode</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the performance mode for model computing. </p>
<p>Allows you to set the performance mode for model computing to meet the requirements of low power consumption and ultimate performance. If this method is not called to set the performance mode in the compilation phase, the compilation instance assigns the <a class="el" href="group___neural_nework_runtime.html#gga2ea6daeb95ee5bea81f97a5d9aeef5bfa9d728aef494dfd8a589ac3478b5859b0">OH_NN_PERFORMANCE_NONE</a> mode for the model by default. In this case, the device performs computing in the default performance mode. <br />
 If this method is called on the device that does not support the setting of the performance mode, the <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a59a096979dfc59e124f7889012bbf233">OH_NN_UNAVALIDABLE_DEVICE</a> error code is returned. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">compilation</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance. </td></tr>
    <tr><td class="paramname">performanceMode</td><td>Performance mode. For details about the available performance modes, see <a class="el" href="group___neural_nework_runtime.html#ga2ea6daeb95ee5bea81f97a5d9aeef5bf">OH_NN_PerformanceMode</a>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set performance mode successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set performance mode. The possible reason for failure is that the <b>compilation</b> is nullptr, or <b>performanceMode</b> is invalid.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae480781f8f23d040a2c8aad3c170783e">OH_NN_FAILED</a> fail to query whether the backend device supports setting performance mode.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> the backend device is not supported to set performance mode.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga6299bad43aee5562ab729211b46cb121"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga6299bad43aee5562ab729211b46cb121">&#9670;&nbsp;</a></span>OH_NNCompilation_SetPriority()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNCompilation_SetPriority </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td>
          <td class="paramname"><em>compilation</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga5ae0ed09b49e98f608f47ac4fe9ecb07">OH_NN_Priority</a>&#160;</td>
          <td class="paramname"><em>priority</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the model computing priority. </p>
<p>Allows you to set computing priorities for models. The priorities apply only to models created by the process with the same UID. The settings will not affect models created by processes with different UIDs on different devices. <br />
 If this method is called on the device that does not support the priority setting, the <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a59a096979dfc59e124f7889012bbf233">OH_NN_UNAVALIDABLE_DEVICE</a> error code is returned. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">compilation</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance. </td></tr>
    <tr><td class="paramname">priority</td><td>Priority. For details about the optional priorities, see <a class="el" href="group___neural_nework_runtime.html#ga5ae0ed09b49e98f608f47ac4fe9ecb07">OH_NN_Priority</a>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set priority successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set priority. The possible reason for failure is that the <b>compilation</b> is nullptr, or <b>priority</b> is invalid.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae480781f8f23d040a2c8aad3c170783e">OH_NN_FAILED</a> fail to query whether the backend device supports setting priority.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> the backend device is not supported to set priority.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga66c11e4935abaa6710a1d8eb4d09395f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga66c11e4935abaa6710a1d8eb4d09395f">&#9670;&nbsp;</a></span>OH_NNDevice_GetAllDevicesID()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNDevice_GetAllDevicesID </td>
          <td>(</td>
          <td class="paramtype">const size_t **&#160;</td>
          <td class="paramname"><em>allDevicesID</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t *&#160;</td>
          <td class="paramname"><em>deviceCount</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Obtains the IDs of all devices connected. </p>
<p>Each device has an unique and fixed ID. This method returns device IDs on the current device through the uint32_t array.<br />
 Device IDs are returned through the size_t array. Each element of the array is the ID of a single device.<br />
 The array memory is managed inside, so you do not need to care about it. The data pointer is valid before this method is called next time.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">allDevicesID</td><td>Pointer to the size_t array. The input <b>*allDevicesID</b> must be a null pointer. Otherwise, <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> is returned. </td></tr>
    <tr><td class="paramname">deviceCount</td><td>Pointer of the uint32_t type, which is used to return the length of <b>*allDevicesID</b>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get all devices id successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get all devices id. The possible reason for failure is that the <b>allDevicesID</b> or <b>deviceCount</b> is nullptr, or <b>*allDevicesID</b> is not nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga27d205f26b9e1dd1a0a5bb2e058341a9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga27d205f26b9e1dd1a0a5bb2e058341a9">&#9670;&nbsp;</a></span>OH_NNDevice_GetName()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNDevice_GetName </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>deviceID</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const char **&#160;</td>
          <td class="paramname"><em>name</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Obtains the name of the specified device. </p>
<p><b>deviceID</b> specifies the device whose name will be obtained. The device ID needs to be obtained by calling <a class="el" href="group___neural_nework_runtime.html#ga66c11e4935abaa6710a1d8eb4d09395f">OH_NNDevice_GetAllDevicesID</a>. If it is 0, the first device in the current device list will be used by default.<br />
 The value of <b>*name</b> is a C-style string ended with <b>'\0'</b>. <b>*name</b> must be a null pointer. Otherwise, <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> is returned. Fou example, you should define char* deviceName = NULL, and pass &amp;deviceName as the argument of <b>name</b>.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">deviceID</td><td>Device ID. If it is 0, the first device in the current device list will be used by default. </td></tr>
    <tr><td class="paramname">name</td><td>The device name returned. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get name of specific device successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get name of specific device. The possible reason for failure is that the <b>name</b> is nullptr or <b>*name</b> is not nullptr.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae480781f8f23d040a2c8aad3c170783e">OH_NN_FAILED</a> fail to get name of specific device. The possible reason for failure is that the <b>deviceID</b> is invalid.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga06a27876b3f3bf0170031b26c4d04f62"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga06a27876b3f3bf0170031b26c4d04f62">&#9670;&nbsp;</a></span>OH_NNDevice_GetType()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNDevice_GetType </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>deviceID</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#gae99ff81dfe03a24f714261009b5d2bff">OH_NN_DeviceType</a> *&#160;</td>
          <td class="paramname"><em>deviceType</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Obtains the type information of the specified device. </p>
<p><b>deviceID</b> specifies the device whose type will be obtained. If it is 0, the first device in the current device list will be used. Currently the following device types are supported:</p><ul>
<li><b>OH_NN_CPU</b>: CPU device.</li>
<li><b>OH_NN_GPU</b>: GPU device.</li>
<li><b>OH_NN_ACCELERATOR</b>: machine learning dedicated accelerator.</li>
<li><b>OH_NN_OTHERS</b>: other hardware types. <br />
 <dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">deviceID</td><td>Device ID. If it is 0, the first device in the current device list will be used by default. </td></tr>
    <tr><td class="paramname">deviceType</td><td>The device type <a class="el" href="group___neural_nework_runtime.html#gae99ff81dfe03a24f714261009b5d2bff">OH_NN_DeviceType</a> returned. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get type of specific device successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get type of specific device. The possible reason for failure is that the <b>deviceType</b> is nullptr.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae480781f8f23d040a2c8aad3c170783e">OH_NN_FAILED</a> fail to get type of specific device. The possible reason for failure is that the <b>deviceID</b> is invalid.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>
</li>
</ul>

</div>
</div>
<a id="ga7d32f81c66dc901ab40c36b17995e32d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga7d32f81c66dc901ab40c36b17995e32d">&#9670;&nbsp;</a></span>OH_NNExecutor_AllocateInputMemory()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a>* OH_NNExecutor_AllocateInputMemory </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>inputIndex</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>length</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Allocates shared memory to a single input on a device. </p>
<p>Neural Network Runtime provides a method for proactively allocating shared memory on a device. By specifying the executor and input index value, this method allocates shared memory whose size is specified by <b>length</b> on the device associated with a single input and returns the operation result through the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">inputIndex</td><td>Input index value, which is in the same sequence of the data input when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. Assume that the value of <b>inputIndices</b> is <b>{1, 5, 9}</b> when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. In the memory input application, the index value for the three inputs is <b>{0, 1, 2}</b>. </td></tr>
    <tr><td class="paramname">length</td><td>Memory size to be applied for, in bytes. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pointer to a <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance, or NULL if it fails to create. </dd></dl>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000098">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#gad658b50239b119baece25c7470ccf403">OH_NNTensor_CreateWithSize</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gad0ed524e694defcf0b2d7479a7784967"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gad0ed524e694defcf0b2d7479a7784967">&#9670;&nbsp;</a></span>OH_NNExecutor_AllocateOutputMemory()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a>* OH_NNExecutor_AllocateOutputMemory </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>outputIndex</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>length</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Allocates shared memory to a single output on a device. </p>
<p>Neural Network Runtime provides a method for proactively allocating shared memory on a device. By specifying the executor and output index value, this method allocates shared memory whose size is specified by <b>length</b> on the device associated with a single output and returns the operation result through the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">outputIndex</td><td>Output Index value, which is in the same sequence of the data output when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. Assume that the value of <b>outputIndices</b> is <b>{4, 6, 8}</b> when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. In output memory application, the index value for the three outputs is <b>{0, 1, 2}</b>. </td></tr>
    <tr><td class="paramname">length</td><td>Memory size to be applied for, in bytes. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pointer to a <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance, or NULL if it fails to create. </dd></dl>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000099">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#gad658b50239b119baece25c7470ccf403">OH_NNTensor_CreateWithSize</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gaca3d4495ef04ebb7e32400bbf9d61c5d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaca3d4495ef04ebb7e32400bbf9d61c5d">&#9670;&nbsp;</a></span>OH_NNExecutor_Construct()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a>* OH_NNExecutor_Construct </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> *&#160;</td>
          <td class="paramname"><em>compilation</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates an executor instance of the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> type. </p>
<p>This method constructs a model inference executor associated with the device based on the passed compilation. <br />
 After the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance is created, you can release the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance if you do not need to create any other executors. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">compilation</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga717fa252bcbce3942c2ec2b25238492b">OH_NNCompilation</a> instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pointer to a <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance, or NULL if it fails to create. The possible reason for failure is that the <b>compilation</b> is nullptr, or memory error occurred. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gad55eda40f5087c543911469a6e003916"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gad55eda40f5087c543911469a6e003916">&#9670;&nbsp;</a></span>OH_NNExecutor_CreateInputTensorDesc()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>* OH_NNExecutor_CreateInputTensorDesc </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>index</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates an input tensor descriptor with its index. </p>
<p>The input tensor descriptor contains all attributes of the input tensor. If the <b>index</b> is greater than or equal to the input tensor number, this method will return error code. The input tensor number can be got by <a class="el" href="group___neural_nework_runtime.html#ga15cc92e6c5cb825b154c367108bc28cc">OH_NNExecutor_GetInputCount</a>.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">index</td><td>Input tensor index. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pointer to <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance, or NULL if it fails to create. The possible reason for failure is that the <b>executor</b> is nullptr, or <b>index</b> is out of range. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga9f8d0428fc8f7568e7569e8bf9e3eb4e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga9f8d0428fc8f7568e7569e8bf9e3eb4e">&#9670;&nbsp;</a></span>OH_NNExecutor_CreateOutputTensorDesc()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>* OH_NNExecutor_CreateOutputTensorDesc </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>index</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates an output tensor descriptor with its index. </p>
<p>The output tensor descriptor contains all attributes of the output tensor. If the <b>index</b> is greater than or equal to the output tensor number, this method will return error code. The output tensor number can be got by <a class="el" href="group___neural_nework_runtime.html#gaf679893bb2413ce72229b310c91e3663">OH_NNExecutor_GetOutputCount</a>.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">index</td><td>Output tensor index. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pointer to <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance, or NULL if it fails to create. The possible reason for failure is that the <b>executor</b> is nullptr, or <b>index</b> is out of range. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga8d7fade61b14d54446f7076e688c30f2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga8d7fade61b14d54446f7076e688c30f2">&#9670;&nbsp;</a></span>OH_NNExecutor_Destroy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void OH_NNExecutor_Destroy </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> **&#160;</td>
          <td class="paramname"><em>executor</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Destroys an executor instance to release the memory occupied by the executor. </p>
<p>This method needs to be called to release the executor instance created by calling <a class="el" href="group___neural_nework_runtime.html#gaca3d4495ef04ebb7e32400bbf9d61c5d">OH_NNExecutor_Construct</a>. Otherwise, the memory leak will occur. <br />
 If <b>executor</b> or <b>*executor</b> is a null pointer, this method only prints warning logs and does not execute the release. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Double pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga2e52accdd70c1811eb67da79d557749d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2e52accdd70c1811eb67da79d557749d">&#9670;&nbsp;</a></span>OH_NNExecutor_DestroyInputMemory()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void OH_NNExecutor_DestroyInputMemory </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>inputIndex</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> **&#160;</td>
          <td class="paramname"><em>memory</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Releases the input memory to which the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance points. </p>
<p>This method needs to be called to release the memory instance created by calling <a class="el" href="group___neural_nework_runtime.html#ga7d32f81c66dc901ab40c36b17995e32d">OH_NNExecutor_AllocateInputMemory</a>. Otherwise, memory leak will occur. The mapping between <b>inputIndex</b> and <b>memory</b> must be the same as that in memory instance creation.<br />
 If <b>memory</b> or <b>*memory</b> is a null pointer, this method only prints warning logs and does not execute the release logic.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">inputIndex</td><td>Input index value, which is in the same sequence of the data input when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. Assume that the value of <b>inputIndices</b> is <b>{1, 5, 9}</b> when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. In memory input release, the index value for the three inputs is <b>{0, 1, 2}</b>. </td></tr>
    <tr><td class="paramname">memory</td><td>Double pointer to the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance. After shared memory is destroyed, this method sets <b>*memory</b> to a null pointer. </td></tr>
  </table>
  </dd>
</dl>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000100">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#ga33d9d392ba17947c578b099961d32789">OH_NNTensor_Destroy</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga7702355958f4c9faf48509bdf386695b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga7702355958f4c9faf48509bdf386695b">&#9670;&nbsp;</a></span>OH_NNExecutor_DestroyOutputMemory()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void OH_NNExecutor_DestroyOutputMemory </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>outputIndex</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> **&#160;</td>
          <td class="paramname"><em>memory</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Releases the output memory to which the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance points. </p>
<p>This method needs to be called to release the memory instance created by calling <a class="el" href="group___neural_nework_runtime.html#gad0ed524e694defcf0b2d7479a7784967">OH_NNExecutor_AllocateOutputMemory</a>. Otherwise, memory leak will occur. The mapping between <b>outputIndex</b> and <b>memory</b> must be the same as that in memory instance creation.<br />
 If <b>memory</b> or <b>*memory</b> is a null pointer, this method only prints warning logs and does not execute the release logic.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">outputIndex</td><td>Output Index value, which is in the same sequence of the data output when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. Assume that the value of <b>outputIndices</b> is <b>{4, 6, 8}</b> when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. In output memory release, the index value for the three outputs is <b>{0, 1, 2}</b>. </td></tr>
    <tr><td class="paramname">memory</td><td>Double pointer to the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance. After shared memory is destroyed, this method sets <b>*memory</b> to a null pointer. </td></tr>
  </table>
  </dd>
</dl>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000101">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#ga33d9d392ba17947c578b099961d32789">OH_NNTensor_Destroy</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga15cc92e6c5cb825b154c367108bc28cc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga15cc92e6c5cb825b154c367108bc28cc">&#9670;&nbsp;</a></span>OH_NNExecutor_GetInputCount()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_GetInputCount </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>inputCount</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the input tensor count. </p>
<p>You can get the input tensor count from the executor, and then create an input tensor descriptor with its index by <a class="el" href="group___neural_nework_runtime.html#gad55eda40f5087c543911469a6e003916">OH_NNExecutor_CreateInputTensorDesc</a>. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">inputCount</td><td>Input tensor count returned. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get input count successfully. The return value is saved in <b>inputCount</b>.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get input count. The possible reason for failure is that the <b>executor</b> or <b>inputCount</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga91e0d9066ddd07ed2e12d2bd3971c9fa"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga91e0d9066ddd07ed2e12d2bd3971c9fa">&#9670;&nbsp;</a></span>OH_NNExecutor_GetInputDimRange()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_GetInputDimRange </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t **&#160;</td>
          <td class="paramname"><em>minInputDims</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t **&#160;</td>
          <td class="paramname"><em>maxInputDims</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>shapeLength</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the dimension ranges of an input tensor. </p>
<p>The supported dimension ranges of an input tensor with dynamic shape may be different among various devices. You can call this method to get the dimension ranges of the input tensor supported by the device. <b>*minInputDims</b> contains the minimum demensions of the input tensor, and <b>*maxInputDims</b> contains the maximum, e.g. if an input tensor has dynamic shape [-1, -1, -1, 3], its <b>*minInputDims</b> may be [1, 10, 10, 3] and <b>*maxInputDims</b> may be [100, 1024, 1024, 3] on the device.<br />
 If the <b>index</b> is greater than or equal to the input tensor number, this method will return error code. The input tensor number can be got by <a class="el" href="group___neural_nework_runtime.html#ga15cc92e6c5cb825b154c367108bc28cc">OH_NNExecutor_GetInputCount</a>.<br />
 As an output parameter, <b>*minInputDims</b> or <b>*maxInputDims</b> must be a null pointer, otherwise the method will return an error code. For example, you should define int32_t* minInDims = NULL, and pass &amp;minInDims as the argument of <b>minInputDims</b>.<br />
 You do not need to release the memory of <b>*minInputDims</b> or <b>*maxInputDims</b>. It will be released when <b>executor</b> is destroied.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">index</td><td>Input tensor index. </td></tr>
    <tr><td class="paramname">minInputDims</td><td>Returned pointer to an array contains the minimum dimensions of the input tensor. </td></tr>
    <tr><td class="paramname">maxInputDims</td><td>Returned pointer to an array contains the maximum dimensions of the input tensor. </td></tr>
    <tr><td class="paramname">shapeLength</td><td>Returned length of the shape of input tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get input dim range successfully. The return value is saved in <b>minInputDims</b>, <b>maxInputDims</b> and <b>shapeLength</b>.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get input dim range. The possible reason for failure is that the <b>executor</b>, <b>minInputDims</b>, <b>maxInputDims</b> or <b>shapeLength</b> is nullptr, or <b>*minInputDims</b> or <b>*maxInputDims</b> is not nullptr, or <b>index</b> is out of range.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> the backend device is not supported to get input dim range.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gaf679893bb2413ce72229b310c91e3663"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaf679893bb2413ce72229b310c91e3663">&#9670;&nbsp;</a></span>OH_NNExecutor_GetOutputCount()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_GetOutputCount </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>outputCount</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the output tensor count. </p>
<p>You can get the output tensor count from the executor, and then create an output tensor descriptor with its index by <a class="el" href="group___neural_nework_runtime.html#ga9f8d0428fc8f7568e7569e8bf9e3eb4e">OH_NNExecutor_CreateOutputTensorDesc</a>. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">outputCount</td><td>Output tensor count returned. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get output count successfully. The return value is saved in <b>outputCount</b>.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get output count. The possible reason for failure is that the <b>executor</b> or <b>outputCount</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gadcc8c5d6bec42ffb06d2ce3b6698207d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gadcc8c5d6bec42ffb06d2ce3b6698207d">&#9670;&nbsp;</a></span>OH_NNExecutor_GetOutputShape()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_GetOutputShape </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>outputIndex</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int32_t **&#160;</td>
          <td class="paramname"><em>shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t *&#160;</td>
          <td class="paramname"><em>shapeLength</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Obtains the dimension information about the output tensor. </p>
<p>After <a class="el" href="group___neural_nework_runtime.html#ga716af07e8996677f1364a88c7ae30908">OH_NNExecutor_Run</a> is called to complete a single inference, call this method to obtain the specified output dimension information and number of dimensions. It is commonly used in dynamic shape input and output scenarios.<br />
 If the <b>outputIndex</b> is greater than or equal to the output tensor number, this method will return error code. The output tensor number can be got by <a class="el" href="group___neural_nework_runtime.html#gaf679893bb2413ce72229b310c91e3663">OH_NNExecutor_GetOutputCount</a>.<br />
 As an output parameter, <b>*shape</b> must be a null pointer, otherwise the method will return an error code. Fou example, you should define int32_t* tensorShape = NULL, and pass &amp;tensorShape as the argument of <b>shape</b>.<br />
 You do not need to release the memory of <b>shape</b>. It will be released when <b>executor</b> is destroied.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">outputIndex</td><td>Output Index value, which is in the same sequence of the data output when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. Assume that <b>outputIndices</b> is <b>{4, 6, 8}</b> when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. When <a class="el" href="group___neural_nework_runtime.html#gadcc8c5d6bec42ffb06d2ce3b6698207d">OH_NNExecutor_GetOutputShape</a> is called to obtain dimension information about the output tensor, <b>outputIndices</b> is <b>{0, 1, 2}</b>. </td></tr>
    <tr><td class="paramname">shape</td><td>Pointer to the int32_t array. The value of each element in the array is the length of the output tensor in each dimension. </td></tr>
    <tr><td class="paramname">shapeLength</td><td>Pointer to the uint32_t type. The number of output dimensions is returned. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get tensor output shape successfully. The return value is saved in <b>shape</b> and <b>shapeLength</b>.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get tensor output shape. The possible reason for failure is that the <b>executor</b>, <b>shape</b> or <b>shapeLength</b> is nullptr, or <b>*shape</b> is not nullptr, or <b>outputIndex</b> is out of range.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga716af07e8996677f1364a88c7ae30908"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga716af07e8996677f1364a88c7ae30908">&#9670;&nbsp;</a></span>OH_NNExecutor_Run()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_Run </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Performs inference. </p>
<p>Performs end-to-end inference and computing of the model on the device associated with the executor.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> run model successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to run model. The possible reason for failure is that the <b>executor</b> is nullptr.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae480781f8f23d040a2c8aad3c170783e">OH_NN_FAILED</a> fail to set model output. The possible reason for failure is that the backend device failed to run model.<br />
</dd></dl>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000097">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#ga77aca3467cfda5a05042be320d34e90c">OH_NNExecutor_RunSync</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga199e78f7de09248707a4aaa3bf76d07a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga199e78f7de09248707a4aaa3bf76d07a">&#9670;&nbsp;</a></span>OH_NNExecutor_RunAsync()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_RunAsync </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *&#160;</td>
          <td class="paramname"><em>inputTensor</em>[], </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>inputCount</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *&#160;</td>
          <td class="paramname"><em>outputTensor</em>[], </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>outputCount</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int32_t&#160;</td>
          <td class="paramname"><em>timeout</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>userData</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Asynchronous execution of the model inference. </p>
<p>Input and output tensors should be created first by <a class="el" href="group___neural_nework_runtime.html#ga724c81fe4a2e48021fd545b3c072ba50">OH_NNTensor_Create</a>, <a class="el" href="group___neural_nework_runtime.html#gad658b50239b119baece25c7470ccf403">OH_NNTensor_CreateWithSize</a> or <a class="el" href="group___neural_nework_runtime.html#gafd7fac565ba766422257e1525dc416ce">OH_NNTensor_CreateWithFd</a>. And then the input tensors data which is got by <a class="el" href="group___neural_nework_runtime.html#ga8ac1491aa7df59382a46ffceb7302638">OH_NNTensor_GetDataBuffer</a> must be filled. The executor will yield out the results by inference execution and fill them into output tensors data for you to read.<br />
 In the case of dynamic shape, you can get the real output shape directly by <a class="el" href="group___neural_nework_runtime.html#gadcc8c5d6bec42ffb06d2ce3b6698207d">OH_NNExecutor_GetOutputShape</a>, or you can create a tensor descriptor from an output tensor by <a class="el" href="group___neural_nework_runtime.html#gaaeb03b1a5cd44ba29f71b5814acb73b2">OH_NNTensor_GetTensorDesc</a>, and then read its real shape by <a class="el" href="group___neural_nework_runtime.html#gae2c7ad5cf0133ec216d66d168aedc10f">OH_NNTensorDesc_GetShape</a>.<br />
 The method is non-blocked and will return immediately.<br />
 The callback function handles are set by <a class="el" href="group___neural_nework_runtime.html#ga2a5f79e766f1c4a7eab4dcbd8d582743">OH_NNExecutor_SetOnRunDone</a> and <a class="el" href="group___neural_nework_runtime.html#gaf2fbdb20804b7f4839abc838413bc241">OH_NNExecutor_SetOnServiceDied</a>. The inference results and error code can be got by <a class="el" href="group___neural_nework_runtime.html#ga0686487b4c80b6bd0d686fa1c08b4c8c">NN_OnRunDone</a>. And you can deal with the abnormal termination of device driver service during asynchronous execution by <a class="el" href="group___neural_nework_runtime.html#ga965dabbf39f75570925be38b8d0c8f76">NN_OnServiceDied</a>.<br />
 If the execution time reaches the <b>timeout</b>, the execution will be terminated with no outputs, and the <b>errCode<b> returned in callback function <a class="el" href="group___neural_nework_runtime.html#ga0686487b4c80b6bd0d686fa1c08b4c8c">NN_OnRunDone</a> will be <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a19543b3420787458575cc04c9c582731">OH_NN_TIMEOUT</a>.<br />
 The <b>userData</b> is asynchronous execution identifier and will be returned as the first parameter of the callback function. You can input any value you want as long as it can identify different asynchronous executions.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">inputTensor</td><td>An array of input tensors <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. </td></tr>
    <tr><td class="paramname">inputCount</td><td>Number of input tensors. </td></tr>
    <tr><td class="paramname">outputTensor</td><td>An array of output tensors <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. </td></tr>
    <tr><td class="paramname">outputCount</td><td>Number of output tensors. </td></tr>
    <tr><td class="paramname">timeout</td><td>Time limit (millisecond) of the asynchronous execution, e.g. 1000. </td></tr>
    <tr><td class="paramname">userData</td><td>Asynchronous execution identifier. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> run successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to run. The possible reason for failure is that the <b>executor</b>, <b>inputTensor</b>, <b>outputTensor</b> or <b>userData</b> is nullptr, or <b>inputCount</b> or <b>outputCount</b> is 0.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae480781f8f23d040a2c8aad3c170783e">OH_NN_FAILED</a> the backend device failed to run.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ac086f7e028ece986f71cbcd0a8fa91e9">OH_NN_NULL_PTR</a> the parameters of input or output tensor is invalid.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> the backend device is not supported to run async.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>
<p></b></b></p>

</div>
</div>
<a id="ga77aca3467cfda5a05042be320d34e90c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga77aca3467cfda5a05042be320d34e90c">&#9670;&nbsp;</a></span>OH_NNExecutor_RunSync()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_RunSync </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *&#160;</td>
          <td class="paramname"><em>inputTensor</em>[], </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>inputCount</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *&#160;</td>
          <td class="paramname"><em>outputTensor</em>[], </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>outputCount</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Synchronous execution of the model inference. </p>
<p>Input and output tensors should be created first by <a class="el" href="group___neural_nework_runtime.html#ga724c81fe4a2e48021fd545b3c072ba50">OH_NNTensor_Create</a>, <a class="el" href="group___neural_nework_runtime.html#gad658b50239b119baece25c7470ccf403">OH_NNTensor_CreateWithSize</a> or <a class="el" href="group___neural_nework_runtime.html#gafd7fac565ba766422257e1525dc416ce">OH_NNTensor_CreateWithFd</a>. And then the input tensors data which is got by <a class="el" href="group___neural_nework_runtime.html#ga8ac1491aa7df59382a46ffceb7302638">OH_NNTensor_GetDataBuffer</a> must be filled. The executor will then yield out the results by inference execution and fill them into output tensors data for you to read. <br />
 In the case of dynamic shape, you can get the real output shape directly by <a class="el" href="group___neural_nework_runtime.html#gadcc8c5d6bec42ffb06d2ce3b6698207d">OH_NNExecutor_GetOutputShape</a>, or you can create a tensor descriptor from an output tensor by <a class="el" href="group___neural_nework_runtime.html#gaaeb03b1a5cd44ba29f71b5814acb73b2">OH_NNTensor_GetTensorDesc</a>, and then read its real shape by <a class="el" href="group___neural_nework_runtime.html#gae2c7ad5cf0133ec216d66d168aedc10f">OH_NNTensorDesc_GetShape</a>. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">inputTensor</td><td>An array of input tensors <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. </td></tr>
    <tr><td class="paramname">inputCount</td><td>Number of input tensors. </td></tr>
    <tr><td class="paramname">outputTensor</td><td>An array of output tensors <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. </td></tr>
    <tr><td class="paramname">outputCount</td><td>Number of output tensors. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> run successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to run. The possible reason for failure is that the <b>executor</b>, <b>inputTensor</b> or <b>outputTensor</b> is nullptr, or <b>inputCount</b> or <b>outputCount</b> is 0.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae480781f8f23d040a2c8aad3c170783e">OH_NN_FAILED</a> the backend device failed to run.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ac086f7e028ece986f71cbcd0a8fa91e9">OH_NN_NULL_PTR</a> the parameters of input or output tensor is invalid.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga3e71e749f923dbf8578c6f9bcb2aac86"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga3e71e749f923dbf8578c6f9bcb2aac86">&#9670;&nbsp;</a></span>OH_NNExecutor_SetInput()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_SetInput </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>inputIndex</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a> *&#160;</td>
          <td class="paramname"><em>tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const void *&#160;</td>
          <td class="paramname"><em>dataBuffer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>length</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the single input data for a model. </p>
<p>This method copies the data whose length is specified by <b>length</b> (in bytes) in <b>dataBuffer</b> to the shared memory of the underlying device. <b>inputIndex</b> specifies the input to be set and <b>tensor</b> sets information such as the input shape, type, and quantization parameters.<br />
 Neural Network Runtime supports models with dynamical shape input. For fixed shape input and dynamic shape input scenarios, this method uses different processing policies.<br />
</p><ul>
<li>Fixed shape input: The attributes of <b>tensor</b> must be the same as those of the tensor added by calling <a class="el" href="group___neural_nework_runtime.html#gaca9b631aeec257142ed7acd9a66afad4">OH_NNModel_AddTensor</a> in the composition phase.</li>
<li>Dynamic shape input: In the composition phase, because the shape is not fixed, each value in <b>tensor.dimensions</b> must be greater than <b>0</b> in the method calls to determine the shape input in the calculation phase. When setting the shape, you can modify only the dimension whose value is <b>-1</b>. Assume that <b>[-1, 224, 224, 3]</b> is input as the the dimension of A in the composition phase. When this method is called, only the size of the first dimension can be modified, e.g. to <b>[3, 224, 224, 3]</b>. If other dimensions are adjusted, <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> is returned.<br />
 <dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">inputIndex</td><td>Input index value, which is in the same sequence of the data input when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. Assume that the value of <b>inputIndices</b> is <b>{1, 5, 9}</b> when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. In input settings, the index value for the three inputs is <b>{0, 1, 2}</b>.<br />
</td></tr>
    <tr><td class="paramname">tensor</td><td>Sets the tensor corresponding to the input data. </td></tr>
    <tr><td class="paramname">dataBuffer</td><td>Pointer to the input data. </td></tr>
    <tr><td class="paramname">length</td><td>Length of the data buffer, in bytes. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set model input successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set model input. The possible reason for failure is that the <b>executor</b>, <b>tensor</b> or <b>dataBuffer</b> is nullptr, or <b>inputIndex</b> is out of range, or <b>length</b> is 0.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a91a3c5cc6c688cc3feaace95d199e249">OH_NN_MEMORY_ERROR</a> fail to set model input. The possible reason for failure is that the memory error occurred such as failure to create an object.<br />
</dd></dl>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000095">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#ga77aca3467cfda5a05042be320d34e90c">OH_NNExecutor_RunSync</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>
</li>
</ul>

</div>
</div>
<a id="gab98aa06c8215d47be190254cc8682236"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gab98aa06c8215d47be190254cc8682236">&#9670;&nbsp;</a></span>OH_NNExecutor_SetInputWithMemory()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_SetInputWithMemory </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>inputIndex</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a> *&#160;</td>
          <td class="paramname"><em>tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> *&#160;</td>
          <td class="paramname"><em>memory</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Specifies the hardware shared memory pointed to by the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance as the shared memory used by a single input. </p>
<p>In scenarios where memory needs to be managed by yourself, this method binds the execution input to the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> memory instance. During computing, the underlying device reads the input data from the shared memory pointed to by the memory instance. By using this method, concurrent execution of input setting, computing, and read can be implemented to improve inference efficiency of a data flow.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">inputIndex</td><td>Input index value, which is in the same sequence of the data input when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. Assume that the value of <b>inputIndices</b> is <b>{1, 5, 9}</b> when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. When the input shared memory is specified, the index value for the three inputs is <b>{0, 1, 2}</b>. </td></tr>
    <tr><td class="paramname">tensor</td><td>Pointer to <a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a>, used to set the tensor corresponding to a single input. </td></tr>
    <tr><td class="paramname">memory</td><td>Pointer to <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set input with memory successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set input with memory. The possible reason for failure is that the <b>executor</b>, <b>tensor</b> or <b>memory</b> is nullptr, or <b>inputIndex</b> is out of range, or memory length is less than tensor length.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a91a3c5cc6c688cc3feaace95d199e249">OH_NN_MEMORY_ERROR</a> fail to set input with memory. The possible reason for failure is that the memory error occurred such as failure to create an object.<br />
</dd></dl>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000102">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#ga77aca3467cfda5a05042be320d34e90c">OH_NNExecutor_RunSync</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga2a5f79e766f1c4a7eab4dcbd8d582743"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2a5f79e766f1c4a7eab4dcbd8d582743">&#9670;&nbsp;</a></span>OH_NNExecutor_SetOnRunDone()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_SetOnRunDone </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga0686487b4c80b6bd0d686fa1c08b4c8c">NN_OnRunDone</a>&#160;</td>
          <td class="paramname"><em>onRunDone</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the callback function handle for the post-process when the asynchronous execution has been done. </p>
<p>The definition fo the callback function: <a class="el" href="group___neural_nework_runtime.html#ga0686487b4c80b6bd0d686fa1c08b4c8c">NN_OnRunDone</a>. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">onRunDone</td><td>Callback function handle <a class="el" href="group___neural_nework_runtime.html#ga0686487b4c80b6bd0d686fa1c08b4c8c">NN_OnRunDone</a>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set on run done successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set on run done. The possible reason for failure is that the <b>executor</b> or <b>onRunDone</b> is nullptr.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> the backend device is not supported to set on run done.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gaf2fbdb20804b7f4839abc838413bc241"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaf2fbdb20804b7f4839abc838413bc241">&#9670;&nbsp;</a></span>OH_NNExecutor_SetOnServiceDied()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_SetOnServiceDied </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga965dabbf39f75570925be38b8d0c8f76">NN_OnServiceDied</a>&#160;</td>
          <td class="paramname"><em>onServiceDied</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the callback function handle for the post-process when the device driver service is dead during asynchronous execution. </p>
<p>The definition fo the callback function: <a class="el" href="group___neural_nework_runtime.html#ga965dabbf39f75570925be38b8d0c8f76">NN_OnServiceDied</a>. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">onServiceDied</td><td>Callback function handle <a class="el" href="group___neural_nework_runtime.html#ga965dabbf39f75570925be38b8d0c8f76">NN_OnServiceDied</a>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set on service died successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set on service died. The possible reason for failure is that the <b>executor</b> or <b>onServiceDied</b> is nullptr.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> the backend device is not supported to set on service died.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gac066212acc73c1522032fcb04a4f06df"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gac066212acc73c1522032fcb04a4f06df">&#9670;&nbsp;</a></span>OH_NNExecutor_SetOutput()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_SetOutput </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>outputIndex</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *&#160;</td>
          <td class="paramname"><em>dataBuffer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>length</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the buffer for a single output of a model. </p>
<p>This method binds the buffer to which <b>dataBuffer</b> points to the output specified by <b>outputIndex</b>. The length of the buffer is specified by <b>length</b>.<br />
 After <a class="el" href="group___neural_nework_runtime.html#ga716af07e8996677f1364a88c7ae30908">OH_NNExecutor_Run</a> is called to complete a single model inference, Neural Network Runtime compares the length of the buffer to which <b>dataBuffer</b> points with the length of the output data and returns different results based on the actual situation.<br />
</p><ul>
<li>If the buffer length is greater than or equal to the data length, the inference result is copied to the buffer and <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> is returned. You can read the inference result from <b>dataBuffer</b>.</li>
<li>If the buffer length is smaller than the data length, <a class="el" href="group___neural_nework_runtime.html#ga716af07e8996677f1364a88c7ae30908">OH_NNExecutor_Run</a> returns <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> and generates a log indicating that the buffer is too small.<br />
 <dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> instance. </td></tr>
    <tr><td class="paramname">outputIndex</td><td>Output Index value, which is in the same sequence of the data output when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. Assume that the value of <b>outputIndices</b> is <b>{4, 6, 8}</b> when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. In output buffer settings, the index value for the three outputs is <b>{0, 1, 2}</b>. </td></tr>
    <tr><td class="paramname">dataBuffer</td><td>Pointer to the output data. </td></tr>
    <tr><td class="paramname">length</td><td>Length of the data buffer, in bytes. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set model output successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set model output. The possible reason for failure is that the <b>executor</b>, <b>tensor</b> or <b>dataBuffer</b> is nullptr, or <b>outputIndex</b> is out of range, or <b>length</b> is 0.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a91a3c5cc6c688cc3feaace95d199e249">OH_NN_MEMORY_ERROR</a> fail to set model output. The possible reason for failure is that the memory error occurred such as failure to create an object.<br />
</dd></dl>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000096">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#ga77aca3467cfda5a05042be320d34e90c">OH_NNExecutor_RunSync</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>
</li>
</ul>

</div>
</div>
<a id="ga98e76f299d78cf688c8fc515dbab1c33"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga98e76f299d78cf688c8fc515dbab1c33">&#9670;&nbsp;</a></span>OH_NNExecutor_SetOutputWithMemory()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNExecutor_SetOutputWithMemory </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga93fd7e632b29ec5174a09e138478ce78">OH_NNExecutor</a> *&#160;</td>
          <td class="paramname"><em>executor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>outputIndex</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> *&#160;</td>
          <td class="paramname"><em>memory</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Specifies the hardware shared memory pointed to by the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> instance as the shared memory used by a single output. </p>
<p>In scenarios where memory needs to be managed by yourself, this method binds the execution output to the <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a> memory instance. When computing is performed, the underlying hardware directly writes the computing result to the shared memory to which the memory instance points. By using this method, concurrent execution of input setting, computing, and read can be implemented to improve inference efficiency of a data flow.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">executor</td><td>Executor. </td></tr>
    <tr><td class="paramname">outputIndex</td><td>Output Index value, which is in the same sequence of the data output when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. Assume that the value of <b>outputIndices</b> is <b>{4, 6, 8}</b> when <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> is called. When the output shared memory is specified, the index value for the three outputs is <b>{0, 1, 2}</b>. </td></tr>
    <tr><td class="paramname">memory</td><td>Pointer to <a class="el" href="struct_o_h___n_n___memory.html">OH_NN_Memory</a>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set output with memory successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set output with memory. The possible reason for failure is that the <b>executor</b>, <b>tensor</b> or <b>memory</b> is nullptr, or <b>outputIndex</b> is out of range, or memory length is less than tensor length.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a91a3c5cc6c688cc3feaace95d199e249">OH_NN_MEMORY_ERROR</a> fail to set output with memory. The possible reason for failure is that the memory error occurred such as failure to create an object.<br />
</dd></dl>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000103">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#ga77aca3467cfda5a05042be320d34e90c">OH_NNExecutor_RunSync</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga665a2d23d91063d7e7de63723db44a20"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga665a2d23d91063d7e7de63723db44a20">&#9670;&nbsp;</a></span>OH_NNModel_AddOperation()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNModel_AddOperation </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#gab333095762fd39a9f952523d717b95de">OH_NN_OperationType</a>&#160;</td>
          <td class="paramname"><em>op</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a> *&#160;</td>
          <td class="paramname"><em>paramIndices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a> *&#160;</td>
          <td class="paramname"><em>inputIndices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a> *&#160;</td>
          <td class="paramname"><em>outputIndices</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Adds an operator to a model instance. </p>
<p>This method is used to add an operator to a model instance. The operator type is specified by <b>op</b>, and the operator parameters, inputs, and outputs are specified by <b>paramIndices</b>, <b>inputIndices</b>, and <b>outputIndices</b> respectively. This method verifies the attributes of operator parameters and the number of input and output parameters. These attributes must be correctly set when <a class="el" href="group___neural_nework_runtime.html#ga32d5660e09c42a92636c7c97bfa35c7d">OH_NNModel_AddTensorToModel</a> is called to add tensors. For details about the expected parameters, input attributes, and output attributes of each operator, see <a class="el" href="group___neural_nework_runtime.html#gab333095762fd39a9f952523d717b95de">OH_NN_OperationType</a>. <br />
 <b>paramIndices</b>, <b>inputIndices</b>, and <b>outputIndices</b> store the indices of tensors. The indices are determined by the order in which tensors are added to the model. For details about how to add a tensor, see <a class="el" href="group___neural_nework_runtime.html#ga32d5660e09c42a92636c7c97bfa35c7d">OH_NNModel_AddTensorToModel</a>. <br />
 If unnecessary parameters are added when adding an operator, this method returns <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a>. If no operator parameter is set, the operator uses the default parameter value. For details about the default values, see <a class="el" href="group___neural_nework_runtime.html#gab333095762fd39a9f952523d717b95de">OH_NN_OperationType</a>. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">model</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance. </td></tr>
    <tr><td class="paramname">op</td><td>Specifies the type of an operator to be added. For details, see the enumerated values of <a class="el" href="group___neural_nework_runtime.html#gab333095762fd39a9f952523d717b95de">OH_NN_OperationType</a>. </td></tr>
    <tr><td class="paramname">paramIndices</td><td>Pointer to the <b><a class="el" href="struct_o_h___n_n___u_int32_array.html" title="This structure is used to store a 32-bit unsigned integer array. ">OH_NN_UInt32Array</a></b> instance, which is used to set operator parameters. </td></tr>
    <tr><td class="paramname">inputIndices</td><td>Pointer to the <b><a class="el" href="struct_o_h___n_n___u_int32_array.html" title="This structure is used to store a 32-bit unsigned integer array. ">OH_NN_UInt32Array</a></b> instance, which is used to set the operator input. </td></tr>
    <tr><td class="paramname">outputIndices</td><td>Pointer to the <b><a class="el" href="struct_o_h___n_n___u_int32_array.html" title="This structure is used to store a 32-bit unsigned integer array. ">OH_NN_UInt32Array</a></b> instance, which is used to set the operator output. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> add operation to model successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to add operation to model. The possible reason for failure is that the <b>model</b>, <b>paramIndices</b>, <b>inputIndices</b> or <b>outputIndices</b> is nullptr, or parameters are invalid.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> fail to add operation to model. The possible reason for failure is that the model is invalid.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gaca9b631aeec257142ed7acd9a66afad4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaca9b631aeec257142ed7acd9a66afad4">&#9670;&nbsp;</a></span>OH_NNModel_AddTensor()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNModel_AddTensor </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a> *&#160;</td>
          <td class="paramname"><em>tensor</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Adds a tensor to a model instance. </p>
<p>The data node and operator parameters in the Neural Network Runtime model are composed of tensors of the model. This method is used to add tensors to a model instance based on the <b>tensor</b> parameter. The sequence of adding tensors is specified by the index value recorded in the model. The <a class="el" href="group___neural_nework_runtime.html#ga3cb6d4ba6e56798e47794127f77b31a8">OH_NNModel_SetTensorData</a>, <a class="el" href="group___neural_nework_runtime.html#ga665a2d23d91063d7e7de63723db44a20">OH_NNModel_AddOperation</a>, and <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> methods specifies tensors based on the index value.<br />
 Neural Network Runtime supports inputs and outputs of the dynamic shape. When adding a data node with a dynamic shape, you need to set the dimensions that support dynamic changes in <b>tensor.dimensions</b> to <b>-1</b>. For example, if <b>tensor.dimensions</b> of a four-dimensional tensor is set to <b>[1, -1, 2, 2]</b>, the second dimension supports dynamic changes.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">model</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance. </td></tr>
    <tr><td class="paramname">tensor</td><td>Pointer to the <a class="el" href="struct_o_h___n_n___tensor.html">OH_NN_Tensor</a> tensor. The tensor specifies the attributes of the tensor added to the model instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> add tensor to model successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to add tensor to model. The possible reason for failure is that the <b>model</b> or <b>tensor</b> is nullptr.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> fail to add tensor to model. The possible reason for failure is that the model is invalid.<br />
</dd></dl>
<dl class="deprecated"><dt><b><a class="el" href="deprecated.html#_deprecated000094">Deprecated:</a></b></dt><dd>since 11  <a class="el" href="group___neural_nework_runtime.html#ga32d5660e09c42a92636c7c97bfa35c7d">OH_NNModel_AddTensorToModel</a> </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga32d5660e09c42a92636c7c97bfa35c7d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga32d5660e09c42a92636c7c97bfa35c7d">&#9670;&nbsp;</a></span>OH_NNModel_AddTensorToModel()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNModel_AddTensorToModel </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Adds a tensor to the model instance. </p>
<p>The data node and operator parameters in the Neural Network Runtime model are composed of tensors of the model. This method is used to add tensors to a model instance based on the <b>tensorDesc</b> parameter with type of <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> contains some attributes such as shape, format, data type and provides corresponding APIs to access them. The order of adding tensors is specified by the indices recorded in the model. The <a class="el" href="group___neural_nework_runtime.html#ga3cb6d4ba6e56798e47794127f77b31a8">OH_NNModel_SetTensorData</a>, <a class="el" href="group___neural_nework_runtime.html#ga665a2d23d91063d7e7de63723db44a20">OH_NNModel_AddOperation</a>, and <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> methods specify tensors based on the indices. <br />
 Neural Network Runtime supports inputs and outputs of the dynamic shape. When adding a data node with a dynamic shape, you need to set the dimensions that support dynamic changes to <b>-1</b>. For example, if the shape of a four-dimensional tensor is set to <b>[1, -1, 2, 2]</b>, the second dimension supports dynamic changes. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">model</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance. </td></tr>
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. The tensor descriptor specifies the attributes of the tensor added to the model instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> add tensor to model successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to add tensor to model. The possible reason for failure is that the <b>model</b> or <b>tensorDesc</b> is nullptr.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a91a3c5cc6c688cc3feaace95d199e249">OH_NN_MEMORY_ERROR</a> fail to add tensor to model. The possible reason for failure is that the memory error occurred such as failure to create an object.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga65e3b60e38110bd0aa951f3fa6618262"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga65e3b60e38110bd0aa951f3fa6618262">&#9670;&nbsp;</a></span>OH_NNModel_Construct()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a>* OH_NNModel_Construct </td>
          <td>(</td>
          <td class="paramtype">void&#160;</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates a model instance of the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> type and uses other APIs provided by OH_NNModel to construct the model instance. </p>
<p>Before composition, call <a class="el" href="group___neural_nework_runtime.html#ga65e3b60e38110bd0aa951f3fa6618262">OH_NNModel_Construct</a> to create a model instance. Based on the model topology, call the <a class="el" href="group___neural_nework_runtime.html#ga32d5660e09c42a92636c7c97bfa35c7d">OH_NNModel_AddTensorToModel</a>, <a class="el" href="group___neural_nework_runtime.html#ga665a2d23d91063d7e7de63723db44a20">OH_NNModel_AddOperation</a>, and <a class="el" href="group___neural_nework_runtime.html#ga3cb6d4ba6e56798e47794127f77b31a8">OH_NNModel_SetTensorData</a> methods to fill in the data and operator nodes of the model, and then call <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> to specify the inputs and outputs of the model. After the model topology is constructed, call <a class="el" href="group___neural_nework_runtime.html#ga39888a40afb57fc807c11d6b880e8442">OH_NNModel_Finish</a> to build the model. <br />
 After a model instance is no longer used, you need to destroy it by calling <a class="el" href="group___neural_nework_runtime.html#ga6a56b11554f4eb977c9d7fd831c89020">OH_NNModel_Destroy</a> to avoid memory leak. <br />
 </p><dl class="section return"><dt>Returns</dt><dd>Pointer to a <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance, or NULL if it fails to create. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga6a56b11554f4eb977c9d7fd831c89020"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga6a56b11554f4eb977c9d7fd831c89020">&#9670;&nbsp;</a></span>OH_NNModel_Destroy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void OH_NNModel_Destroy </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> **&#160;</td>
          <td class="paramname"><em>model</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Releases a model instance. </p>
<p>This method needs to be called to release the model instance created by calling <a class="el" href="group___neural_nework_runtime.html#ga65e3b60e38110bd0aa951f3fa6618262">OH_NNModel_Construct</a>. Otherwise, memory leak will occur. <br />
 If <b>model</b> or <b>*model</b> is a null pointer, this method only prints warning logs and does not execute the release. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">model</td><td>Double pointer to the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance. After a model instance is destroyed, this method sets <b>*model</b> to a null pointer. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga39888a40afb57fc807c11d6b880e8442"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga39888a40afb57fc807c11d6b880e8442">&#9670;&nbsp;</a></span>OH_NNModel_Finish()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNModel_Finish </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *&#160;</td>
          <td class="paramname"><em>model</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Completes model composition. </p>
<p>After the model topology is set up, call this method to indicate that the composition is complete. After this method is called, additional composition operations cannot be performed. If <a class="el" href="group___neural_nework_runtime.html#ga32d5660e09c42a92636c7c97bfa35c7d">OH_NNModel_AddTensorToModel</a>, <a class="el" href="group___neural_nework_runtime.html#ga665a2d23d91063d7e7de63723db44a20">OH_NNModel_AddOperation</a>, <a class="el" href="group___neural_nework_runtime.html#ga3cb6d4ba6e56798e47794127f77b31a8">OH_NNModel_SetTensorData</a>, and <a class="el" href="group___neural_nework_runtime.html#ga57188ebd6bbbbfd127bf96af994a4310">OH_NNModel_SpecifyInputsAndOutputs</a> are called, <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> is returned. <br />
 Before calling <a class="el" href="group___neural_nework_runtime.html#ga9d4bdd9a5e7689b7778a927865389302">OH_NNModel_GetAvailableOperations</a> and <a class="el" href="group___neural_nework_runtime.html#ga642ce605311075cd5d5cc5b527fee3b6">OH_NNCompilation_Construct</a>, you must call this method to complete composition. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">model</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> the composition is complete successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> composition failed. The possible reason for failure is that the <b>model</b> is nullptr, or parameters set before are invalid.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> composition failed. The possible reason for failure is that the model is invalid.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a91a3c5cc6c688cc3feaace95d199e249">OH_NN_MEMORY_ERROR</a> composition failed. The possible reason for failure is that the memory error occurred such as failure to create an object.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga9d4bdd9a5e7689b7778a927865389302"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga9d4bdd9a5e7689b7778a927865389302">&#9670;&nbsp;</a></span>OH_NNModel_GetAvailableOperations()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNModel_GetAvailableOperations </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>deviceID</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const bool **&#160;</td>
          <td class="paramname"><em>isSupported</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t *&#160;</td>
          <td class="paramname"><em>opCount</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Queries whether the device supports operators in the model. The support status is indicated by the Boolean value. </p>
<p>Queries whether underlying device supports operators in a model instance. The device is specified by <b>deviceID</b>, and the result is represented by the array pointed by <b>isSupported</b>. If the <em>i</em>th operator is supported, the value of <b>(*isSupported)</b>[<em>i</em>] is <b>true</b>. Otherwise, the value is <b>false</b>. <br />
 After this method is successfully executed, <b>(*isSupported)</b> points to the bool array that records the operator support status. The operator quantity for the array length is the same as that for the model instance. The memory corresponding to this array is managed by Neural Network Runtime and is automatically destroyed after the model instance is destroyed or this method is called again. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">model</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance. </td></tr>
    <tr><td class="paramname">deviceID</td><td>Device ID to be queried, which can be obtained by using <a class="el" href="group___neural_nework_runtime.html#ga66c11e4935abaa6710a1d8eb4d09395f">OH_NNDevice_GetAllDevicesID</a>. </td></tr>
    <tr><td class="paramname">isSupported</td><td>Pointer to the bool array. When this method is called, <b>(*isSupported)</b> must be a null pointer. Otherwise, <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> is returned. </td></tr>
    <tr><td class="paramname">opCount</td><td>Number of operators in a model instance, corresponding to the length of the <b>(*isSupported)</b> array. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get available operations successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get available operations. The possible reason for failure is that the <b>model</b>, <b>isSupported</b> or <b>opCount</b> is nullptr, or <b>*isSupported</b> is not nullptr.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> fail to get available operations. The possible reason for failure is that the model is invalid.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823ae480781f8f23d040a2c8aad3c170783e">OH_NN_FAILED</a> fail to get available operations. The possible reason for failure is that the <b>deviceID</b> is invalid.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga3cb6d4ba6e56798e47794127f77b31a8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga3cb6d4ba6e56798e47794127f77b31a8">&#9670;&nbsp;</a></span>OH_NNModel_SetTensorData()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNModel_SetTensorData </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const void *&#160;</td>
          <td class="paramname"><em>dataBuffer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>length</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the tensor value. </p>
<p>For tensors with constant values (such as model weights), you need to use this method to set their data. The index of a tensor is determined by the order in which the tensor is added to the model. For details about how to add a tensor, see <a class="el" href="group___neural_nework_runtime.html#ga32d5660e09c42a92636c7c97bfa35c7d">OH_NNModel_AddTensorToModel</a>. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">model</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance. </td></tr>
    <tr><td class="paramname">index</td><td>Index of a tensor. </td></tr>
    <tr><td class="paramname">dataBuffer</td><td>Pointer to real data. </td></tr>
    <tr><td class="paramname">length</td><td>Length of the data buffer. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set tensor data successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set tensor data. The possible reason for failure is that the <b>model</b> or <b>dataBuffer</b> is nullptr, or <b>length</b> is 0, or <b>index</b> is out of range.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> fail to set tensor data. The possible reason for failure is that the model is invalid.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gab9a0bf7d814e23bb400c8f1ed05bff18"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gab9a0bf7d814e23bb400c8f1ed05bff18">&#9670;&nbsp;</a></span>OH_NNModel_SetTensorQuantParams()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNModel_SetTensorQuantParams </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> *&#160;</td>
          <td class="paramname"><em>quantParam</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the quantization parameter of a tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">model</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance. </td></tr>
    <tr><td class="paramname">index</td><td>Index of a tensor. </td></tr>
    <tr><td class="paramname">quantParam</td><td>Pointer to the quantization parameter instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set tensor quant parameters successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set tensor quant parameters. The possible reason for failure is that the <b>model</b> or <b>quantParam</b> is nullptr, or <b>index</b> is out of range.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> fail to set tensor quant parameters. The possible reason for failure is that the model is invalid.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga2b62ce6e33d810a38aa1137e70c9c274"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2b62ce6e33d810a38aa1137e70c9c274">&#9670;&nbsp;</a></span>OH_NNModel_SetTensorType()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNModel_SetTensorType </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint32_t&#160;</td>
          <td class="paramname"><em>index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga09f35be6e0f86f5d747192ce5812552f">OH_NN_TensorType</a>&#160;</td>
          <td class="paramname"><em>tensorType</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the tensor type. See <a class="el" href="group___neural_nework_runtime.html#ga09f35be6e0f86f5d747192ce5812552f">OH_NN_TensorType</a> for details. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">model</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance. </td></tr>
    <tr><td class="paramname">index</td><td>Index of a tensor. </td></tr>
    <tr><td class="paramname">tensorType</td><td>Tensor type of <a class="el" href="group___neural_nework_runtime.html#ga09f35be6e0f86f5d747192ce5812552f">OH_NN_TensorType</a>. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set tensor type successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set tensor type. The possible reason for failure is that the <b>model</b> is nullptr, or <b>index</b> is out of range, or <b>tensorType</b> is invalid.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> fail to set tensor type. The possible reason for failure is that the model is invalid.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga57188ebd6bbbbfd127bf96af994a4310"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga57188ebd6bbbbfd127bf96af994a4310">&#9670;&nbsp;</a></span>OH_NNModel_SpecifyInputsAndOutputs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNModel_SpecifyInputsAndOutputs </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> *&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a> *&#160;</td>
          <td class="paramname"><em>inputIndices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="struct_o_h___n_n___u_int32_array.html">OH_NN_UInt32Array</a> *&#160;</td>
          <td class="paramname"><em>outputIndices</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Specifies the inputs and outputs of a model. </p>
<p>A tensor must be specified as the end-to-end inputs and outputs of a model instance. This type of tensor cannot be set using <a class="el" href="group___neural_nework_runtime.html#ga3cb6d4ba6e56798e47794127f77b31a8">OH_NNModel_SetTensorData</a>. <br />
 The index of a tensor is determined by the order in which the tensor is added to the model. For details about how to add a tensor, see <a class="el" href="group___neural_nework_runtime.html#ga32d5660e09c42a92636c7c97bfa35c7d">OH_NNModel_AddTensorToModel</a>. <br />
 Currently, the model inputs and outputs cannot be set asynchronously. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">model</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga4169d4a60707eae47f924a19a77db0a1">OH_NNModel</a> instance. </td></tr>
    <tr><td class="paramname">inputIndices</td><td>Pointer to the <b><a class="el" href="struct_o_h___n_n___u_int32_array.html" title="This structure is used to store a 32-bit unsigned integer array. ">OH_NN_UInt32Array</a></b> instance, which is used to set the operator input. </td></tr>
    <tr><td class="paramname">outputIndices</td><td>Pointer to the <b><a class="el" href="struct_o_h___n_n___u_int32_array.html" title="This structure is used to store a 32-bit unsigned integer array. ">OH_NN_UInt32Array</a></b> instance, which is used to set the operator output. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> specify inputs and outputs successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to specify inputs and outputs. The possible reason for failure is that the <b>model</b>, <b>inputIndices</b> or <b>outputIndices</b> is nullptr, or parameters are invalid.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823afb4a19c226c8f6250cbc0628b1dd40c4">OH_NN_OPERATION_FORBIDDEN</a> fail to specify inputs and outputs. The possible reason for failure is that the model is invalid.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>9 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga5e4fff1b517ad1451f0106983d04491b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga5e4fff1b517ad1451f0106983d04491b">&#9670;&nbsp;</a></span>OH_NNQuantParam_Create()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a>* OH_NNQuantParam_Create </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates a <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance. </p>
<p>After the <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance is created, call <a class="el" href="group___neural_nework_runtime.html#gaff24853638b0aa423fcd2724e3dfaaf1">OH_NNQuantParam_SetScales</a>, <a class="el" href="group___neural_nework_runtime.html#gac6c284fb1577981a5e9da90cedc65b86">OH_NNQuantParam_SetZeroPoints</a>, <a class="el" href="group___neural_nework_runtime.html#gad7ddfbe997bb70b6b5fb1d490dbeb8d8">OH_NNQuantParam_SetNumBits</a> to set its attributes, and then call <a class="el" href="group___neural_nework_runtime.html#gab9a0bf7d814e23bb400c8f1ed05bff18">OH_NNModel_SetTensorQuantParams</a> to set it to a tensor. After that you should destroy it by calling <a class="el" href="group___neural_nework_runtime.html#gadb7a5667b419e1d3e1065e542a7f78a9">OH_NNQuantParam_Destroy</a> to avoid memory leak. <br />
 </p><dl class="section return"><dt>Returns</dt><dd>Pointer to a <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance, or NULL if it fails to create. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gadb7a5667b419e1d3e1065e542a7f78a9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gadb7a5667b419e1d3e1065e542a7f78a9">&#9670;&nbsp;</a></span>OH_NNQuantParam_Destroy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNQuantParam_Destroy </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> **&#160;</td>
          <td class="paramname"><em>quantParams</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Releases a <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance. </p>
<p>The <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance needs to be released to avoid memory leak after it is set to a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. <br />
 If <b>quantParams</b> or <b>*quantParams</b> is a null pointer, this method only prints warning logs and does not execute the release. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">quantParams</td><td>Double pointer to the <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> destroy quant parameters object successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to destroy quant parameters object. The possible reason for failure is that the <b>quantParams</b> or <b>*quantParams</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gad7ddfbe997bb70b6b5fb1d490dbeb8d8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gad7ddfbe997bb70b6b5fb1d490dbeb8d8">&#9670;&nbsp;</a></span>OH_NNQuantParam_SetNumBits()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNQuantParam_SetNumBits </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> *&#160;</td>
          <td class="paramname"><em>quantParams</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint32_t *&#160;</td>
          <td class="paramname"><em>numBits</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>quantCount</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the number bits of the <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance. </p>
<p>The parameter <b>quantCount</b> is the number of quantization parameters of a tensor, e.g. the quantCount is the channel count if the tensor is per-channel quantized.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">quantParams</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance. </td></tr>
    <tr><td class="paramname">numBits</td><td>An array of number bits for all quantization parameters of the tensor. </td></tr>
    <tr><td class="paramname">quantCount</td><td>Number of quantization parameters of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set num bits of quant parameters successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set num bits of quant parameters. The possible reason for failure is that the <b>quantParams</b> or <b>numBits</b> is nullptr, or <b>quantCount</b> is 0.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gaff24853638b0aa423fcd2724e3dfaaf1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaff24853638b0aa423fcd2724e3dfaaf1">&#9670;&nbsp;</a></span>OH_NNQuantParam_SetScales()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNQuantParam_SetScales </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> *&#160;</td>
          <td class="paramname"><em>quantParams</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>scales</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>quantCount</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the scales of the <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance. </p>
<p>The parameter <b>quantCount</b> is the number of quantization parameters of a tensor, e.g. the quantCount is the channel count if the tensor is per-channel quantized.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">quantParams</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance. </td></tr>
    <tr><td class="paramname">scales</td><td>An array of scales for all quantization parameters of the tensor. </td></tr>
    <tr><td class="paramname">quantCount</td><td>Number of quantization parameters of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set scales of quant parameters successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set scales of quant parameters. The possible reason for failure is that the <b>quantParams</b> or <b>scales</b> is nullptr, or <b>quantCount</b> is 0.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gac6c284fb1577981a5e9da90cedc65b86"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gac6c284fb1577981a5e9da90cedc65b86">&#9670;&nbsp;</a></span>OH_NNQuantParam_SetZeroPoints()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNQuantParam_SetZeroPoints </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> *&#160;</td>
          <td class="paramname"><em>quantParams</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t *&#160;</td>
          <td class="paramname"><em>zeroPoints</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>quantCount</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the zero points of the <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance. </p>
<p>The parameter <b>quantCount</b> is the number of quantization parameters of a tensor, e.g. the quantCount is the channel count if the tensor is per-channel quantized.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">quantParams</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga94cf3b68da13f278ad55431da7342619">NN_QuantParam</a> instance. </td></tr>
    <tr><td class="paramname">zeroPoints</td><td>An array of zero points for all quantization parameters of the tensor. </td></tr>
    <tr><td class="paramname">quantCount</td><td>Number of quantization parameters of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set zero points of quant parameters successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set zero points of quant parameters. The possible reason for failure is that the <b>quantParams</b> or <b>zeroPoints</b> is nullptr, or <b>quantCount</b> is 0.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga724c81fe4a2e48021fd545b3c072ba50"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga724c81fe4a2e48021fd545b3c072ba50">&#9670;&nbsp;</a></span>OH_NNTensor_Create()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>* OH_NNTensor_Create </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>deviceID</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance from <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>This method use <a class="el" href="group___neural_nework_runtime.html#gabeca8077942b547ad6160807f38332c1">OH_NNTensorDesc_GetByteSize</a> to calculate the byte size of tensor data and allocate shared memory on device for it. The device dirver will get the tensor data directly by the "zero-copy" way.<br />
 Note that this method will copy the <b>tensorDesc</b> into <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. Therefore you should destroy <b>tensorDesc</b> by <a class="el" href="group___neural_nework_runtime.html#ga4e299cd1a5d0f2c1e2507f86b880fb60">OH_NNTensorDesc_Destroy</a> if it is no longer used.<br />
 If the tensor shape is dynamic, this method will return error code.<br />
 <b>deviceID</b> indicates the selected device. If it is 0, the first device in the current device list will be used by default.<br />
 <b>tensorDesc</b> must be provided, and this method will return an error code if it is a null pointer.<br />
 Call <a class="el" href="group___neural_nework_runtime.html#ga33d9d392ba17947c578b099961d32789">OH_NNTensor_Destroy</a> to release the <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance if it is no longer used.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">deviceID</td><td>Device id. If it is 0, the first device in the current device list will be used by default. </td></tr>
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pointer to a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance, or NULL if it fails to create. The possible reason for failure is that the <b>tensorDesc</b> is nullptr, or <b>deviceID</b> is invalid, or memory error occurred. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gafd7fac565ba766422257e1525dc416ce"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gafd7fac565ba766422257e1525dc416ce">&#9670;&nbsp;</a></span>OH_NNTensor_CreateWithFd()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>* OH_NNTensor_CreateWithFd </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>deviceID</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>fd</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>offset</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance with specified file descriptor and <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>This method reuses the shared memory corresponding to the file descriptor <b>fd</b> passed. It may comes from another <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance. When you call the <a class="el" href="group___neural_nework_runtime.html#ga33d9d392ba17947c578b099961d32789">OH_NNTensor_Destroy</a> method to release the tensor created by this method, the tensor data memory will not be released.<br />
 Note that this method will copy the <b>tensorDesc</b> into <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. Therefore you should destroy <b>tensorDesc</b> by <a class="el" href="group___neural_nework_runtime.html#ga4e299cd1a5d0f2c1e2507f86b880fb60">OH_NNTensorDesc_Destroy</a> if it is no longer used.<br />
 <b>deviceID</b> indicates the selected device. If it is 0, the first device in the current device list will be used by default.<br />
</p>
<p><b>tensorDesc</b> must be provided, if it is a null pointer, the method returns an error code.<br />
 Call <a class="el" href="group___neural_nework_runtime.html#ga33d9d392ba17947c578b099961d32789">OH_NNTensor_Destroy</a> to release the <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance if it is no longer used.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">deviceID</td><td>Device id. If it is 0, the first device in the current device list will be used by default. </td></tr>
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
    <tr><td class="paramname">fd</td><td>file descriptor of the shared memory to be resued. </td></tr>
    <tr><td class="paramname">size</td><td>Size of the shared memory to be resued. </td></tr>
    <tr><td class="paramname">offset</td><td>Offset of the shared memory to be resued. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pinter to a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance, or NULL if it fails to create. The possible reason for failure is that the <b>tensorDesc</b> is nullptr, or <b>deviceID</b>, <b>fd</b>, <b>size</b> or <b>offset</b> is invalid, or memory error occurred. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gad658b50239b119baece25c7470ccf403"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gad658b50239b119baece25c7470ccf403">&#9670;&nbsp;</a></span>OH_NNTensor_CreateWithSize()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>* OH_NNTensor_CreateWithSize </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>deviceID</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>size</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance with specified size and <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>This method use <b>size</b> as the byte size of tensor data and allocate shared memory on device for it. The device dirver will get the tensor data directly by the "zero-copy" way.<br />
 Note that this method will copy the <b>tensorDesc</b> into <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. Therefore you should destroy <b>tensorDesc</b> by <a class="el" href="group___neural_nework_runtime.html#ga4e299cd1a5d0f2c1e2507f86b880fb60">OH_NNTensorDesc_Destroy</a> if it is no longer used.<br />
 <b>deviceID</b> indicates the selected device. If it is 0, the first device in the current device list will be used by default.<br />
 <b>tensorDesc</b> must be provided, if it is a null pointer, the method returns an error code. <b>size</b> must be no less than the byte size of tensorDesc. Otherwise, this method will return an error code. If the tensor shape is dynamic, the <b>size</b> will not be checked.<br />
 Call <a class="el" href="group___neural_nework_runtime.html#ga33d9d392ba17947c578b099961d32789">OH_NNTensor_Destroy</a> to release the <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance if it is no longer used.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">deviceID</td><td>Device id. If it is 0, the first device in the current device list will be used by default. </td></tr>
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
    <tr><td class="paramname">size</td><td>Size of tensor data that need to be allocated. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pointer to a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance, or NULL if it fails to create. The possible reason for failure is that the <b>tensorDesc</b> is nullptr, or <b>deviceID</b> or size is invalid, or memory error occurred. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga33d9d392ba17947c578b099961d32789"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga33d9d392ba17947c578b099961d32789">&#9670;&nbsp;</a></span>OH_NNTensor_Destroy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensor_Destroy </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> **&#160;</td>
          <td class="paramname"><em>tensor</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Releases a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance. </p>
<p>When the <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance is no longer used, this method needs to be called to release the instance. Otherwise, the memory leak will occur.<br />
 If <b>tensor</b> or <b>*tensor</b> is a null pointer, this method will return error code and does not execute the release.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>Double pointer to the <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> destroy tensor successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to destroy tensor. The possible reason for failure is that the <b>tensor</b> is nullptr, or <b>*tensor</b> is not nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga8ac1491aa7df59382a46ffceb7302638"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga8ac1491aa7df59382a46ffceb7302638">&#9670;&nbsp;</a></span>OH_NNTensor_GetDataBuffer()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void* OH_NNTensor_GetDataBuffer </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *&#160;</td>
          <td class="paramname"><em>tensor</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the data buffer of a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. </p>
<p>You can read/write data from/to the tensor data buffer. The buffer is mapped from a shared memory on device, so the device dirver will get the tensor data directly by this "zero-copy" way.<br />
 Note that the real tensor data only uses the segment [offset, size) of the shared memory. The offset can be got by <a class="el" href="group___neural_nework_runtime.html#gae4d72419b8e7fc0ac70bf91ab7510ecb">OH_NNTensor_GetOffset</a> and the size can be got by <a class="el" href="group___neural_nework_runtime.html#gac5f825923842d3484b66e799b6bda650">OH_NNTensor_GetSize</a>.<br />
 if <b>tensor</b> is a null pointer, this method will return null pointer.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pointer to data buffer of the tensor, or NULL if it fails to create. The possible reason for failure is that the <b>tensor</b> is nullptr, or <b>tensor</b> is invalid. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga581e19d403376f61434edd1d86b30c11"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga581e19d403376f61434edd1d86b30c11">&#9670;&nbsp;</a></span>OH_NNTensor_GetFd()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensor_GetFd </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *&#160;</td>
          <td class="paramname"><em>tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int *&#160;</td>
          <td class="paramname"><em>fd</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the file descriptor of the shared memory of a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. </p>
<p>The file descriptor <b>fd</b> corresponds to the shared memory of the tensor data, and can be resued by another <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> through <a class="el" href="group___neural_nework_runtime.html#gafd7fac565ba766422257e1525dc416ce">OH_NNTensor_CreateWithFd</a>.<br />
 if <b>tensor</b> or <b>fd</b> is a null pointer, this method will return error code.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance. </td></tr>
    <tr><td class="paramname">fd</td><td>The returned file descriptor of the shared memory. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get tensor fd successfully. The return value is saved in parameter fd.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get tensor fd. The possible reason for failure is that the <b>tensor</b> or <b>fd</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gae4d72419b8e7fc0ac70bf91ab7510ecb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gae4d72419b8e7fc0ac70bf91ab7510ecb">&#9670;&nbsp;</a></span>OH_NNTensor_GetOffset()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensor_GetOffset </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *&#160;</td>
          <td class="paramname"><em>tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>offset</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Get the data offset of a tensor. </p>
<p>The <b>offset</b> corresponds to the shared memory of the tensor data, and can be resued by another <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> through <a class="el" href="group___neural_nework_runtime.html#gafd7fac565ba766422257e1525dc416ce">OH_NNTensor_CreateWithFd</a>.<br />
 Note that the real tensor data only uses the segment [offset, size) of the shared memory. The offset can be got by <a class="el" href="group___neural_nework_runtime.html#gae4d72419b8e7fc0ac70bf91ab7510ecb">OH_NNTensor_GetOffset</a> and the size can be got by <a class="el" href="group___neural_nework_runtime.html#gac5f825923842d3484b66e799b6bda650">OH_NNTensor_GetSize</a>.<br />
 if <b>tensor</b> or <b>offset</b> is a null pointer, this method will return error code.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance. </td></tr>
    <tr><td class="paramname">offset</td><td>The returned offset of tensor data. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get tensor offset successfully. The return value is saved in <b>offset</b>.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get tensor offset. The possible reason for failure is that the <b>tensor</b> or <b>offset</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gac5f825923842d3484b66e799b6bda650"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gac5f825923842d3484b66e799b6bda650">&#9670;&nbsp;</a></span>OH_NNTensor_GetSize()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensor_GetSize </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *&#160;</td>
          <td class="paramname"><em>tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>size</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the size of the shared memory of a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. </p>
<p>The <b>size</b> corresponds to the shared memory of the tensor data, and can be resued by another <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> through <a class="el" href="group___neural_nework_runtime.html#gafd7fac565ba766422257e1525dc416ce">OH_NNTensor_CreateWithFd</a>.<br />
 The <b>size</b> is as same as the argument <b>size</b> of <a class="el" href="group___neural_nework_runtime.html#gad658b50239b119baece25c7470ccf403">OH_NNTensor_CreateWithSize</a> and <a class="el" href="group___neural_nework_runtime.html#gafd7fac565ba766422257e1525dc416ce">OH_NNTensor_CreateWithFd</a>. But for a tensor created by <a class="el" href="group___neural_nework_runtime.html#ga724c81fe4a2e48021fd545b3c072ba50">OH_NNTensor_Create</a>, it equals to the tensor byte size.<br />
 Note that the real tensor data only uses the segment [offset, size) of the shared memory. The offset can be got by <a class="el" href="group___neural_nework_runtime.html#gae4d72419b8e7fc0ac70bf91ab7510ecb">OH_NNTensor_GetOffset</a> and the size can be got by <a class="el" href="group___neural_nework_runtime.html#gac5f825923842d3484b66e799b6bda650">OH_NNTensor_GetSize</a>.<br />
 if <b>tensor</b> or <b>size</b> is a null pointer, this method will return error code.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance. </td></tr>
    <tr><td class="paramname">size</td><td>The returned size of tensor data. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get tensor size successfully. The return value is saved in <b>size</b>.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get tensor size. The possible reason for failure is that the <b>tensor</b> or <b>size</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gaaeb03b1a5cd44ba29f71b5814acb73b2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaaeb03b1a5cd44ba29f71b5814acb73b2">&#9670;&nbsp;</a></span>OH_NNTensor_GetTensorDesc()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>* OH_NNTensor_GetTensorDesc </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> *&#160;</td>
          <td class="paramname"><em>tensor</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance of a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. </p>
<p>Call this method to obtain the inner <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance pointer of the specified <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance. You can get various types of the tensor attributes such as name/format/data type/shape from the returned <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance.<br />
 You should not destory the returned <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance because it points to the inner instance of <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. Otherwise, a menory corruption of double free will occur when <a class="el" href="group___neural_nework_runtime.html#ga33d9d392ba17947c578b099961d32789">OH_NNTensor_Destroy</a> is called.<br />
 if <b>tensor</b> is a null pointer, this method will return null pointer.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance, or NULL if it fails to create. The possible reason for failure is that the <b>tensor</b> is nullptr, or <b>tensor</b> is invalid. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga69293f465289045fd6ce120d8f7fb58c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga69293f465289045fd6ce120d8f7fb58c">&#9670;&nbsp;</a></span>OH_NNTensorDesc_Create()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>* OH_NNTensorDesc_Create </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Creates an <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </p>
<p>The <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> describes various tensor attributes, such as name/data type/shape/format, etc.<br />
 The following methods can be called to create a <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instance based on the passed <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance:<br />
<a class="el" href="group___neural_nework_runtime.html#ga724c81fe4a2e48021fd545b3c072ba50">OH_NNTensor_Create</a><br />
<a class="el" href="group___neural_nework_runtime.html#gad658b50239b119baece25c7470ccf403">OH_NNTensor_CreateWithSize</a><br />
<a class="el" href="group___neural_nework_runtime.html#gafd7fac565ba766422257e1525dc416ce">OH_NNTensor_CreateWithFd</a><br />
 Note that these methods will copy the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance into <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a>. Therefore you can create multiple <a class="el" href="group___neural_nework_runtime.html#ga36df49080d9305767e96668e948a8f3b">NN_Tensor</a> instances with the same <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. And you should destroy the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance by <a class="el" href="group___neural_nework_runtime.html#ga4e299cd1a5d0f2c1e2507f86b880fb60">OH_NNTensorDesc_Destroy</a> when it is no longer used.<br />
 </p><dl class="section return"><dt>Returns</dt><dd>Pointer to a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance, or NULL if it fails to create. The possible reason for failure is that the memory error occurred during object creation. </dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga4e299cd1a5d0f2c1e2507f86b880fb60"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga4e299cd1a5d0f2c1e2507f86b880fb60">&#9670;&nbsp;</a></span>OH_NNTensorDesc_Destroy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensorDesc_Destroy </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> **&#160;</td>
          <td class="paramname"><em>tensorDesc</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Releases an <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </p>
<p>When the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance is no longer used, this method needs to be called to release it. Otherwise, the memory leak will occur. <br />
 If <b>tensorDesc</b> or <b>*tensorDesc</b> is a null pointer, this method will return error code and does not execute the release. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensorDesc</td><td>Double pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> destroy tensor description successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to destroy tensor description. The possible reason for failure is that the <b>tensorDesc</b> or <b>*tensorDesc</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gabeca8077942b547ad6160807f38332c1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gabeca8077942b547ad6160807f38332c1">&#9670;&nbsp;</a></span>OH_NNTensorDesc_GetByteSize()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensorDesc_GetByteSize </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>byteSize</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the byte size of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>Call this method to obtain the byte size of the specified <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. <br />
 If the tensor shape is dynamic, this method will return error code, and <b>byteSize</b> will be 0. <br />
 If you need to obtain element count of the tensor data, call <a class="el" href="group___neural_nework_runtime.html#gada602068be210bc6c7f778f0931e5848">OH_NNTensorDesc_GetElementCount</a>. <br />
 if <b>tensorDesc</b> or <b>byteSize</b> is a null pointer, this method will return error code. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
    <tr><td class="paramname">byteSize</td><td>The returned byte size of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get tensor byte size successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get tensor byte size. The possible reason for failure is that the <b>tensorDesc</b> or <b>byteSize</b> is nullptr, or tensor data type is invalid.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a6d639b026c18ecc858873d1cb45f41eb">OH_NN_DYNAMIC_SHAPE</a> dim is less than zero.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gaea89367832dba35f842c526d0325bc5b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaea89367832dba35f842c526d0325bc5b">&#9670;&nbsp;</a></span>OH_NNTensorDesc_GetDataType()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensorDesc_GetDataType </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga917dcf1cb1e7bb745ccf984e9e67940e">OH_NN_DataType</a> *&#160;</td>
          <td class="paramname"><em>dataType</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the data type of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>Call this method to obtain the data type of the specified <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. <br />
 if <b>tensorDesc</b> or <b>dataType</b> is a null pointer, this method will return error code. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
    <tr><td class="paramname">dataType</td><td>The returned data type of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get tensor data type successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get tensor data type. The possible reason for failure is that the <b>tensorDesc</b> or <b>dataType</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gada602068be210bc6c7f778f0931e5848"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gada602068be210bc6c7f778f0931e5848">&#9670;&nbsp;</a></span>OH_NNTensorDesc_GetElementCount()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensorDesc_GetElementCount </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>elementCount</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the element count of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>Call this method to obtain the element count of the specified <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. If you need to obtain byte size of the tensor data, call <a class="el" href="group___neural_nework_runtime.html#gabeca8077942b547ad6160807f38332c1">OH_NNTensorDesc_GetByteSize</a>. <br />
 If the tensor shape is dynamic, this method will return error code, and <b>elementCount</b> will be 0. <br />
 if <b>tensorDesc</b> or <b>elementCount</b> is a null pointer, this method will return error code. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
    <tr><td class="paramname">elementCount</td><td>The returned element count of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get tensor element count successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get tensor element count. The possible reason for failure is that the <b>tensorDesc</b> or <b>elementCount</b> is nullptr.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a6d639b026c18ecc858873d1cb45f41eb">OH_NN_DYNAMIC_SHAPE</a> dim is less than zero.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga54953cf8b797779898660fd50e550623"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga54953cf8b797779898660fd50e550623">&#9670;&nbsp;</a></span>OH_NNTensorDesc_GetFormat()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensorDesc_GetFormat </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga467f4b3c524b8ec4ebf3c86d680f7f59">OH_NN_Format</a> *&#160;</td>
          <td class="paramname"><em>format</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the format of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>Call this method to obtain the format of the specified <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. <br />
 if <b>tensorDesc</b> or <b>format</b> is a null pointer, this method will return error code. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
    <tr><td class="paramname">format</td><td>The returned format of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get tensor format successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get tensor format. The possible reason for failure is that the <b>tensorDesc</b> or <b>format</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga6a4c3c06e3c85c9169da0d2645d7ef9e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga6a4c3c06e3c85c9169da0d2645d7ef9e">&#9670;&nbsp;</a></span>OH_NNTensorDesc_GetName()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensorDesc_GetName </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const char **&#160;</td>
          <td class="paramname"><em>name</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the name of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>Call this method to obtain the name of the specified <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. The value of <b>*name</b> is a C-style string ended with <b>'\0'</b>.<br />
 if <b>tensorDesc</b> or <b>name</b> is a null pointer, this method will return error code. As an output parameter, <b>*name</b> must be a null pointer, otherwise the method will return an error code. Fou example, you should define char* tensorName = NULL, and pass &amp;tensorName as the argument of <b>name</b>.<br />
 You do not need to release the memory of <b>name</b>. It will be released when <b>tensorDesc</b> is destroied.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
    <tr><td class="paramname">name</td><td>The retured name of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get tensor name successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get tensor name. The possible reason for failure is that the <b>tensorDesc</b> or <b>name</b> is nullptr, or <b>*name</b> is not nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gae2c7ad5cf0133ec216d66d168aedc10f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gae2c7ad5cf0133ec216d66d168aedc10f">&#9670;&nbsp;</a></span>OH_NNTensorDesc_GetShape()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensorDesc_GetShape </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int32_t **&#160;</td>
          <td class="paramname"><em>shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t *&#160;</td>
          <td class="paramname"><em>shapeLength</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Gets the shape of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>Call this method to obtain the shape of the specified <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. <br />
 if <b>tensorDesc</b>, <b>shape</b> or <b>shapeLength</b> is a null pointer, this method will return error code. As an output parameter, <b>*shape</b> must be a null pointer, otherwise the method will return an error code. Fou example, you should define int32_t* tensorShape = NULL, and pass &amp;tensorShape as the argument of <b>shape</b>. <br />
 You do not need to release the memory of <b>shape</b>. It will be released when <b>tensorDesc</b> is destroied. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
    <tr><td class="paramname">shape</td><td>Return the shape list of the tensor. </td></tr>
    <tr><td class="paramname">shapeLength</td><td>The returned length of the shape list. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> get tensor shape successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to get tensor shape. The possible reason for failure is that the <b>tensorDesc</b>, <b>shape</b> or <b>shapeLength</b> is nullptr, or <b>*shape</b> is not nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga5da36f7eb8fa101edb9965439f728ac6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga5da36f7eb8fa101edb9965439f728ac6">&#9670;&nbsp;</a></span>OH_NNTensorDesc_SetDataType()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensorDesc_SetDataType </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga917dcf1cb1e7bb745ccf984e9e67940e">OH_NN_DataType</a>&#160;</td>
          <td class="paramname"><em>dataType</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the data type of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>After the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance is created, call this method to set the tensor data type. <br />
 if <b>tensorDesc</b> is a null pointer, this method will return error code. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
    <tr><td class="paramname">dataType</td><td>The data type of the tensor that needs to be set. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set tensor data type successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set tensor data type. The possible reason for failure is that the <b>tensorDesc</b> is nullptr, or <b>dataType</b> is invalid.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga066b3fdcea563ab96e9d8fadf9df3c8d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga066b3fdcea563ab96e9d8fadf9df3c8d">&#9670;&nbsp;</a></span>OH_NNTensorDesc_SetFormat()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensorDesc_SetFormat </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga467f4b3c524b8ec4ebf3c86d680f7f59">OH_NN_Format</a>&#160;</td>
          <td class="paramname"><em>format</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the format of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>After the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance is created, call this method to set the tensor format. <br />
 if <b>tensorDesc</b> is a null pointer, this method will return error code. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
    <tr><td class="paramname">format</td><td>The format of the tensor that needs to be set. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set tensor format successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set tensor format. The possible reason for failure is that the <b>tensorDesc</b> is nullptr, or <b>format</b> is invalid.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="gaca0cfcef499c386c2a49725798a55ed2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaca0cfcef499c386c2a49725798a55ed2">&#9670;&nbsp;</a></span>OH_NNTensorDesc_SetName()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensorDesc_SetName </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const char *&#160;</td>
          <td class="paramname"><em>name</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the name of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>After the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance is created, call this method to set the tensor name. The value of <b>*name</b> is a C-style string ended with <b>'\0'</b>.<br />
 if <b>tensorDesc</b> or <b>name</b> is a null pointer, this method will return error code.<br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
    <tr><td class="paramname">name</td><td>The name of the tensor that needs to be set. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set tensor name successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set tensor name. The possible reason for failure is that the <b>tensorDesc</b> or <b>name</b> is nullptr.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
<a id="ga2050fd8e0fac1a8a234629c43c80fad3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2050fd8e0fac1a8a234629c43c80fad3">&#9670;&nbsp;</a></span>OH_NNTensorDesc_SetShape()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group___neural_nework_runtime.html#ga28c9d17051cc5833d0b749c0b61e1823">OH_NN_ReturnCode</a> OH_NNTensorDesc_SetShape </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> *&#160;</td>
          <td class="paramname"><em>tensorDesc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t *&#160;</td>
          <td class="paramname"><em>shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>shapeLength</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sets the shape of a <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a>. </p>
<p>After the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance is created, call this method to set the tensor shape. <br />
 if <b>tensorDesc</b> or <b>shape</b> is a null pointer, or <b>shapeLength</b> is 0, this method will return error code. <br />
 </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensorDesc</td><td>Pointer to the <a class="el" href="group___neural_nework_runtime.html#ga15da798ec7d8a9e924f172a252fcf080">NN_TensorDesc</a> instance. </td></tr>
    <tr><td class="paramname">shape</td><td>The shape list of the tensor that needs to be set. </td></tr>
    <tr><td class="paramname">shapeLength</td><td>The length of the shape list that needs to be set. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Execution result of the function. <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823a791d792b34ed27124534fbf35c35ff61">OH_NN_SUCCESS</a> set tensor shape successfully.<br />
 <a class="el" href="group___neural_nework_runtime.html#gga28c9d17051cc5833d0b749c0b61e1823aa5a97cb90566c37442c024d5be876289">OH_NN_INVALID_PARAMETER</a> fail to set tensor shape. The possible reason for failure is that the <b>tensorDesc</b> or <b>shape</b> is nullptr, or <b>shapeLength</b> is 0.<br />
</dd></dl>
<dl class="section since"><dt>Since</dt><dd>11 </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
